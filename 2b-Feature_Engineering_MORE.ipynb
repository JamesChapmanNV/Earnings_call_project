{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256c016d-3a52-4525-b92f-d9eafcb7899f",
   "metadata": {},
   "source": [
    "# Earnings Call Project: MORE\n",
    "<br>\n",
    "CIS 831 Deep Learning – Term Project<br>\n",
    "Kansas State University\n",
    "<br><br>\n",
    "James Chapman<br>\n",
    "John Woods<br>\n",
    "Nathan Diehl<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88afe45e-603e-4403-b191-ecd36af94ef7",
   "metadata": {},
   "source": [
    "### This notebook featurizes the text data from the earnings calls with RoBERTa and SentenceTransformer.\n",
    "\n",
    "RoBERTa documentation can be found at https://huggingface.co/FacebookAI/roberta-large\n",
    "\n",
    "\n",
    "[SentenceTransformer](https://www.sbert.net/) from hugging face is used with the following 3 models.\n",
    "- [finance-embeddings-investopedia](https://huggingface.co/FinLang/finance-embeddings-investopedia)\n",
    "- [bge-m3-financial-matryoshka](https://huggingface.co/haophancs/bge-m3-financial-matryoshka)\n",
    "- [bge-base-financial-matryoshka](https://huggingface.co/philschmid/bge-base-financial-matryoshka)\n",
    "\n",
    "The data from this notebook is stored in the \"data/data_prep\" directory as the following CSVs.\n",
    "- RoBERTa_features\n",
    "- MACE_RoBERTa_features\n",
    "- RoBERTa_features2\n",
    "- MACE_RoBERTa_features2\n",
    "- investopedia_features -------768 features\n",
    "- MACE_investopedia_features --768 features\n",
    "- bge_features ----------------1024 features\n",
    "- MACE_bge_features -----------1024 features\n",
    "- bge_base_features -----------768 features\n",
    "- MACE_bge_base_features ------768 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88d0789-36df-4250-90e7-1485c0dc9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5822f8a-897e-4fd5-b77b-1c38e8d7ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAEC_dir = 'data/MAEC/MAEC_Dataset' # https://github.com/Earnings-Call-Dataset/MAEC-A-Multimodal-Aligned-Earnings-Conference-Call-Dataset-for-Financial-Risk-Prediction\n",
    "\n",
    "############# too big for GitHub ########################\n",
    "############# stored on local disk ######################\n",
    "original_data_dir = r\"D:\\original_dataset\" # https://github.com/GeminiLn/EarningsCall_Dataset \n",
    "MAEC_audio_dir = r\"D:\\MAEC_audio\" \n",
    "# there is a link for the audio data in the MAEC GitHub, but it does not work\n",
    "# I emailed the authors, and they send another link.\n",
    "# There is like a half-million files, but only 19 GB\n",
    "# https://drive.google.com/file/d/1m1GRCHgKn9Vz9IFMC_SpCog6uP3-gFgY/view?usp=drive_link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7908bf-5a46-4a22-b959-acc40ce09ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the directory, each folder represents an earnings conference call; the folders are named as \"CompanyName_Date\".\n",
    "filename_data = []\n",
    "for filename in os.listdir(original_data_dir):\n",
    "    company_name, date_str = filename.rsplit('_', 1)\n",
    "    date_str = date_str.split('.')[0] \n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    filename_data.append([company_name, date])\n",
    "filename_data = pd.DataFrame(filename_data, columns=[\"Company\", \"Date\"])\n",
    "company_ticker = pd.read_csv('data/data_prep/company_ticker.csv')\n",
    "filename_data = filename_data.merge(company_ticker, on=\"Company\", how=\"left\")\n",
    "\n",
    "# Loop through the directory, each folder represents an earnings conference call; the folders are named as \"Date_CompanyName\".\n",
    "MAEC_filename_data = []\n",
    "for filename in os.listdir(MAEC_dir):\n",
    "    date_str, ticker = filename.rsplit('_', 1)\n",
    "    date_str = date_str.split('.')[0] \n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    MAEC_filename_data.append([ticker, date])\n",
    "MAEC_filename_data = pd.DataFrame(MAEC_filename_data, columns=[\"Ticker\", \"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e6b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(model, model_name, num_features ):\n",
    "    print(f'Applying {model_name} to the original dataset …')\n",
    "    # num_features\n",
    "    columns = [f'{model_name}_{j}' for j in range(num_features)] + ['Company', 'Date', 'Sentence_num']\n",
    "    features = []\n",
    "    errors = []\n",
    "    for Company, Date in tqdm(filename_data[['Company', 'Date']].values):\n",
    "        Date = Date.replace('-', '')\n",
    "        text_path = f\"D:/original_dataset/{Company}_{Date}/TextSequence.txt\"\n",
    "        try:\n",
    "            with open(text_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                for i, line in enumerate(file, start=1):\n",
    "                    # apply model\n",
    "                    sentence_embedding = model(line.strip())\n",
    "                    features_row = np.concatenate([sentence_embedding.flatten(), [Company, Date, i]])\n",
    "                    features.append(features_row)\n",
    "        except KeyboardInterrupt: break\n",
    "        except Exception as e:\n",
    "            errors.append((Company, Date, str(e)))\n",
    "    features = np.array(features, dtype=object)\n",
    "    features = pd.DataFrame(features, columns=columns)\n",
    "    features.info(verbose=False)\n",
    "    \n",
    "    print(f\"Number of errors: {len(errors)}\")\n",
    "    print(errors)\n",
    "    ###############################################\n",
    "    features.to_csv(f'data/data_prep/{model_name}.csv', index=False)\n",
    "    ###############################################\n",
    "\n",
    "def apply_model_MAEC(model, model_name, num_features):\n",
    "    print(f'Applying {model_name} to the MAEC dataset …')\n",
    "    # num_features\n",
    "    columns = [f'{model_name}_{j}' for j in range(num_features)] + ['Ticker', 'Date', 'Sentence_num']\n",
    "    features = []\n",
    "    errors = []\n",
    "    for Ticker, Date in tqdm(MAEC_filename_data[['Ticker', 'Date']].values):\n",
    "        Date = Date.replace('-', '')\n",
    "        text_path = f\"D:/MAEC_audio/{Date}_{Ticker}/text.txt\"\n",
    "        try:\n",
    "            with open(text_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                for i, line in enumerate(file, start=1):\n",
    "                    # apply model\n",
    "                    sentence_embedding = model(line.strip())\n",
    "                    features_row = np.concatenate([sentence_embedding.flatten(), [Ticker, Date, i]])\n",
    "                    features.append(features_row)\n",
    "        except KeyboardInterrupt: break\n",
    "        except Exception as e:\n",
    "            errors.append((Ticker, Date, str(e)))\n",
    "    features = np.array(features, dtype=object)\n",
    "    features = pd.DataFrame(features, columns=columns)\n",
    "    features.info(verbose=False)\n",
    "    \n",
    "    print(f\"Number of errors: {len(errors)}\")\n",
    "    print(errors)\n",
    "    ###############################################\n",
    "    features.to_csv(f'data/data_prep/MAEC_{model_name}.csv', index=False)\n",
    "    ###############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055641c3-e617-46a5-99f0-4808ba98edcf",
   "metadata": {},
   "source": [
    "# RoBERTa features from meeting transcript text files\n",
    "\n",
    "RoBERTa documentation can be found at https://huggingface.co/FacebookAI/roberta-large\n",
    "\n",
    "### Following code is adapted FROM\n",
    "[GitHub HTML Encoder](https://github.com/YangLinyi/HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction/blob/master/Model/Token-Level%20Encoder/HuggingFace-Roberta-Token-Encoder.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21943fb0-68be-4c8b-9c29-6983cef7f8ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RobertaModel.from_pretrained('roberta-large').to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "def get_RoBERTa(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # [CLS] embedding for sentence-level representation\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    #print(cls_embedding.shape)\n",
    "    # 1024 features \n",
    "    return cls_embedding\n",
    "\n",
    "\n",
    "def get_RoBERTa_with_averaging(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # average pooling over token embeddings\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(token_embeddings, dim=1).cpu().numpy()  \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b87fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying RoBERTa_features to the original dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [12:21<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89722 entries, 0 to 89721\n",
      "Columns: 1027 entries, RoBERTa_features_0 to Sentence_num\n",
      "dtypes: object(1027)\n",
      "memory usage: 703.0+ MB\n",
      "Number of errors: 0\n",
      "[]\n",
      "Applying RoBERTa_features to the MAEC dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [55:33<00:00,  1.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394277 entries, 0 to 394276\n",
      "Columns: 1027 entries, RoBERTa_features_0 to Sentence_num\n",
      "dtypes: object(1027)\n",
      "memory usage: 3.0+ GB\n",
      "Number of errors: 0\n",
      "[]\n",
      "Applying RoBERTa_features2 to the original dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [11:53<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89722 entries, 0 to 89721\n",
      "Columns: 1027 entries, RoBERTa_features2_0 to Sentence_num\n",
      "dtypes: object(1027)\n",
      "memory usage: 703.0+ MB\n",
      "Number of errors: 0\n",
      "[]\n",
      "Applying RoBERTa_features2 to the MAEC dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [53:02<00:00,  1.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394277 entries, 0 to 394276\n",
      "Columns: 1027 entries, RoBERTa_features2_0 to Sentence_num\n",
      "dtypes: object(1027)\n",
      "memory usage: 3.0+ GB\n",
      "Number of errors: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa\n",
    "apply_model(get_RoBERTa, 'RoBERTa_features', 1024 )\n",
    "apply_model_MAEC(get_RoBERTa, 'RoBERTa_features', 1024)\n",
    "# RoBERTa_with_averaging\n",
    "apply_model(get_RoBERTa_with_averaging, 'RoBERTa_features2', 1024 )\n",
    "apply_model_MAEC(get_RoBERTa_with_averaging, 'RoBERTa_features2', 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb468653",
   "metadata": {},
   "source": [
    "# SentenceTransformers\n",
    "\n",
    "## 1. FinLang/finance-embeddings-investopedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245581e7-3eed-46a8-a7c0-4e949f4c026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying investopedia_features to the original dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [10:37<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89722 entries, 0 to 89721\n",
      "Columns: 771 entries, investopedia_features_0 to Sentence_num\n",
      "dtypes: object(771)\n",
      "memory usage: 527.8+ MB\n",
      "Number of errors: 0\n",
      "[]\n",
      "Applying investopedia_features to the MAEC dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [41:49<00:00,  1.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394277 entries, 0 to 394276\n",
      "Columns: 771 entries, investopedia_features_0 to Sentence_num\n",
      "dtypes: object(771)\n",
      "memory usage: 2.3+ GB\n",
      "Number of errors: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "Sentence_Transformer_model = SentenceTransformer(\"FinLang/finance-embeddings-investopedia\", device=device) # [, 768]\n",
    "def get_Sentence_Transformer(sentence):\n",
    "    sentence_embedding = Sentence_Transformer_model.encode(sentence)\n",
    "    return sentence_embedding.flatten()\n",
    "# investopedia\n",
    "apply_model(get_Sentence_Transformer, 'investopedia_features', 768)\n",
    "apply_model_MAEC(get_Sentence_Transformer, 'investopedia_features', 768)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ccef8f",
   "metadata": {},
   "source": [
    "## 2. haophancs/bge-m3-financial-matryoshka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52a413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying bge_features to the original dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [18:57<00:00,  1.99s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89722 entries, 0 to 89721\n",
      "Columns: 1027 entries, bge_features_0 to Sentence_num\n",
      "dtypes: object(1027)\n",
      "memory usage: 703.0+ MB\n",
      "Number of errors: 0\n",
      "[]\n",
      "Applying bge_features to the MAEC dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [1:15:20<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394277 entries, 0 to 394276\n",
      "Columns: 1027 entries, bge_features_0 to Sentence_num\n",
      "dtypes: object(1027)\n",
      "memory usage: 3.0+ GB\n",
      "Number of errors: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "Sentence_Transformer_model = SentenceTransformer(\"haophancs/bge-m3-financial-matryoshka\", device=device) # [, 1024]\n",
    "def get_Sentence_Transformer(sentence):\n",
    "    sentence_embedding = Sentence_Transformer_model.encode(sentence)\n",
    "    return sentence_embedding.flatten()\n",
    "# bge\n",
    "apply_model(get_Sentence_Transformer, 'bge_features', 1024)\n",
    "apply_model_MAEC(get_Sentence_Transformer, 'bge_features', 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a315d0",
   "metadata": {},
   "source": [
    "## 3. philschmid/bge-base-financial-matryoshka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ddde3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying bge_base_features to the original dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [09:20<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89722 entries, 0 to 89721\n",
      "Columns: 771 entries, bge_base_features_0 to Sentence_num\n",
      "dtypes: object(771)\n",
      "memory usage: 527.8+ MB\n",
      "Number of errors: 0\n",
      "[]\n",
      "Applying bge_base_features to the MAEC dataset …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [40:15<00:00,  1.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394277 entries, 0 to 394276\n",
      "Columns: 771 entries, bge_base_features_0 to Sentence_num\n",
      "dtypes: object(771)\n",
      "memory usage: 2.3+ GB\n",
      "Number of errors: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "Sentence_Transformer_model = SentenceTransformer(\"philschmid/bge-base-financial-matryoshka\", device=device) # [, 768]\n",
    "def get_Sentence_Transformer(sentence):\n",
    "    sentence_embedding = Sentence_Transformer_model.encode(sentence)\n",
    "    return sentence_embedding.flatten()\n",
    "# bge_base\n",
    "apply_model(get_Sentence_Transformer, 'bge_base_features', 768)\n",
    "apply_model_MAEC(get_Sentence_Transformer, 'bge_base_features', 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb39f2-d018-4f3f-8628-fccaa6dcceb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
