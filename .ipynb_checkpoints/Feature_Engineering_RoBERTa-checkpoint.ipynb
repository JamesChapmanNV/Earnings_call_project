{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256c016d-3a52-4525-b92f-d9eafcb7899f",
   "metadata": {},
   "source": [
    "# Earnings Call Project: RoBERTa\n",
    "<br>\n",
    "CIS 831 Deep Learning – Term Project<br>\n",
    "Kansas State University\n",
    "<br><br>\n",
    "James Chapman<br>\n",
    "John Woods<br>\n",
    "Nathan Diehl<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88afe45e-603e-4403-b191-ecd36af94ef7",
   "metadata": {},
   "source": [
    "This notebook featurizes the text and audio data from the earnings calls. Each earnings calls data comes pre-processed such that each sentence of the call corresponds to 1 line-of-text and 1 MP3 audio file. The transcript text is processed with the Glove–300 pre-trained word embedding, and the audio files are processed with Praat using parselmouth.\n",
    "- Text (Glove–300) [Glove Download](https://nlp.stanford.edu/projects/glove/)\n",
    "- Audio (Praat) [Parselmouth](https://parselmouth.readthedocs.io/en/stable/)\n",
    "\n",
    "The data from this notebook is stored in the \"data/data_prep\" directory as the following CSVs.\n",
    "- glove_features\n",
    "- MAEC_glove_features\n",
    "- praat_features\n",
    "- MAEC_praat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d13c039-9d28-49d3-a557-ccf588b46f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install transformers\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    %cd gdrive/My Drive/831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88d0789-36df-4250-90e7-1485c0dc9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5822f8a-897e-4fd5-b77b-1c38e8d7ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAEC_dir = 'data/MAEC/MAEC_Dataset' # https://github.com/Earnings-Call-Dataset/MAEC-A-Multimodal-Aligned-Earnings-Conference-Call-Dataset-for-Financial-Risk-Prediction\n",
    "\n",
    "############# too big for GitHub ########################\n",
    "############# stored on local disk ######################\n",
    "original_data_dir = r\"D:\\original_dataset\" # https://github.com/GeminiLn/EarningsCall_Dataset \n",
    "MAEC_audio_dir = r\"D:\\MAEC_audio\" \n",
    "# there is a link for the audio data in the MAEC GitHub, but it does not work\n",
    "# I emailed the authors, and they send another link.\n",
    "# There is like a half-million files, but only 19 GB\n",
    "# https://drive.google.com/file/d/1m1GRCHgKn9Vz9IFMC_SpCog6uP3-gFgY/view?usp=drive_link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7908bf-5a46-4a22-b959-acc40ce09ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the directory, each folder represents an earnings conference call; the folders are named as \"CompanyName_Date\".\n",
    "filename_data = []\n",
    "for filename in os.listdir(original_data_dir):\n",
    "    company_name, date_str = filename.rsplit('_', 1)\n",
    "    date_str = date_str.split('.')[0] \n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    filename_data.append([company_name, date])\n",
    "filename_data = pd.DataFrame(filename_data, columns=[\"Company\", \"Date\"])\n",
    "company_ticker = pd.read_csv('data/data_prep/company_ticker.csv')\n",
    "filename_data = filename_data.merge(company_ticker, on=\"Company\", how=\"left\")\n",
    "\n",
    "# Loop through the directory, each folder represents an earnings conference call; the folders are named as \"Date_CompanyName\".\n",
    "MAEC_filename_data = []\n",
    "for filename in os.listdir(MAEC_dir):\n",
    "    date_str, ticker = filename.rsplit('_', 1)\n",
    "    date_str = date_str.split('.')[0] \n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    MAEC_filename_data.append([ticker, date])\n",
    "MAEC_filename_data = pd.DataFrame(MAEC_filename_data, columns=[\"Ticker\", \"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055641c3-e617-46a5-99f0-4808ba98edcf",
   "metadata": {},
   "source": [
    "# RoBERTa features from meeting transcript text files\n",
    "\n",
    "RoBERTa documentation can be found at https://huggingface.co/FacebookAI/roberta-large\n",
    "\n",
    "### Following code is adapted FROM\n",
    "[GitHub HTML Encoder](https://github.com/YangLinyi/HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction/blob/master/Model/Token-Level%20Encoder/HuggingFace-Roberta-Token-Encoder.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21943fb0-68be-4c8b-9c29-6983cef7f8ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This is adapted from\n",
    "# https://github.com/YangLinyi/HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction/blob/master/Model/Token-Level%20Encoder/HuggingFace-Roberta-Token-Encoder.py\n",
    "model = RobertaModel.from_pretrained('roberta-large').to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "def get_RoBERTa(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # [CLS] embedding for sentence-level representation\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    #print(cls_embedding.shape)\n",
    "    # 1024 features \n",
    "    return cls_embedding\n",
    "####################################################\n",
    "####################################################\n",
    "# # How to implement average pooling instead of CLS\n",
    "#         # average pooling over token embeddings\n",
    "#         token_embeddings = outputs.last_hidden_state\n",
    "#         sentence_embedding = torch.mean(token_embeddings, dim=1).cpu().numpy()  \n",
    "#     return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a9d56-c46f-48fd-b827-d118023a974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 210/572 [18:49<1:09:29, 11.52s/it]"
     ]
    }
   ],
   "source": [
    "RoBERTa_features = pd.DataFrame()\n",
    "RoBERTa_error = []\n",
    "for Company, Date in tqdm(filename_data[['Company', 'Date']].values):\n",
    "    Date = Date.replace('-', '')\n",
    "    text_path = f\"D:/original_dataset/{Company}_{Date}/TextSequence.txt\"\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            for i, line in enumerate(file, start=1):\n",
    "                sentence_embedding = get_RoBERTa(line.strip())\n",
    "                # 1024 features \n",
    "                features_df = pd.DataFrame(sentence_embedding, columns=[f'RoBERTa_{j}' for j in range(1024)])\n",
    "                features_df['Company'] = Company\n",
    "                features_df['Date'] = Date\n",
    "                features_df['Sentence_num'] = i\n",
    "                RoBERTa_features = pd.concat([RoBERTa_features, features_df], ignore_index=True)\n",
    "    except KeyboardInterrupt: break\n",
    "    except Exception as e:\n",
    "        RoBERTa_error.append((Company, Date, str(e)))\n",
    "\n",
    "print(len(RoBERTa_error))\n",
    "print(RoBERTa_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd62746-1b5f-427b-9c20-86e03ad4f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "RoBERTa_features.info(verbose=True)\n",
    "###############################################\n",
    "RoBERTa_features.to_csv('data/data_prep/RoBERTa_features.csv', index=False)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad836b41-7243-4c4e-b3ff-bd3cebb9068a",
   "metadata": {},
   "source": [
    "# Repeat with MACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff2da0-2bf1-4074-9fef-d817f15a12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACE_RoBERTa_features = pd.DataFrame()\n",
    "MACE_RoBERTa_error = []\n",
    "for Company, Date in tqdm(MAEC_filename_data[['Ticker', 'Date']].values):\n",
    "    Date = Date.replace('-', '')\n",
    "    text_path = f\"D:/MAEC_audio/{Date}_{Ticker}/text.txt\"\n",
    "    try:\n",
    "        with open(text_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            for i, line in enumerate(file, start=1):\n",
    "                sentence_embedding = get_RoBERTa(line.strip())\n",
    "                # 1024 features \n",
    "                features_df = pd.DataFrame(sentence_embedding, columns=[f'RoBERTa_{j}' for j in range(1024)])\n",
    "                features_df['Ticker'] = Ticker\n",
    "                features_df['Date'] = Date\n",
    "                features_df['Sentence_num'] = i\n",
    "                MACE_RoBERTa_features = pd.concat([MACE_RoBERTa_features, features_df], ignore_index=True)\n",
    "    except KeyboardInterrupt: break\n",
    "    except Exception as e:\n",
    "        MACE_RoBERTa_error.append((Company, Date, str(e)))\n",
    "\n",
    "print(len(MACE_RoBERTa_error))\n",
    "print(MACE_RoBERTa_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654795bb-1eaf-41af-9450-9d9572f12bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACE_RoBERTa_features.info(verbose=True)\n",
    "###############################################\n",
    "MACE_RoBERTa_features.to_csv('data/data_prep/MACE_RoBERTa_features.csv', index=False)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c2c77-b4f5-41b9-a989-839cab1494e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245581e7-3eed-46a8-a7c0-4e949f4c026b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b3d34-1ce1-41b6-9b19-906869aadd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb39f2-d018-4f3f-8628-fccaa6dcceb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
