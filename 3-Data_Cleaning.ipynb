{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d912700f-d412-4827-bf51-73285938ce34",
   "metadata": {},
   "source": [
    "# Earnings Call Project: Data Cleaning\n",
    "<br>\n",
    "CIS 831 Deep Learning – Term Project<br>\n",
    "Kansas State University\n",
    "<br><br>\n",
    "James Chapman<br>\n",
    "John Woods<br>\n",
    "Nathan Diehl<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d09ba3-32b9-47a0-a079-1375b22e3afe",
   "metadata": {},
   "source": [
    "This notebook creates data used for training/testing.\n",
    "- Calculates the targets for both datasets\n",
    "- Corrects Praat features for both datasets\n",
    "- Combines features audio (Praat) & text (6 different) and targets\n",
    "- save 9 numpy files specifically for HTML (for each audio/text pair)\n",
    "    - train (features, targets, secondary_targets)\n",
    "    - validation (features, targets, secondary_targets)\n",
    "    - test (features, targets, secondary_targets)\n",
    "- it is important to note 12 meetings were removed because we could not find stock data (Yahoo or alphadvantage)<br>\n",
    "- 218 meetings were removed from MAEC <br>\n",
    "\n",
    "The rest data from this notebook is stored in the \"data\" directory as the following CSVs (for each audio/text pair)\n",
    "- MAEC_dataset\n",
    "- MAEC_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88d0789-36df-4250-90e7-1485c0dc9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5822f8a-897e-4fd5-b77b-1c38e8d7ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAEC_dir = 'data/MAEC/MAEC_Dataset' # https://github.com/Earnings-Call-Dataset/MAEC-A-Multimodal-Aligned-Earnings-Conference-Call-Dataset-for-Financial-Risk-Prediction\n",
    "\n",
    "############# too big for GitHub ########################\n",
    "############# stored on local disk ######################\n",
    "original_data_dir = r\"D:\\original_dataset\" # https://github.com/GeminiLn/EarningsCall_Dataset \n",
    "MAEC_audio_dir = r\"D:\\MAEC_audio\" \n",
    "# there is a link for the audio data in the MAEC GitHub, but it does not work\n",
    "# I emailed the authors, and they send another link.\n",
    "# There is like a half-million files, but only 19 GB\n",
    "# https://drive.google.com/file/d/1m1GRCHgKn9Vz9IFMC_SpCog6uP3-gFgY/view?usp=drive_link \n",
    "\n",
    "# from Webscraping\n",
    "alpha_dir = 'data/data_prep/alpha_data/{}.csv' #.format(ticker) # I saved the raw alphadvantage data, so I don't have to do it again\n",
    "yahoo_data = pd.read_csv('data/data_prep/yahoo_data.csv', index_col=0)\n",
    "alpha_data = pd.read_csv('data/data_prep/alpha_data.csv', index_col=0)\n",
    "MAEC_yahoo_data = pd.read_csv('data/data_prep/MAEC_yahoo_data.csv', index_col=0)\n",
    "MAEC_alpha_data = pd.read_csv('data/data_prep/MAEC_alpha_data.csv', index_col=0)\n",
    "\n",
    "yahoo_data.index = pd.to_datetime(yahoo_data.index)\n",
    "alpha_data.index = pd.to_datetime(alpha_data.index)\n",
    "MAEC_yahoo_data.index = pd.to_datetime(MAEC_yahoo_data.index)\n",
    "MAEC_alpha_data.index = pd.to_datetime(MAEC_alpha_data.index)\n",
    "\n",
    "# from Feature Engineering\n",
    "glove_features = pd.read_csv('data/data_prep/glove_features.csv')\n",
    "praat_features = pd.read_csv('data/data_prep/praat_features.csv', low_memory=False)\n",
    "MAEC_glove_features = pd.read_csv('data/data_prep/MAEC_glove_features.csv')\n",
    "MAEC_praat_features = pd.read_csv('data/data_prep/MAEC_praat_features.csv', low_memory=False)\n",
    "\n",
    "# from Feature Engineering MORE\n",
    "RoBERTa_features = pd.read_csv('data/data_prep/RoBERTa_features.csv', low_memory=False)\n",
    "MAEC_RoBERTa_features = pd.read_csv('data/data_prep/MAEC_RoBERTa_features.csv', low_memory=False)\n",
    "# Roberta with averages\n",
    "RoBERTa_features2 = pd.read_csv('data/data_prep/RoBERTa_features2.csv', low_memory=False)\n",
    "MAEC_RoBERTa_features2 = pd.read_csv('data/data_prep/MAEC_RoBERTa_features2.csv', low_memory=False)\n",
    "\n",
    "investopedia_features = pd.read_csv('data/data_prep/investopedia_features.csv', low_memory=False)\n",
    "MAEC_investopedia_features = pd.read_csv('data/data_prep/MAEC_investopedia_features.csv', low_memory=False)\n",
    "bge_features = pd.read_csv('data/data_prep/bge_features.csv', low_memory=False)\n",
    "MAEC_bge_features = pd.read_csv('data/data_prep/MAEC_bge_features.csv', low_memory=False)\n",
    "bge_base_features = pd.read_csv('data/data_prep/bge_base_features.csv', low_memory=False)\n",
    "MAEC_bge_base_features = pd.read_csv('data/data_prep/MAEC_bge_base_features.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fd74f4-2546-4c1f-8939-206d43abdf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the directory, each folder represents an earnings conference call; the folders are named as \"CompanyName_Date\".\n",
    "filename_data = []\n",
    "for filename in os.listdir(original_data_dir):\n",
    "    company_name, date_str = filename.rsplit('_', 1)\n",
    "    date_str = date_str.split('.')[0] \n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    filename_data.append([company_name, date])\n",
    "filename_data = pd.DataFrame(filename_data, columns=[\"Company\", \"Date\"])\n",
    "company_ticker = pd.read_csv('data/data_prep/company_ticker.csv')\n",
    "filename_data = filename_data.merge(company_ticker, on=\"Company\", how=\"left\")\n",
    "\n",
    "# Loop through the directory, each folder represents an earnings conference call; the folders are named as \"Date_CompanyName\".\n",
    "MAEC_filename_data = []\n",
    "for filename in os.listdir(MAEC_dir):\n",
    "    date_str, ticker = filename.rsplit('_', 1)\n",
    "    date_str = date_str.split('.')[0] \n",
    "    date = datetime.strptime(date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    MAEC_filename_data.append([ticker, date])\n",
    "MAEC_filename_data = pd.DataFrame(MAEC_filename_data, columns=[\"Ticker\", \"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb88572-dfc3-4e8f-80c3-9319683e4262",
   "metadata": {},
   "source": [
    "# Add TARGET of the regression\n",
    "\n",
    "**n-day volatility predictions**: The predicted average volatility over the following n days.<br>\n",
    "\n",
    "$$\n",
    "v[0,n] = \\ln \\left( \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (r_i - \\bar{r})^2 } \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( r_i \\) is the stock return on day \\(i\\),\n",
    "- \\( \\bar{r} \\) is the average stock return over \\(n\\) days.\n",
    "\n",
    "The stock return \\(r_i\\) is defined as:\n",
    "\n",
    "$$\n",
    "r_i = \\frac{P_i - P_{i-1}}{P_{i-1}}\n",
    "$$\n",
    "\n",
    "Where \\(P_i\\) is the adjusted closing price of the stock on day \\(i\\).\n",
    "\n",
    "For **single-day log volatility**, we estimate it using the **daily log absolute return**:\n",
    "\n",
    "$$\n",
    "v_n = \\ln \\left( \\left| \\frac{P_n - P_{n-1}}{P_{n-1}} \\right| \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(P_n\\) is the adjusted closing price of the stock on day \\(n\\),\n",
    "- \\(P_{n-1}\\) is the adjusted closing price on the previous day.\n",
    "\n",
    "Our multi-task learning objective is to **simultaneously predict** these two quantities:\n",
    "- \\(v[0,n]\\): The average volatility over \\(n\\) days (the main task).\n",
    "- \\(v_n\\): The single-day volatility (the auxiliary task).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f118fbe1-9457-4d6e-9577-9702da80bda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/572 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:00<00:00, 1846.13it/s]\n",
      "100%|██████████| 572/572 [00:00<00:00, 2142.25it/s]\n",
      "100%|██████████| 572/572 [00:00<00:00, 2179.19it/s]\n",
      "100%|██████████| 572/572 [00:00<00:00, 2148.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 572 entries, 0 to 571\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Company        572 non-null    object \n",
      " 1   Date           572 non-null    object \n",
      " 2   Ticker         572 non-null    object \n",
      " 3   3_day          572 non-null    float64\n",
      " 4   3_day_single   572 non-null    float64\n",
      " 5   7_day          572 non-null    float64\n",
      " 6   7_day_single   572 non-null    float64\n",
      " 7   15_day         572 non-null    float64\n",
      " 8   15_day_single  572 non-null    float64\n",
      " 9   30_day         572 non-null    float64\n",
      " 10  30_day_single  572 non-null    float64\n",
      "dtypes: float64(8), object(3)\n",
      "memory usage: 49.3+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "targets = filename_data.copy()\n",
    "\n",
    "# Yahoo (missing companies)\n",
    "def add_n_day(row, n_day):\n",
    "    Ticker = row['Ticker']\n",
    "    if Ticker in ['GGP', 'CA', 'STI', 'FLT', 'NLSN', 'WRK','RTN', 'UTX', 'DISH', 'VIACOM', \n",
    "                     'TIF', 'CELG', 'RHT', 'FLIR', 'AGN', 'KSU', 'NBL', 'ALXN', 'ABMD ', 'CTXS', \n",
    "                     'CBS', 'XL', 'VAR', 'ATVI', 'XLNX', 'SIVB']:\n",
    "        return float('inf'), float('inf')\n",
    "    Date = pd.to_datetime(row['Date'])\n",
    "    Date_index = yahoo_data.index.get_loc(Date)\n",
    "    data = yahoo_data.iloc[Date_index:(Date_index + 1 + n_day)][f\"{Ticker}_Adj Close\"]  \n",
    "    #data = yahoo_data.iloc[(Date_index-1):(Date_index + n_day)][f\"{Ticker}_Adj Close\"]  \n",
    "    stock_return = data.pct_change(fill_method=None).dropna() # ri =(Pi − Pi−1)/Pi−1\n",
    "    std_dev = np.std(stock_return)\n",
    "    if pd.isna(std_dev) or std_dev == 0:\n",
    "        return 0, 0\n",
    "    else:\n",
    "        return np.log(std_dev), np.log(abs(stock_return.iloc[-1]) + 1e-10) #log 0 is undefined\n",
    "\n",
    "targets['3_day'], targets['3_day_single'] = zip(*targets.progress_apply(lambda row: add_n_day(row, 3), axis=1))\n",
    "targets['7_day'], targets['7_day_single'] = zip(*targets.progress_apply(lambda row: add_n_day(row, 7), axis=1))\n",
    "targets['15_day'], targets['15_day_single'] = zip(*targets.progress_apply(lambda row: add_n_day(row, 15), axis=1))\n",
    "targets['30_day'], targets['30_day_single'] = zip(*targets.progress_apply(lambda row: add_n_day(row, 30), axis=1))\n",
    "targets.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04495ec8-544b-434a-bb6f-d9e11e3a7ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:00<00:00, 1908.42it/s]\n",
      "100%|██████████| 572/572 [00:00<00:00, 1958.80it/s]\n",
      "100%|██████████| 572/572 [00:00<00:00, 1991.09it/s]\n",
      "100%|██████████| 572/572 [00:00<00:00, 2026.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# using alphadvantage \n",
    "def add_n_day(row, n_day):\n",
    "    Ticker = row['Ticker'].strip()\n",
    "    if Ticker in ['FLT', 'VIACOM', 'BA', 'BIIB', 'BDX', 'ABMD', 'CBS']:\n",
    "        return 0, 0\n",
    "    Date = pd.to_datetime(row['Date'])\n",
    "    Date_index = alpha_data.index.get_loc(Date)\n",
    "    data = alpha_data.iloc[Date_index:(Date_index + 1 + n_day)][Ticker]  \n",
    "    #data = alpha_data.iloc[(Date_index-1):(Date_index + n_day)][Ticker]  \n",
    "    stock_return = data.pct_change(fill_method=None).dropna() # ri =(Pi − Pi−1)/Pi−1\n",
    "    std_dev = np.std(stock_return)\n",
    "    if pd.isna(std_dev) or std_dev == 0:\n",
    "        return 0, 0\n",
    "    else:\n",
    "        return np.log(std_dev), np.log(abs(stock_return.iloc[-1]) + 1e-10)\n",
    "\n",
    "targets['3_day_alpha'], targets['3_day_single_alpha'] = zip(*targets.progress_apply(lambda row: add_n_day(row, 3), axis=1))\n",
    "targets['7_day_alpha'], targets['7_day_single_alpha'] = zip(*targets.progress_apply(lambda row: add_n_day(row, 7), axis=1))\n",
    "targets['15_day_alpha'], targets['15_day_single_alpha'] = zip(*targets.progress_apply(lambda row: add_n_day(row, 15), axis=1))\n",
    "targets['30_day_alpha'], targets['30_day_single_alpha'] = zip(*targets.progress_apply(lambda row: add_n_day(row, 30), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947d878-2782-42f3-b664-4a07a8e25e3b",
   "metadata": {},
   "source": [
    "Here I am comparing the Yahoo data to alphadvantage. Why is there a difference? How does each calculate dividend adjustment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "060f432d-d037-45e6-9d47-dc4d91a4adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets['3_day_diff'] = targets['3_day_alpha'] - targets['3_day']\n",
    "targets['7_day_diff'] = targets['7_day_alpha'] - targets['7_day']\n",
    "targets['15_day_diff'] = targets['15_day_alpha'] - targets['15_day']\n",
    "targets['30_day_diff'] = targets['30_day_alpha'] - targets['30_day']\n",
    "\n",
    "targets['3_day_pct_change'] = abs(((targets['3_day_alpha'] - targets['3_day']) / targets['3_day_alpha']) * 100)\n",
    "targets['7_day_pct_change'] = abs(((targets['7_day_alpha'] - targets['7_day']) / targets['7_day_alpha']) * 100)\n",
    "targets['15_day_pct_change'] = abs(((targets['15_day_alpha'] - targets['15_day']) / targets['15_day_alpha']) * 100)\n",
    "targets['30_day_pct_change'] = abs(((targets['30_day_alpha'] - targets['30_day']) / targets['30_day_alpha']) * 100)\n",
    " \n",
    "targets['3_day_single_diff'] = targets['3_day_single_alpha'] - targets['3_day_single']\n",
    "targets['7_day_single_diff'] = targets['7_day_single_alpha'] - targets['7_day_single']\n",
    "targets['15_day_single_diff'] = targets['15_day_single_alpha'] - targets['15_day_single']\n",
    "targets['30_day_single_diff'] = targets['30_day_single_alpha'] - targets['30_day_single']\n",
    "\n",
    "targets['3_day_single_pct_change'] = abs(((targets['3_day_single_alpha'] - targets['3_day_single']) / targets['3_day_single_alpha']) * 100)\n",
    "targets['7_day_single_pct_change'] = abs(((targets['7_day_single_alpha'] - targets['7_day_single']) / targets['7_day_single_alpha']) * 100)\n",
    "targets['15_day_single_pct_change'] = abs(((targets['15_day_single_alpha'] - targets['15_day_single']) / targets['15_day_single_alpha']) * 100)\n",
    "targets['30_day_single_pct_change'] = abs(((targets['30_day_single_alpha'] - targets['30_day_single']) / targets['30_day_single_alpha']) * 100)\n",
    "\n",
    "# investigate discrepancies\n",
    "targets = targets.sort_values(by='30_day_pct_change', ascending=False)\n",
    "targets = targets.sort_values(by='30_day_single_pct_change', ascending=False)\n",
    "targets.to_csv('data/data_prep/temp.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa19a771-e55c-41bb-bc13-291c13541700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows without data- 12\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Change Yahoo errors to the values found with alphadvantage \n",
    "targets['3_day_single'] = np.where(targets['3_day'] == float('inf'), targets['3_day_single_alpha'], targets['3_day_single'])\n",
    "targets['7_day_single'] = np.where(targets['7_day'] == float('inf'), targets['7_day_single_alpha'], targets['7_day_single'])\n",
    "targets['15_day_single'] = np.where(targets['15_day'] == float('inf'), targets['15_day_single_alpha'], targets['15_day_single'])\n",
    "targets['30_day_single'] = np.where(targets['30_day'] == float('inf'), targets['30_day_single_alpha'], targets['30_day_single'])\n",
    "targets['3_day'] = np.where(targets['3_day'] == float('inf'), targets['3_day_alpha'], targets['3_day'])\n",
    "targets['7_day'] = np.where(targets['7_day'] == float('inf'), targets['7_day_alpha'], targets['7_day'])\n",
    "targets['15_day'] = np.where(targets['15_day'] == float('inf'), targets['15_day_alpha'], targets['15_day'])\n",
    "targets['30_day'] = np.where(targets['30_day'] == float('inf'), targets['30_day_alpha'], targets['30_day'])\n",
    "\n",
    "targets['3_day_single'] = np.where(targets['3_day'] == 0, targets['3_day_single_alpha'], targets['3_day_single'])\n",
    "targets['7_day_single'] = np.where(targets['7_day'] == 0, targets['7_day_single_alpha'], targets['7_day_single'])\n",
    "targets['15_day_single'] = np.where(targets['15_day'] == 0, targets['15_day_single_alpha'], targets['15_day_single'])\n",
    "targets['30_day_single'] = np.where(targets['30_day'] == 0, targets['30_day_single_alpha'], targets['30_day_single'])\n",
    "targets['3_day'] = np.where(targets['3_day'] == 0, targets['3_day_alpha'], targets['3_day'])\n",
    "targets['7_day'] = np.where(targets['7_day'] == 0, targets['7_day_alpha'], targets['7_day'])\n",
    "targets['15_day'] = np.where(targets['15_day'] == 0, targets['15_day_alpha'], targets['15_day'])\n",
    "targets['30_day'] = np.where(targets['30_day'] == 0, targets['30_day_alpha'], targets['30_day'])\n",
    "\n",
    "print('Number of rows without data-', len(targets[((targets['3_day'] == 0) & \n",
    "                                                    (targets['7_day'] == 0) & \n",
    "                                                    (targets['15_day'] == 0) & \n",
    "                                                    (targets['30_day'] == 0))]))\n",
    "print('----------------------------------------------------------------------')\n",
    "\n",
    "# delete?\n",
    "# targets = targets[~((targets['3_day'] == 0) & \n",
    "#                 (targets['7_day'] == 0) & \n",
    "#                 (targets['15_day'] == 0) & \n",
    "#                 (targets['30_day'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b73cd1d1-a779-4605-a735-64769dcff056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 572 entries, 214 to 273\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Company        572 non-null    object \n",
      " 1   Date           572 non-null    object \n",
      " 2   Ticker         572 non-null    object \n",
      " 3   3_day          572 non-null    float64\n",
      " 4   3_day_single   572 non-null    float64\n",
      " 5   7_day          572 non-null    float64\n",
      " 6   7_day_single   572 non-null    float64\n",
      " 7   15_day         572 non-null    float64\n",
      " 8   15_day_single  572 non-null    float64\n",
      " 9   30_day         572 non-null    float64\n",
      " 10  30_day_single  572 non-null    float64\n",
      "dtypes: float64(8), object(3)\n",
      "memory usage: 53.6+ KB\n"
     ]
    }
   ],
   "source": [
    "targets = targets.drop(['3_day_alpha', '3_day_single_alpha', '7_day_alpha', '7_day_single_alpha', '15_day_alpha', \n",
    "                        '15_day_single_alpha', '30_day_alpha', '30_day_single_alpha', '3_day_diff', '7_day_diff', \n",
    "                        '15_day_diff', '30_day_diff', '3_day_pct_change', '7_day_pct_change', '15_day_pct_change', \n",
    "                        '30_day_pct_change', '3_day_single_diff', '7_day_single_diff', '15_day_single_diff', \n",
    "                        '30_day_single_diff', '3_day_single_pct_change', '7_day_single_pct_change', \n",
    "                        '15_day_single_pct_change', '30_day_single_pct_change'], axis=1)\n",
    "targets = targets.sort_values(by='3_day', ascending=True)\n",
    "targets.info(verbose=True)\n",
    "### save ############################################\n",
    "targets.to_csv('data/data_prep/targets.csv', index=False)\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e0d33-2ccc-4514-bf09-84801b7b79f7",
   "metadata": {},
   "source": [
    "# MAEC dataset TARGETS\n",
    "\n",
    "[paper](https://dl.acm.org/doi/10.1145/3340531.3412879)\n",
    "[GitHub](https://github.com/Earnings-Call-Dataset/MAEC-A-Multimodal-Aligned-Earnings-Conference-Call-Dataset-for-Financial-Risk-Prediction/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "999f7900-8799-4b0c-aa4e-54741eeb61e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [00:01<00:00, 1941.30it/s]\n",
      "100%|██████████| 3443/3443 [00:01<00:00, 1815.20it/s]\n",
      "100%|██████████| 3443/3443 [00:01<00:00, 1957.53it/s]\n",
      "100%|██████████| 3443/3443 [00:01<00:00, 1977.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3443 entries, 0 to 3442\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Ticker         3443 non-null   object \n",
      " 1   Date           3443 non-null   object \n",
      " 2   3_day          3443 non-null   float64\n",
      " 3   3_day_single   3443 non-null   float64\n",
      " 4   7_day          3443 non-null   float64\n",
      " 5   7_day_single   3443 non-null   float64\n",
      " 6   15_day         3443 non-null   float64\n",
      " 7   15_day_single  3443 non-null   float64\n",
      " 8   30_day         3443 non-null   float64\n",
      " 9   30_day_single  3443 non-null   float64\n",
      "dtypes: float64(8), object(2)\n",
      "memory usage: 269.1+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAEC_targets = MAEC_filename_data.copy()\n",
    "\n",
    "# Yahoo (missing 9 companies)\n",
    "def add_n_day(row, n_day):\n",
    "    Ticker = row['Ticker']\n",
    "    Date = pd.to_datetime(row['Date'])\n",
    "    Date_index = MAEC_yahoo_data.index.get_loc(Date)\n",
    "    data = MAEC_yahoo_data.iloc[Date_index:(Date_index + 1 + n_day)][f\"{Ticker}_Adj Close\"]  \n",
    "    # data = MAEC_yahoo_data.iloc[(Date_index-1):(Date_index + n_day)][f\"{Ticker}_Adj Close\"]  \n",
    "    stock_return = data.pct_change(fill_method=None).dropna() # ri =(Pi − Pi−1)/Pi−1\n",
    "    std_dev = np.std(stock_return)\n",
    "    if pd.isna(std_dev) or std_dev == 0:\n",
    "        return 0, 0\n",
    "    else:\n",
    "        return np.log(std_dev), np.log(abs(stock_return.iloc[-1]) + 1e-10)\n",
    "\n",
    "MAEC_targets['3_day'], MAEC_targets['3_day_single'] = zip(*MAEC_targets.progress_apply(lambda row: add_n_day(row, 3), axis=1))\n",
    "MAEC_targets['7_day'], MAEC_targets['7_day_single'] = zip(*MAEC_targets.progress_apply(lambda row: add_n_day(row, 7), axis=1))\n",
    "MAEC_targets['15_day'], MAEC_targets['15_day_single'] = zip(*MAEC_targets.progress_apply(lambda row: add_n_day(row, 15), axis=1))\n",
    "MAEC_targets['30_day'], MAEC_targets['30_day_single'] = zip(*MAEC_targets.progress_apply(lambda row: add_n_day(row, 30), axis=1))\n",
    "MAEC_targets.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e774ff-d9aa-4a07-bf75-d481b7ecac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [00:01<00:00, 2023.78it/s]\n",
      "100%|██████████| 3443/3443 [00:01<00:00, 2033.75it/s]\n",
      "100%|██████████| 3443/3443 [00:01<00:00, 2036.40it/s]\n",
      "100%|██████████| 3443/3443 [00:01<00:00, 2055.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# 58 tickers that did not return data from alphadvantage\n",
    "bad_tickers = ['GPS', 'JCP', 'TUP', 'BBT', 'MDP', 'LL', 'ABC', 'PKI', 'HFC', 'HSC', 'CBB', 'ILG', 'JCOM', 'EBIX', 'ENDP', \n",
    "               'BIG', 'ASNA', 'IVC', 'BCOR', 'INT', 'FRED', 'CAMP', 'ELY', 'COG', 'CLD', 'CRY', 'PLT', 'FRAN', 'ADS', 'CPSI', \n",
    "               'FBHS', 'CHFC', 'UIHC', 'OFC', 'TMST', 'FTD', 'SWM', 'WPG', 'WLTW', 'AKRX', 'BLL', 'DF', 'TLRD', 'SPN', 'CLI', \n",
    "               'ESV', 'RCII', 'ANTM', 'RE', 'NCR', 'NEWM', 'PEI', 'LCI', 'ERA', 'ACOR', 'FB', 'AAXN', 'NLS']\n",
    "def add_n_day(row, n_day):\n",
    "    Ticker = row['Ticker']\n",
    "    if Ticker in bad_tickers:\n",
    "        return float('inf'), float('inf')\n",
    "    Date = pd.to_datetime(row['Date'])\n",
    "    Date_index = MAEC_alpha_data.index.get_loc(Date)\n",
    "    data = MAEC_alpha_data.iloc[Date_index:(Date_index + 1 + n_day)][Ticker]  \n",
    "    # data = MAEC_alpha_data.iloc[(Date_index-1):(Date_index + n_day)][Ticker]  \n",
    "    stock_return = data.pct_change(fill_method=None).dropna() # ri =(Pi − Pi−1)/Pi−1\n",
    "    std_dev = np.std(stock_return)\n",
    "    if pd.isna(std_dev) or std_dev == 0:\n",
    "        return 0, 0\n",
    "    else:\n",
    "        return np.log(std_dev), np.log(abs(stock_return.iloc[-1]) + 1e-10)\n",
    "\n",
    "MAEC_targets['3_day_alpha'], MAEC_targets['3_day_single_alpha'] = zip(*MAEC_targets.progress_apply(lambda row: add_n_day(row, 3), axis=1))\n",
    "MAEC_targets['7_day_alpha'], MAEC_targets['7_day_single_alpha'] = zip(*MAEC_targets.progress_apply(lambda row: add_n_day(row, 7), axis=1))\n",
    "MAEC_targets['15_day_alpha'], MAEC_targets['15_day_single_alpha'] = zip(*MAEC_targets.progress_apply(lambda row: add_n_day(row, 15), axis=1))\n",
    "MAEC_targets['30_day_alpha'], MAEC_targets['30_day_single_alpha'] = zip(*MAEC_targets.progress_apply(lambda row: add_n_day(row, 30), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "076548b0-9b2c-4099-aed2-9c0e070e70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAEC_targets['3_day_diff'] = MAEC_targets['3_day_alpha'] - MAEC_targets['3_day']\n",
    "MAEC_targets['7_day_diff'] = MAEC_targets['7_day_alpha'] - MAEC_targets['7_day']\n",
    "MAEC_targets['15_day_diff'] = MAEC_targets['15_day_alpha'] - MAEC_targets['15_day']\n",
    "MAEC_targets['30_day_diff'] = MAEC_targets['30_day_alpha'] - MAEC_targets['30_day']\n",
    "\n",
    "MAEC_targets['3_day_pct_change'] = abs(((MAEC_targets['3_day_alpha'] - MAEC_targets['3_day']) / MAEC_targets['3_day_alpha']) * 100)\n",
    "MAEC_targets['7_day_pct_change'] = abs(((MAEC_targets['7_day_alpha'] - MAEC_targets['7_day']) / MAEC_targets['7_day_alpha']) * 100)\n",
    "MAEC_targets['15_day_pct_change'] = abs(((MAEC_targets['15_day_alpha'] - MAEC_targets['15_day']) / MAEC_targets['15_day_alpha']) * 100)\n",
    "MAEC_targets['30_day_pct_change'] = abs(((MAEC_targets['30_day_alpha'] - MAEC_targets['30_day']) / MAEC_targets['30_day_alpha']) * 100)\n",
    " \n",
    "MAEC_targets['3_day_single_diff'] = MAEC_targets['3_day_single_alpha'] - MAEC_targets['3_day_single']\n",
    "MAEC_targets['7_day_single_diff'] = MAEC_targets['7_day_single_alpha'] - MAEC_targets['7_day_single']\n",
    "MAEC_targets['15_day_single_diff'] = MAEC_targets['15_day_single_alpha'] - MAEC_targets['15_day_single']\n",
    "MAEC_targets['30_day_single_diff'] = MAEC_targets['30_day_single_alpha'] - MAEC_targets['30_day_single']\n",
    "\n",
    "MAEC_targets['3_day_single_pct_change'] = abs(((MAEC_targets['3_day_single_alpha'] - MAEC_targets['3_day_single']) / MAEC_targets['3_day_single_alpha']) * 100)\n",
    "MAEC_targets['7_day_single_pct_change'] = abs(((MAEC_targets['7_day_single_alpha'] - MAEC_targets['7_day_single']) / MAEC_targets['7_day_single_alpha']) * 100)\n",
    "MAEC_targets['15_day_single_pct_change'] = abs(((MAEC_targets['15_day_single_alpha'] - MAEC_targets['15_day_single']) / MAEC_targets['15_day_single_alpha']) * 100)\n",
    "MAEC_targets['30_day_single_pct_change'] = abs(((MAEC_targets['30_day_single_alpha'] - MAEC_targets['30_day_single']) / MAEC_targets['30_day_single_alpha']) * 100)\n",
    "# investigate discrepancies\n",
    "MAEC_targets = MAEC_targets.sort_values(by='30_day_pct_change', ascending=False)\n",
    "MAEC_targets.to_csv('data/data_prep/MAEC_temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46c905c2-43d7-4081-a688-cd474102763f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows without data- 218\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Change Yahoo errors to the values found with alphadvantage \n",
    "MAEC_targets['3_day_single'] = np.where(MAEC_targets['3_day'] == float('inf'), MAEC_targets['3_day_single_alpha'], MAEC_targets['3_day_single'])\n",
    "MAEC_targets['7_day_single'] = np.where(MAEC_targets['7_day'] == float('inf'), MAEC_targets['7_day_single_alpha'], MAEC_targets['7_day_single'])\n",
    "MAEC_targets['15_day_single'] = np.where(MAEC_targets['15_day'] == float('inf'), MAEC_targets['15_day_single_alpha'], MAEC_targets['15_day_single'])\n",
    "MAEC_targets['30_day_single'] = np.where(MAEC_targets['30_day'] == float('inf'), MAEC_targets['30_day_single_alpha'], MAEC_targets['30_day_single'])\n",
    "MAEC_targets['3_day'] = np.where(MAEC_targets['3_day'] == float('inf'), MAEC_targets['3_day_alpha'], MAEC_targets['3_day'])\n",
    "MAEC_targets['7_day'] = np.where(MAEC_targets['7_day'] == float('inf'), MAEC_targets['7_day_alpha'], MAEC_targets['7_day'])\n",
    "MAEC_targets['15_day'] = np.where(MAEC_targets['15_day'] == float('inf'), MAEC_targets['15_day_alpha'], MAEC_targets['15_day'])\n",
    "MAEC_targets['30_day'] = np.where(MAEC_targets['30_day'] == float('inf'), MAEC_targets['30_day_alpha'], MAEC_targets['30_day'])\n",
    "\n",
    "MAEC_targets['3_day_single'] = np.where(MAEC_targets['3_day'] == 0, MAEC_targets['3_day_single_alpha'], MAEC_targets['3_day_single'])\n",
    "MAEC_targets['7_day_single'] = np.where(MAEC_targets['7_day'] == 0, MAEC_targets['7_day_single_alpha'], MAEC_targets['7_day_single'])\n",
    "MAEC_targets['15_day_single'] = np.where(MAEC_targets['15_day'] == 0, MAEC_targets['15_day_single_alpha'], MAEC_targets['15_day_single'])\n",
    "MAEC_targets['30_day_single'] = np.where(MAEC_targets['30_day'] == 0, MAEC_targets['30_day_single_alpha'], MAEC_targets['30_day_single'])\n",
    "MAEC_targets['3_day'] = np.where(MAEC_targets['3_day'] == 0, MAEC_targets['3_day_alpha'], MAEC_targets['3_day'])\n",
    "MAEC_targets['7_day'] = np.where(MAEC_targets['7_day'] == 0, MAEC_targets['7_day_alpha'], MAEC_targets['7_day'])\n",
    "MAEC_targets['15_day'] = np.where(MAEC_targets['15_day'] == 0, MAEC_targets['15_day_alpha'], MAEC_targets['15_day'])\n",
    "MAEC_targets['30_day'] = np.where(MAEC_targets['30_day'] == 0, MAEC_targets['30_day_alpha'], MAEC_targets['30_day'])\n",
    "\n",
    "MAEC_targets.replace([float('inf'), -float('inf')], 0, inplace=True)\n",
    "print('Number of rows without data-', len(MAEC_targets[((MAEC_targets['3_day'] == 0) & \n",
    "                                                        (MAEC_targets['7_day'] == 0) & \n",
    "                                                        (MAEC_targets['15_day'] == 0)& \n",
    "                                                        (MAEC_targets['30_day'] == 0))]))\n",
    "print('----------------------------------------------------------------------')\n",
    "\n",
    "# delete?\n",
    "# MAEC_targets = MAEC_targets[~((MAEC_targets['3_day'] == 0) & \n",
    "#                 (MAEC_targets['7_day'] == 0) & \n",
    "#                 (MAEC_targets['15_day'] == 0) & \n",
    "#                 (MAEC_targets['30_day'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9464d1bb-a81b-44bf-bdac-bd455efe8580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3443 entries, 640 to 3441\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Ticker         3443 non-null   object \n",
      " 1   Date           3443 non-null   object \n",
      " 2   3_day          3443 non-null   float64\n",
      " 3   3_day_single   3443 non-null   float64\n",
      " 4   7_day          3443 non-null   float64\n",
      " 5   7_day_single   3443 non-null   float64\n",
      " 6   15_day         3443 non-null   float64\n",
      " 7   15_day_single  3443 non-null   float64\n",
      " 8   30_day         3443 non-null   float64\n",
      " 9   30_day_single  3443 non-null   float64\n",
      "dtypes: float64(8), object(2)\n",
      "memory usage: 295.9+ KB\n"
     ]
    }
   ],
   "source": [
    "MAEC_targets = MAEC_targets.drop(['3_day_alpha', '3_day_single_alpha', '7_day_alpha', '7_day_single_alpha', '15_day_alpha', \n",
    "                        '15_day_single_alpha', '30_day_alpha', '30_day_single_alpha', '3_day_diff', '7_day_diff', \n",
    "                        '15_day_diff', '30_day_diff', '3_day_pct_change', '7_day_pct_change', '15_day_pct_change', \n",
    "                        '30_day_pct_change', '3_day_single_diff', '7_day_single_diff', '15_day_single_diff', \n",
    "                        '30_day_single_diff', '3_day_single_pct_change', '7_day_single_pct_change', \n",
    "                        '15_day_single_pct_change', '30_day_single_pct_change'], axis=1)\n",
    "MAEC_targets = MAEC_targets.sort_values(by='7_day', ascending=True)\n",
    "MAEC_targets.info(verbose=True)\n",
    "### save ############################################\n",
    "MAEC_targets.to_csv('data/data_prep/MAEC_targets.csv', index=False)\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc663d1f-3ead-4e9f-b9a3-d96522e03012",
   "metadata": {},
   "source": [
    "# Clean Praat features\n",
    "\n",
    "- Replace '--undefined--' data\n",
    "- Convert to numeric float64\n",
    "- Compare our Praat data (MAEC), which was calculated from MP3 files.\n",
    "    - To the provided Praat data (MAEC) [GitHub](https://github.com/Earnings-Call-Dataset/MAEC-A-Multimodal-Aligned-Earnings-Conference-Call-Dataset-for-Financial-Risk-Prediction/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a5c76d-47ab-437b-8e0f-7068d5d68856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 76.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 716.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89722 entries, 0 to 89721\n",
      "Data columns (total 33 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Mean pitch                    89722 non-null  float64\n",
      " 1   Standard deviation            89722 non-null  float64\n",
      " 2   Minimum pitch                 89722 non-null  float64\n",
      " 3   Maximum pitch                 89722 non-null  float64\n",
      " 4   Number of pulses              89722 non-null  float64\n",
      " 5   Number of periods             89722 non-null  float64\n",
      " 6   Mean period                   89722 non-null  float64\n",
      " 7   Mean intensity                89722 non-null  float64\n",
      " 8   Minimum intensity             89722 non-null  float64\n",
      " 9   Maximum intensity             89722 non-null  float64\n",
      " 10  Standard deviation of period  89722 non-null  float64\n",
      " 11  Fraction of unvoiced          89722 non-null  float64\n",
      " 12  Number of voice breaks        89722 non-null  float64\n",
      " 13  Degree of voice breaks        89722 non-null  float64\n",
      " 14  Jitter local                  89722 non-null  float64\n",
      " 15  Jitter local absolute         89722 non-null  float64\n",
      " 16  Jitter rap                    89722 non-null  float64\n",
      " 17  Jitter ppq5                   89722 non-null  float64\n",
      " 18  Jitter ddp                    89722 non-null  float64\n",
      " 19  Shimmer local                 89722 non-null  float64\n",
      " 20  Shimmer local dB              89722 non-null  float64\n",
      " 21  Shimmer apq3                  89722 non-null  float64\n",
      " 22  Shimmer apq5                  89722 non-null  float64\n",
      " 23  Shimmer apq11                 89722 non-null  float64\n",
      " 24  Shimmer dda                   89722 non-null  float64\n",
      " 25  Mean autocorrelation          89722 non-null  float64\n",
      " 26  Mean NHR                      89722 non-null  float64\n",
      " 27  Mean HNR                      89722 non-null  float64\n",
      " 28  Audio Length                  89722 non-null  float64\n",
      " 29  Company                       89722 non-null  object \n",
      " 30  Date                          89722 non-null  int64  \n",
      " 31  Sentence_num                  89722 non-null  int64  \n",
      " 32  audio_file                    89722 non-null  object \n",
      "dtypes: float64(29), int64(2), object(2)\n",
      "memory usage: 22.6+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "praat_features = praat_features.replace(['--undefined--', '--undefined-', '--undefined-- ',  '--'], np.nan)\n",
    "\n",
    "# Convert all columns into float64 except \n",
    "cols_to_convert = praat_features.columns.difference(['Company', 'Date', 'audio_file'])\n",
    "praat_features[cols_to_convert] = praat_features[cols_to_convert].progress_apply(pd.to_numeric)\n",
    "# impute median on all NULL (--undefined--)\n",
    "praat_features[cols_to_convert] = praat_features[cols_to_convert].progress_apply(\n",
    "    lambda col: col.fillna(col.median()) if pd.api.types.is_numeric_dtype(col) else col\n",
    ")\n",
    "praat_features.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c65509ab-fc83-40d5-b372-5b28368baae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 22.64it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 190.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394277 entries, 0 to 394276\n",
      "Data columns (total 33 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   Mean pitch                    394277 non-null  float64\n",
      " 1   Standard deviation            394277 non-null  float64\n",
      " 2   Minimum pitch                 394277 non-null  float64\n",
      " 3   Maximum pitch                 394277 non-null  float64\n",
      " 4   Number of pulses              394277 non-null  int64  \n",
      " 5   Number of periods             394277 non-null  int64  \n",
      " 6   Mean period                   394277 non-null  float64\n",
      " 7   Mean intensity                394277 non-null  float64\n",
      " 8   Minimum intensity             394277 non-null  float64\n",
      " 9   Maximum intensity             394277 non-null  float64\n",
      " 10  Standard deviation of period  394277 non-null  float64\n",
      " 11  Fraction of unvoiced          394277 non-null  float64\n",
      " 12  Number of voice breaks        394277 non-null  int64  \n",
      " 13  Degree of voice breaks        394277 non-null  float64\n",
      " 14  Jitter local                  394277 non-null  float64\n",
      " 15  Jitter local absolute         394277 non-null  float64\n",
      " 16  Jitter rap                    394277 non-null  float64\n",
      " 17  Jitter ppq5                   394277 non-null  float64\n",
      " 18  Jitter ddp                    394277 non-null  float64\n",
      " 19  Shimmer local                 394277 non-null  float64\n",
      " 20  Shimmer local dB              394277 non-null  float64\n",
      " 21  Shimmer apq3                  394277 non-null  float64\n",
      " 22  Shimmer apq5                  394277 non-null  float64\n",
      " 23  Shimmer apq11                 394277 non-null  float64\n",
      " 24  Shimmer dda                   394277 non-null  float64\n",
      " 25  Mean autocorrelation          394277 non-null  float64\n",
      " 26  Mean NHR                      394277 non-null  float64\n",
      " 27  Mean HNR                      394277 non-null  float64\n",
      " 28  Audio Length                  394277 non-null  float64\n",
      " 29  Ticker                        394277 non-null  object \n",
      " 30  Date                          394277 non-null  int64  \n",
      " 31  Sentence_num                  394277 non-null  int64  \n",
      " 32  audio_file                    394277 non-null  object \n",
      "dtypes: float64(26), int64(5), object(2)\n",
      "memory usage: 99.3+ MB\n"
     ]
    }
   ],
   "source": [
    "MAEC_praat_features = MAEC_praat_features.replace(['--undefined--', '--undefined-', '--undefined-- ',  '--'], np.nan)\n",
    "\n",
    "# Convert all columns into float64 except \n",
    "cols_to_convert = MAEC_praat_features.columns.difference(['Ticker', 'Date', 'audio_file'])\n",
    "MAEC_praat_features[cols_to_convert] = MAEC_praat_features[cols_to_convert].progress_apply(pd.to_numeric)\n",
    "# impute median on all NULL (--undefined--)\n",
    "MAEC_praat_features[cols_to_convert] = MAEC_praat_features[cols_to_convert].progress_apply(\n",
    "    lambda col: col.fillna(col.median()) if pd.api.types.is_numeric_dtype(col) else col\n",
    ")\n",
    "MAEC_praat_features.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8886cd-0e7d-4cc2-abe7-11b8c9466caa",
   "metadata": {},
   "source": [
    "Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f58138-f0ed-4ae0-91e0-f7e90d71c2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [02:28<00:00, 23.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 31.76it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 193.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394277 entries, 0 to 394276\n",
      "Data columns (total 32 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   Mean pitch                    394277 non-null  float64\n",
      " 1   Standard deviation            394277 non-null  float64\n",
      " 2   Minimum pitch                 394277 non-null  float64\n",
      " 3   Maximum pitch                 394277 non-null  float64\n",
      " 4   Mean intensity                394277 non-null  float64\n",
      " 5   Minimum intensity             394277 non-null  float64\n",
      " 6   Maximum intensity             394277 non-null  float64\n",
      " 7   Number of pulses              394277 non-null  float64\n",
      " 8   Number of periods             394277 non-null  float64\n",
      " 9   Mean period                   394277 non-null  float64\n",
      " 10  Standard deviation of period  394277 non-null  float64\n",
      " 11  Fraction of unvoiced          394277 non-null  float64\n",
      " 12  Number of voice breaks        394277 non-null  float64\n",
      " 13  Degree of voice breaks        394277 non-null  float64\n",
      " 14  Jitter local                  394277 non-null  float64\n",
      " 15  Jitter local absolute         394277 non-null  float64\n",
      " 16  Jitter rap                    394277 non-null  float64\n",
      " 17  Jitter ppq5                   394277 non-null  float64\n",
      " 18  Jitter ddp                    394277 non-null  float64\n",
      " 19  Shimmer local                 394277 non-null  float64\n",
      " 20  Shimmer local dB              394277 non-null  float64\n",
      " 21  Shimmer apq3                  394277 non-null  float64\n",
      " 22  Shimmer apq5                  394277 non-null  float64\n",
      " 23  Shimmer apq11                 394277 non-null  float64\n",
      " 24  Shimmer dda                   394277 non-null  float64\n",
      " 25  Mean autocorrelation          394277 non-null  float64\n",
      " 26  Mean NHR                      394277 non-null  float64\n",
      " 27  Mean HNR                      394277 non-null  float64\n",
      " 28  Audio Length                  394277 non-null  float64\n",
      " 29  Ticker                        394277 non-null  object \n",
      " 30  Date                          394277 non-null  object \n",
      " 31  Sentence_num                  394277 non-null  int64  \n",
      "dtypes: float64(29), int64(1), object(2)\n",
      "memory usage: 96.3+ MB\n"
     ]
    }
   ],
   "source": [
    "MAEC_GitHub_features = pd.DataFrame()\n",
    "def each_row(row):\n",
    "    Ticker = row['Ticker']\n",
    "    Date = row['Date'].replace('-', '') \n",
    "\n",
    "    features_df = pd.read_csv(f\"data/MAEC/MAEC_Dataset/{Date}_{Ticker}/features.csv\")\n",
    "    features_df['Ticker'] = Ticker\n",
    "    features_df['Date'] = Date\n",
    "    features_df['Sentence_num'] = range(1, len(features_df) + 1)\n",
    "\n",
    "    global MAEC_GitHub_features\n",
    "    MAEC_GitHub_features = pd.concat([MAEC_GitHub_features, features_df], ignore_index=True)\n",
    "\n",
    "MAEC_filename_data.progress_apply(each_row, axis=1)\n",
    "\n",
    "MAEC_GitHub_features = MAEC_GitHub_features.replace(['--undefined--', '--undefined-', '--undefined-- ',  '--'], np.nan)\n",
    "# Convert all columns into float64 except 'Ticker', 'Date', 'audio_file' \n",
    "cols_to_convert = MAEC_GitHub_features.columns.difference(['Ticker', 'Date', 'audio_file'])\n",
    "MAEC_GitHub_features[cols_to_convert] = MAEC_GitHub_features[cols_to_convert].progress_apply(pd.to_numeric)\n",
    "# impute median on all NULL (--undefined--)\n",
    "MAEC_GitHub_features[cols_to_convert] = MAEC_GitHub_features[cols_to_convert].progress_apply(\n",
    "    lambda col: col.fillna(col.median()) if pd.api.types.is_numeric_dtype(col) else col\n",
    ")\n",
    "MAEC_GitHub_features.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223afd3b-bb1f-462c-b703-b11209c90db5",
   "metadata": {},
   "source": [
    "# Compare\n",
    "These are very close. That's good. This gives assurance that the original dataset Pratt features are okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b279403-d79f-4da0-a4cf-d728ec7fe666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean pitch</th>\n",
       "      <th>Standard deviation</th>\n",
       "      <th>Minimum pitch</th>\n",
       "      <th>Maximum pitch</th>\n",
       "      <th>Number of pulses</th>\n",
       "      <th>Number of periods</th>\n",
       "      <th>Mean period</th>\n",
       "      <th>Mean intensity</th>\n",
       "      <th>Minimum intensity</th>\n",
       "      <th>Maximum intensity</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer apq11</th>\n",
       "      <th>Shimmer dda</th>\n",
       "      <th>Mean autocorrelation</th>\n",
       "      <th>Mean NHR</th>\n",
       "      <th>Mean HNR</th>\n",
       "      <th>Audio Length</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentence_num</th>\n",
       "      <th>audio_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122.509</td>\n",
       "      <td>4.582</td>\n",
       "      <td>119.282</td>\n",
       "      <td>132.486</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>48.617273</td>\n",
       "      <td>33.245858</td>\n",
       "      <td>59.811569</td>\n",
       "      <td>...</td>\n",
       "      <td>30.665</td>\n",
       "      <td>18.235</td>\n",
       "      <td>0.675386</td>\n",
       "      <td>0.524708</td>\n",
       "      <td>3.356</td>\n",
       "      <td>0.370979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>2</td>\n",
       "      <td>LMAT_20150225_f000002100.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137.390</td>\n",
       "      <td>76.065</td>\n",
       "      <td>91.597</td>\n",
       "      <td>572.261</td>\n",
       "      <td>103</td>\n",
       "      <td>96</td>\n",
       "      <td>0.007461</td>\n",
       "      <td>42.840412</td>\n",
       "      <td>17.956942</td>\n",
       "      <td>61.970121</td>\n",
       "      <td>...</td>\n",
       "      <td>17.935</td>\n",
       "      <td>29.632</td>\n",
       "      <td>0.712884</td>\n",
       "      <td>0.482183</td>\n",
       "      <td>4.893</td>\n",
       "      <td>2.506979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>3</td>\n",
       "      <td>LMAT_20150225_f000002101.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.168</td>\n",
       "      <td>11.177</td>\n",
       "      <td>91.897</td>\n",
       "      <td>150.239</td>\n",
       "      <td>283</td>\n",
       "      <td>266</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>43.585335</td>\n",
       "      <td>14.603231</td>\n",
       "      <td>68.719015</td>\n",
       "      <td>...</td>\n",
       "      <td>12.035</td>\n",
       "      <td>23.836</td>\n",
       "      <td>0.717230</td>\n",
       "      <td>0.468501</td>\n",
       "      <td>4.744</td>\n",
       "      <td>5.170979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>4</td>\n",
       "      <td>LMAT_20150225_f000002102.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117.238</td>\n",
       "      <td>11.779</td>\n",
       "      <td>97.850</td>\n",
       "      <td>145.271</td>\n",
       "      <td>170</td>\n",
       "      <td>162</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>42.735277</td>\n",
       "      <td>15.718385</td>\n",
       "      <td>63.319417</td>\n",
       "      <td>...</td>\n",
       "      <td>17.816</td>\n",
       "      <td>23.452</td>\n",
       "      <td>0.741878</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>5.231</td>\n",
       "      <td>3.250979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>5</td>\n",
       "      <td>LMAT_20150225_f000002103.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136.303</td>\n",
       "      <td>70.402</td>\n",
       "      <td>98.310</td>\n",
       "      <td>577.893</td>\n",
       "      <td>342</td>\n",
       "      <td>319</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>44.440119</td>\n",
       "      <td>22.618467</td>\n",
       "      <td>62.993807</td>\n",
       "      <td>...</td>\n",
       "      <td>14.846</td>\n",
       "      <td>20.482</td>\n",
       "      <td>0.752116</td>\n",
       "      <td>0.394099</td>\n",
       "      <td>5.627</td>\n",
       "      <td>5.338979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>6</td>\n",
       "      <td>LMAT_20150225_f000002104.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394272</th>\n",
       "      <td>133.531</td>\n",
       "      <td>14.505</td>\n",
       "      <td>105.107</td>\n",
       "      <td>157.856</td>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>59.591205</td>\n",
       "      <td>-15.285724</td>\n",
       "      <td>80.196919</td>\n",
       "      <td>...</td>\n",
       "      <td>6.825</td>\n",
       "      <td>10.370</td>\n",
       "      <td>0.800006</td>\n",
       "      <td>0.290069</td>\n",
       "      <td>6.972</td>\n",
       "      <td>0.743379</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>24</td>\n",
       "      <td>BKS_20180621_f000039100.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394273</th>\n",
       "      <td>128.154</td>\n",
       "      <td>49.849</td>\n",
       "      <td>95.974</td>\n",
       "      <td>567.649</td>\n",
       "      <td>396</td>\n",
       "      <td>377</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>67.647350</td>\n",
       "      <td>45.119193</td>\n",
       "      <td>80.862428</td>\n",
       "      <td>...</td>\n",
       "      <td>13.618</td>\n",
       "      <td>15.330</td>\n",
       "      <td>0.823743</td>\n",
       "      <td>0.265119</td>\n",
       "      <td>8.079</td>\n",
       "      <td>4.452766</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>25</td>\n",
       "      <td>BKS_20180621_f000039101.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394274</th>\n",
       "      <td>130.803</td>\n",
       "      <td>12.533</td>\n",
       "      <td>102.188</td>\n",
       "      <td>160.576</td>\n",
       "      <td>315</td>\n",
       "      <td>304</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>62.469106</td>\n",
       "      <td>38.641636</td>\n",
       "      <td>80.928417</td>\n",
       "      <td>...</td>\n",
       "      <td>11.752</td>\n",
       "      <td>14.277</td>\n",
       "      <td>0.824435</td>\n",
       "      <td>0.265622</td>\n",
       "      <td>8.167</td>\n",
       "      <td>4.452766</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>26</td>\n",
       "      <td>BKS_20180621_f000039102.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394275</th>\n",
       "      <td>116.219</td>\n",
       "      <td>11.366</td>\n",
       "      <td>87.046</td>\n",
       "      <td>137.937</td>\n",
       "      <td>157</td>\n",
       "      <td>148</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>61.795049</td>\n",
       "      <td>36.740310</td>\n",
       "      <td>80.262575</td>\n",
       "      <td>...</td>\n",
       "      <td>13.835</td>\n",
       "      <td>13.291</td>\n",
       "      <td>0.814544</td>\n",
       "      <td>0.285962</td>\n",
       "      <td>8.096</td>\n",
       "      <td>2.075624</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>27</td>\n",
       "      <td>BKS_20180621_f000039103.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394276</th>\n",
       "      <td>114.745</td>\n",
       "      <td>14.108</td>\n",
       "      <td>89.099</td>\n",
       "      <td>144.958</td>\n",
       "      <td>106</td>\n",
       "      <td>103</td>\n",
       "      <td>0.008770</td>\n",
       "      <td>53.664604</td>\n",
       "      <td>-2.796562</td>\n",
       "      <td>77.555516</td>\n",
       "      <td>...</td>\n",
       "      <td>11.363</td>\n",
       "      <td>17.409</td>\n",
       "      <td>0.753636</td>\n",
       "      <td>0.375386</td>\n",
       "      <td>5.439</td>\n",
       "      <td>2.153991</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>28</td>\n",
       "      <td>BKS_20180621_f000039104.mp3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394277 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mean pitch  Standard deviation  Minimum pitch  Maximum pitch  \\\n",
       "0          122.509               4.582        119.282        132.486   \n",
       "1          137.390              76.065         91.597        572.261   \n",
       "2          117.168              11.177         91.897        150.239   \n",
       "3          117.238              11.779         97.850        145.271   \n",
       "4          136.303              70.402         98.310        577.893   \n",
       "...            ...                 ...            ...            ...   \n",
       "394272     133.531              14.505        105.107        157.856   \n",
       "394273     128.154              49.849         95.974        567.649   \n",
       "394274     130.803              12.533        102.188        160.576   \n",
       "394275     116.219              11.366         87.046        137.937   \n",
       "394276     114.745              14.108         89.099        144.958   \n",
       "\n",
       "        Number of pulses  Number of periods  Mean period  Mean intensity  \\\n",
       "0                     15                 14     0.008145       48.617273   \n",
       "1                    103                 96     0.007461       42.840412   \n",
       "2                    283                266     0.008527       43.585335   \n",
       "3                    170                162     0.008542       42.735277   \n",
       "4                    342                319     0.007383       44.440119   \n",
       "...                  ...                ...          ...             ...   \n",
       "394272                42                 40     0.007475       59.591205   \n",
       "394273               396                377     0.007820       67.647350   \n",
       "394274               315                304     0.007669       62.469106   \n",
       "394275               157                148     0.008614       61.795049   \n",
       "394276               106                103     0.008770       53.664604   \n",
       "\n",
       "        Minimum intensity  Maximum intensity  ...  Shimmer apq11  Shimmer dda  \\\n",
       "0               33.245858          59.811569  ...         30.665       18.235   \n",
       "1               17.956942          61.970121  ...         17.935       29.632   \n",
       "2               14.603231          68.719015  ...         12.035       23.836   \n",
       "3               15.718385          63.319417  ...         17.816       23.452   \n",
       "4               22.618467          62.993807  ...         14.846       20.482   \n",
       "...                   ...                ...  ...            ...          ...   \n",
       "394272         -15.285724          80.196919  ...          6.825       10.370   \n",
       "394273          45.119193          80.862428  ...         13.618       15.330   \n",
       "394274          38.641636          80.928417  ...         11.752       14.277   \n",
       "394275          36.740310          80.262575  ...         13.835       13.291   \n",
       "394276          -2.796562          77.555516  ...         11.363       17.409   \n",
       "\n",
       "        Mean autocorrelation  Mean NHR  Mean HNR  Audio Length  Ticker  \\\n",
       "0                   0.675386  0.524708     3.356      0.370979    LMAT   \n",
       "1                   0.712884  0.482183     4.893      2.506979    LMAT   \n",
       "2                   0.717230  0.468501     4.744      5.170979    LMAT   \n",
       "3                   0.741878  0.407407     5.231      3.250979    LMAT   \n",
       "4                   0.752116  0.394099     5.627      5.338979    LMAT   \n",
       "...                      ...       ...       ...           ...     ...   \n",
       "394272              0.800006  0.290069     6.972      0.743379     BKS   \n",
       "394273              0.823743  0.265119     8.079      4.452766     BKS   \n",
       "394274              0.824435  0.265622     8.167      4.452766     BKS   \n",
       "394275              0.814544  0.285962     8.096      2.075624     BKS   \n",
       "394276              0.753636  0.375386     5.439      2.153991     BKS   \n",
       "\n",
       "            Date  Sentence_num                    audio_file  \n",
       "0       20150225             2  LMAT_20150225_f000002100.mp3  \n",
       "1       20150225             3  LMAT_20150225_f000002101.mp3  \n",
       "2       20150225             4  LMAT_20150225_f000002102.mp3  \n",
       "3       20150225             5  LMAT_20150225_f000002103.mp3  \n",
       "4       20150225             6  LMAT_20150225_f000002104.mp3  \n",
       "...          ...           ...                           ...  \n",
       "394272  20180621            24   BKS_20180621_f000039100.mp3  \n",
       "394273  20180621            25   BKS_20180621_f000039101.mp3  \n",
       "394274  20180621            26   BKS_20180621_f000039102.mp3  \n",
       "394275  20180621            27   BKS_20180621_f000039103.mp3  \n",
       "394276  20180621            28   BKS_20180621_f000039104.mp3  \n",
       "\n",
       "[394277 rows x 33 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEC_praat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a20973c2-e1c4-43aa-bf5b-750533dd2ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean pitch</th>\n",
       "      <th>Standard deviation</th>\n",
       "      <th>Minimum pitch</th>\n",
       "      <th>Maximum pitch</th>\n",
       "      <th>Mean intensity</th>\n",
       "      <th>Minimum intensity</th>\n",
       "      <th>Maximum intensity</th>\n",
       "      <th>Number of pulses</th>\n",
       "      <th>Number of periods</th>\n",
       "      <th>Mean period</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer apq5</th>\n",
       "      <th>Shimmer apq11</th>\n",
       "      <th>Shimmer dda</th>\n",
       "      <th>Mean autocorrelation</th>\n",
       "      <th>Mean NHR</th>\n",
       "      <th>Mean HNR</th>\n",
       "      <th>Audio Length</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentence_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122.509</td>\n",
       "      <td>4.582</td>\n",
       "      <td>119.282</td>\n",
       "      <td>132.486</td>\n",
       "      <td>48.617273</td>\n",
       "      <td>59.811569</td>\n",
       "      <td>33.245858</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>...</td>\n",
       "      <td>11.513</td>\n",
       "      <td>30.665</td>\n",
       "      <td>18.235</td>\n",
       "      <td>0.675386</td>\n",
       "      <td>0.524708</td>\n",
       "      <td>3.356</td>\n",
       "      <td>0.370979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137.390</td>\n",
       "      <td>76.065</td>\n",
       "      <td>91.597</td>\n",
       "      <td>572.261</td>\n",
       "      <td>42.840412</td>\n",
       "      <td>61.970121</td>\n",
       "      <td>17.956942</td>\n",
       "      <td>103.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.007461</td>\n",
       "      <td>...</td>\n",
       "      <td>12.248</td>\n",
       "      <td>17.935</td>\n",
       "      <td>29.632</td>\n",
       "      <td>0.712884</td>\n",
       "      <td>0.482183</td>\n",
       "      <td>4.893</td>\n",
       "      <td>2.506979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.168</td>\n",
       "      <td>11.177</td>\n",
       "      <td>91.897</td>\n",
       "      <td>150.239</td>\n",
       "      <td>43.585335</td>\n",
       "      <td>68.719015</td>\n",
       "      <td>14.603231</td>\n",
       "      <td>283.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.008527</td>\n",
       "      <td>...</td>\n",
       "      <td>10.633</td>\n",
       "      <td>12.035</td>\n",
       "      <td>23.836</td>\n",
       "      <td>0.717230</td>\n",
       "      <td>0.468501</td>\n",
       "      <td>4.744</td>\n",
       "      <td>5.170979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117.238</td>\n",
       "      <td>11.779</td>\n",
       "      <td>97.850</td>\n",
       "      <td>145.271</td>\n",
       "      <td>42.735277</td>\n",
       "      <td>63.319417</td>\n",
       "      <td>15.718385</td>\n",
       "      <td>170.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>...</td>\n",
       "      <td>9.040</td>\n",
       "      <td>17.816</td>\n",
       "      <td>23.452</td>\n",
       "      <td>0.741878</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>5.231</td>\n",
       "      <td>3.250979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136.303</td>\n",
       "      <td>70.402</td>\n",
       "      <td>98.310</td>\n",
       "      <td>577.893</td>\n",
       "      <td>44.440119</td>\n",
       "      <td>62.993807</td>\n",
       "      <td>22.618467</td>\n",
       "      <td>342.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>...</td>\n",
       "      <td>8.630</td>\n",
       "      <td>14.846</td>\n",
       "      <td>20.482</td>\n",
       "      <td>0.752116</td>\n",
       "      <td>0.394099</td>\n",
       "      <td>5.627</td>\n",
       "      <td>5.338979</td>\n",
       "      <td>LMAT</td>\n",
       "      <td>20150225</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394272</th>\n",
       "      <td>133.531</td>\n",
       "      <td>14.505</td>\n",
       "      <td>105.107</td>\n",
       "      <td>157.856</td>\n",
       "      <td>59.591205</td>\n",
       "      <td>80.196919</td>\n",
       "      <td>-15.285724</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>...</td>\n",
       "      <td>4.486</td>\n",
       "      <td>6.825</td>\n",
       "      <td>10.370</td>\n",
       "      <td>0.800006</td>\n",
       "      <td>0.290069</td>\n",
       "      <td>6.972</td>\n",
       "      <td>0.743379</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394273</th>\n",
       "      <td>128.154</td>\n",
       "      <td>49.849</td>\n",
       "      <td>95.974</td>\n",
       "      <td>567.649</td>\n",
       "      <td>67.647350</td>\n",
       "      <td>80.862428</td>\n",
       "      <td>45.119193</td>\n",
       "      <td>396.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>...</td>\n",
       "      <td>7.697</td>\n",
       "      <td>13.618</td>\n",
       "      <td>15.330</td>\n",
       "      <td>0.823743</td>\n",
       "      <td>0.265119</td>\n",
       "      <td>8.079</td>\n",
       "      <td>4.452766</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394274</th>\n",
       "      <td>130.803</td>\n",
       "      <td>12.533</td>\n",
       "      <td>102.188</td>\n",
       "      <td>160.576</td>\n",
       "      <td>62.469106</td>\n",
       "      <td>80.928417</td>\n",
       "      <td>38.641636</td>\n",
       "      <td>315.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>...</td>\n",
       "      <td>5.904</td>\n",
       "      <td>11.752</td>\n",
       "      <td>14.277</td>\n",
       "      <td>0.824435</td>\n",
       "      <td>0.265622</td>\n",
       "      <td>8.167</td>\n",
       "      <td>4.452766</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394275</th>\n",
       "      <td>116.219</td>\n",
       "      <td>11.366</td>\n",
       "      <td>87.046</td>\n",
       "      <td>137.937</td>\n",
       "      <td>61.795049</td>\n",
       "      <td>80.262575</td>\n",
       "      <td>36.740310</td>\n",
       "      <td>157.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>...</td>\n",
       "      <td>7.349</td>\n",
       "      <td>13.835</td>\n",
       "      <td>13.291</td>\n",
       "      <td>0.814544</td>\n",
       "      <td>0.285962</td>\n",
       "      <td>8.096</td>\n",
       "      <td>2.075624</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394276</th>\n",
       "      <td>114.745</td>\n",
       "      <td>14.108</td>\n",
       "      <td>89.099</td>\n",
       "      <td>144.958</td>\n",
       "      <td>53.664604</td>\n",
       "      <td>77.555516</td>\n",
       "      <td>-2.796562</td>\n",
       "      <td>106.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.008770</td>\n",
       "      <td>...</td>\n",
       "      <td>7.395</td>\n",
       "      <td>11.363</td>\n",
       "      <td>17.409</td>\n",
       "      <td>0.753636</td>\n",
       "      <td>0.375386</td>\n",
       "      <td>5.439</td>\n",
       "      <td>2.153991</td>\n",
       "      <td>BKS</td>\n",
       "      <td>20180621</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394277 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mean pitch  Standard deviation  Minimum pitch  Maximum pitch  \\\n",
       "0          122.509               4.582        119.282        132.486   \n",
       "1          137.390              76.065         91.597        572.261   \n",
       "2          117.168              11.177         91.897        150.239   \n",
       "3          117.238              11.779         97.850        145.271   \n",
       "4          136.303              70.402         98.310        577.893   \n",
       "...            ...                 ...            ...            ...   \n",
       "394272     133.531              14.505        105.107        157.856   \n",
       "394273     128.154              49.849         95.974        567.649   \n",
       "394274     130.803              12.533        102.188        160.576   \n",
       "394275     116.219              11.366         87.046        137.937   \n",
       "394276     114.745              14.108         89.099        144.958   \n",
       "\n",
       "        Mean intensity  Minimum intensity  Maximum intensity  \\\n",
       "0            48.617273          59.811569          33.245858   \n",
       "1            42.840412          61.970121          17.956942   \n",
       "2            43.585335          68.719015          14.603231   \n",
       "3            42.735277          63.319417          15.718385   \n",
       "4            44.440119          62.993807          22.618467   \n",
       "...                ...                ...                ...   \n",
       "394272       59.591205          80.196919         -15.285724   \n",
       "394273       67.647350          80.862428          45.119193   \n",
       "394274       62.469106          80.928417          38.641636   \n",
       "394275       61.795049          80.262575          36.740310   \n",
       "394276       53.664604          77.555516          -2.796562   \n",
       "\n",
       "        Number of pulses  Number of periods  Mean period  ...  Shimmer apq5  \\\n",
       "0                   15.0               14.0     0.008145  ...        11.513   \n",
       "1                  103.0               96.0     0.007461  ...        12.248   \n",
       "2                  283.0              266.0     0.008527  ...        10.633   \n",
       "3                  170.0              162.0     0.008542  ...         9.040   \n",
       "4                  342.0              319.0     0.007383  ...         8.630   \n",
       "...                  ...                ...          ...  ...           ...   \n",
       "394272              42.0               40.0     0.007475  ...         4.486   \n",
       "394273             396.0              377.0     0.007820  ...         7.697   \n",
       "394274             315.0              304.0     0.007669  ...         5.904   \n",
       "394275             157.0              148.0     0.008614  ...         7.349   \n",
       "394276             106.0              103.0     0.008770  ...         7.395   \n",
       "\n",
       "        Shimmer apq11  Shimmer dda  Mean autocorrelation  Mean NHR  Mean HNR  \\\n",
       "0              30.665       18.235              0.675386  0.524708     3.356   \n",
       "1              17.935       29.632              0.712884  0.482183     4.893   \n",
       "2              12.035       23.836              0.717230  0.468501     4.744   \n",
       "3              17.816       23.452              0.741878  0.407407     5.231   \n",
       "4              14.846       20.482              0.752116  0.394099     5.627   \n",
       "...               ...          ...                   ...       ...       ...   \n",
       "394272          6.825       10.370              0.800006  0.290069     6.972   \n",
       "394273         13.618       15.330              0.823743  0.265119     8.079   \n",
       "394274         11.752       14.277              0.824435  0.265622     8.167   \n",
       "394275         13.835       13.291              0.814544  0.285962     8.096   \n",
       "394276         11.363       17.409              0.753636  0.375386     5.439   \n",
       "\n",
       "        Audio Length  Ticker      Date  Sentence_num  \n",
       "0           0.370979    LMAT  20150225             1  \n",
       "1           2.506979    LMAT  20150225             2  \n",
       "2           5.170979    LMAT  20150225             3  \n",
       "3           3.250979    LMAT  20150225             4  \n",
       "4           5.338979    LMAT  20150225             5  \n",
       "...              ...     ...       ...           ...  \n",
       "394272      0.743379     BKS  20180621            24  \n",
       "394273      4.452766     BKS  20180621            25  \n",
       "394274      4.452766     BKS  20180621            26  \n",
       "394275      2.075624     BKS  20180621            27  \n",
       "394276      2.153991     BKS  20180621            28  \n",
       "\n",
       "[394277 rows x 32 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEC_GitHub_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647bdc27-138f-47ab-aafb-6cf12a52b995",
   "metadata": {},
   "source": [
    "# Final datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92a50004",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 523\n",
    "\n",
    "# The maximum sentences per meeting is 523 \n",
    "# Ensure each meeting has 523 sentences, add zeros to the end\n",
    "def add_zero_padding(group):\n",
    "    complete_index = pd.Index(np.arange(1, (max_sent_len +1)), name='Sentence_num')\n",
    "    group = group.set_index('Sentence_num').reindex(complete_index).reset_index()\n",
    "    group['Date'] = group['Date'].ffill().bfill()\n",
    "    group['Company'] = group['Company'].ffill().bfill()\n",
    "    group.fillna(0.0, inplace=True)\n",
    "    return group\n",
    "\n",
    "def combine_audio_text_targets(audio_features, text_features, targets, num_features, save_directory):\n",
    "    # combine & add_zero_padding\n",
    "    features = pd.merge(audio_features, text_features, how=\"left\",on = ['Company','Date','Sentence_num'])\n",
    "    features = features.drop(['Shimmer apq11','Audio Length','audio_file'], axis=1)\n",
    "    features = features.groupby(['Company', 'Date'], group_keys=False).progress_apply(add_zero_padding).reset_index(drop=True)\n",
    "    features.fillna(0, inplace=True)\n",
    "\n",
    "    # match the targets to the features\n",
    "    # (target values will be duplicated 523 times, for each meeting, but that's okay for now)\n",
    "    # we need the date and company columns to sort\n",
    "    targets['Date'] = targets['Date'].astype(str).str.replace('-', '').astype('Int64')\n",
    "    original_dataset = pd.merge(features, targets, how=\"left\",on = ['Company','Date'])\n",
    "    rows_with_nulls = original_dataset[original_dataset.isnull().any(axis=1)]\n",
    "    original_dataset.fillna(0, inplace=True)\n",
    "    print('Number of rows with NULL values -   ', len(rows_with_nulls))\n",
    "\n",
    "    # dump rows without targets\n",
    "    original_dataset = original_dataset.drop(['Ticker'], axis=1)\n",
    "    original_dataset = original_dataset[~((original_dataset['3_day'] == 0) & \n",
    "                                        (original_dataset['7_day'] == 0) & \n",
    "                                        (original_dataset['15_day'] == 0) & \n",
    "                                        (original_dataset['30_day'] == 0))]\n",
    "    original_dataset = original_dataset.sort_values(by=['Date', 'Company', 'Sentence_num'], ascending=[True, True, True])\n",
    "    original_dataset.info(verbose=False)\n",
    "\n",
    "    # train/val/test split FEATURES ONLY\n",
    "    original_dataset['Date'] = pd.to_datetime(original_dataset['Date'].astype(str), format='%Y%m%d')\n",
    "    train_features = original_dataset[original_dataset['Date'] <= '2017-08-02']\n",
    "    val_features = original_dataset[(original_dataset['Date'] >= '2017-08-03') & (original_dataset['Date'] < '2017-10-24')]\n",
    "    test_features = original_dataset[original_dataset['Date'] >= '2017-10-24']\n",
    "    train_features = train_features.drop(['Company','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    val_features = val_features.drop(['Company','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    test_features = test_features.drop(['Company','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "\n",
    "    # now for the TARGETS - we need 4 sets of targets (3, 7, 15, 30) each with primary/secondary targets \n",
    "    targets = targets[~((targets['3_day'] == 0) & \n",
    "                        (targets['7_day'] == 0) & \n",
    "                        (targets['15_day'] == 0) & \n",
    "                        (targets['30_day'] == 0))] # dump rows without targets\n",
    "    targets = targets.copy().sort_values(by=['Date', 'Company'], ascending=[True, True])\n",
    "    targets['Date'] = pd.to_datetime(targets['Date'].astype(str), format='%Y%m%d')\n",
    "    # train/val/test split\n",
    "    train_targets = targets[targets['Date'] <= '2017-08-02']\n",
    "    val_targets = targets[(targets['Date'] >= '2017-08-03') & (targets['Date'] < '2017-10-24')]\n",
    "    test_targets = targets[targets['Date'] >= '2017-10-24']\n",
    "\n",
    "    #############################  save  ######################################################################\n",
    "    # FEATURES- Reshape the NumPy array to have dimensions ( # meetings, 523 sentences (with padding), num_features)\n",
    "    np.save(f'data/{save_directory}/train_features.npy', train_features.reshape(int(len(train_features)/max_sent_len), max_sent_len, num_features))\n",
    "    np.save(f'data/{save_directory}/val_features.npy', val_features.reshape(int(len(val_features)/max_sent_len), max_sent_len, num_features))\n",
    "    np.save(f'data/{save_directory}/test_features.npy', test_features.reshape(int(len(test_features)/max_sent_len), max_sent_len, num_features))\n",
    "    \n",
    "    np.save(f'data/{save_directory}/train_targets_3.npy', train_targets['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_3.npy', val_targets['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_3.npy', test_targets['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_3.npy', train_targets['3_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_3.npy', val_targets['3_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_3.npy', test_targets['3_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_7.npy', train_targets['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_7.npy', val_targets['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_7.npy', test_targets['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_7.npy', train_targets['7_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_7.npy', val_targets['7_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_7.npy', test_targets['7_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_15.npy', train_targets['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_15.npy', val_targets['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_15.npy', test_targets['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_15.npy', train_targets['15_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_15.npy', val_targets['15_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_15.npy', test_targets['15_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_30.npy', train_targets['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_30.npy', val_targets['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_30.npy', test_targets['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_30.npy', train_targets['30_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_30.npy', val_targets['30_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_30.npy', test_targets['30_day_single'].to_numpy())\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3a22103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:05<00:00, 98.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 292880 entries, 73743 to 75311\n",
      "Columns: 1062 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(1059), int32(1), object(1)\n",
      "memory usage: 2.3+ GB\n"
     ]
    }
   ],
   "source": [
    "praat_features['Date'] = praat_features['Date'].astype('Int64')\n",
    "combine_audio_text_targets(praat_features, RoBERTa_features, targets, 1051, 'RoBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf07e6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:05<00:00, 104.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 292880 entries, 73743 to 75311\n",
      "Columns: 1062 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(1059), int32(1), object(1)\n",
      "memory usage: 2.3+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(praat_features, RoBERTa_features2, targets, 1051, 'RoBERTa2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08656b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:04<00:00, 132.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 292880 entries, 73743 to 75311\n",
      "Columns: 806 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(803), int32(1), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(praat_features, investopedia_features, targets, 795, 'investopedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6cd6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:05<00:00, 104.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 292880 entries, 73743 to 75311\n",
      "Columns: 1062 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(1059), int32(1), object(1)\n",
      "memory usage: 2.3+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(praat_features, bge_features, targets, 1051, 'bge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1db4955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:04<00:00, 133.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 292880 entries, 73743 to 75311\n",
      "Columns: 806 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(803), int32(1), object(1)\n",
      "memory usage: 1.8+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(praat_features, bge_base_features, targets, 795, 'bge_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3e95595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:02<00:00, 266.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 292880 entries, 73743 to 75311\n",
      "Columns: 338 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(335), int32(1), object(1)\n",
      "memory usage: 756.7+ MB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(praat_features, glove_features, targets, 327, 'glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402b950-2a10-45b5-bc00-2b72f70603f9",
   "metadata": {},
   "source": [
    "# Final MAEC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edfee6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 500 \n",
    "\n",
    "# The maximum sentences per meeting is  \n",
    "# Ensure each meeting has  sentences, add zeros to the end\n",
    "def add_zero_padding(group):\n",
    "    complete_index = pd.Index(np.arange(1, (max_sent_len + 1)), name='Sentence_num')\n",
    "    group = group.set_index('Sentence_num').reindex(complete_index).reset_index()\n",
    "    group['Date'] = group['Date'].ffill().bfill()\n",
    "    group['Ticker'] = group['Ticker'].ffill().bfill()\n",
    "    group.fillna(0.0, inplace=True)\n",
    "    return group\n",
    "\n",
    "def combine_audio_text_targets(audio_features, text_features, targets, num_features, save_directory):\n",
    "    # combine & add_zero_padding\n",
    "    features = pd.merge(audio_features, text_features, how=\"left\",on = ['Ticker','Date','Sentence_num'])\n",
    "    features = features.drop(['Shimmer apq11','Audio Length','audio_file'], axis=1)\n",
    "    features = features.groupby(['Ticker', 'Date'], group_keys=False).progress_apply(add_zero_padding).reset_index(drop=True)\n",
    "    features.fillna(0, inplace=True)\n",
    "\n",
    "    # match the targets to the features\n",
    "    # (target values will be duplicated 500 times, for each meeting, but that's okay for now)\n",
    "    # we need the date and company columns to sort\n",
    "    targets['Date'] = targets['Date'].astype(str).str.replace('-', '').astype('Int64')\n",
    "    MAEC_dataset = pd.merge(features, targets, how=\"left\",on = ['Ticker','Date'])\n",
    "    rows_with_nulls = MAEC_dataset[MAEC_dataset.isnull().any(axis=1)]\n",
    "    MAEC_dataset.fillna(0, inplace=True)\n",
    "    print('Number of rows with NULL values -   ', len(rows_with_nulls))\n",
    "\n",
    "    # dump rows without targets\n",
    "    MAEC_dataset = MAEC_dataset[~((MAEC_dataset['3_day'] == 0) & \n",
    "                                        (MAEC_dataset['7_day'] == 0) & \n",
    "                                        (MAEC_dataset['15_day'] == 0) & \n",
    "                                        (MAEC_dataset['30_day'] == 0))]\n",
    "    MAEC_dataset = MAEC_dataset.sort_values(by=['Date', 'Ticker', 'Sentence_num'], ascending=[True, True, True])\n",
    "    MAEC_dataset.info(verbose=False)\n",
    "\n",
    "    # train/val/test split FEATURES ONLY\n",
    "    MAEC_dataset['Date'] = pd.to_datetime(MAEC_dataset['Date'].astype(str), format='%Y%m%d')\n",
    "    train_features_2015 = MAEC_dataset[MAEC_dataset['Date'] <= '2015-10-21']\n",
    "    val_features_2015 = MAEC_dataset[(MAEC_dataset['Date'] >= '2015-10-22') & (MAEC_dataset['Date'] < '2015-10-28')]\n",
    "    test_features_2015 = MAEC_dataset[(MAEC_dataset['Date'] >= '2015-10-29') & (MAEC_dataset['Date'] < '2016-01-01')]\n",
    "    # 2016\n",
    "    train_features_2016 = MAEC_dataset[(MAEC_dataset['Date'] >= '2016-01-01') & (MAEC_dataset['Date'] < '2016-08-03')]\n",
    "    val_features_2016 = MAEC_dataset[(MAEC_dataset['Date'] >= '2016-08-03') & (MAEC_dataset['Date'] < '2016-08-12')]\n",
    "    test_features_2016 = MAEC_dataset[(MAEC_dataset['Date'] >= '2016-08-12') & (MAEC_dataset['Date'] < '2017-01-01')]\n",
    "    # 2017\n",
    "    train_features_2017 = MAEC_dataset[(MAEC_dataset['Date'] >= '2017-01-01') & (MAEC_dataset['Date'] < '2017-11-07')]\n",
    "    val_features_2017 = MAEC_dataset[(MAEC_dataset['Date'] >= '2017-11-07') & (MAEC_dataset['Date'] < '2018-02-12')]\n",
    "    test_features_2017 = MAEC_dataset[(MAEC_dataset['Date'] >= '2018-02-15') & (MAEC_dataset['Date'] < '2019-01-01')]\n",
    "\n",
    "    train_features_2015 = train_features_2015.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    val_features_2015 = val_features_2015.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    test_features_2015 = test_features_2015.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    # 2016\n",
    "    train_features_2016 = train_features_2016.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    val_features_2016 = val_features_2016.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    test_features_2016 = test_features_2016.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    # 2017\n",
    "    train_features_2017 = train_features_2017.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    val_features_2017 = val_features_2017.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "    test_features_2017 = test_features_2017.drop(['Ticker','Date','Sentence_num','3_day','3_day_single','7_day','7_day_single', '15_day','15_day_single','30_day','30_day_single'], axis=1).to_numpy()\n",
    "\n",
    "    # now for the TARGETS - we need 4 sets of targets (3, 7, 15, 30) each with primary/secondary targets \n",
    "    targets = targets[~((targets['3_day'] == 0) & \n",
    "                        (targets['7_day'] == 0) & \n",
    "                        (targets['15_day'] == 0) & \n",
    "                        (targets['30_day'] == 0))] # dump rows without targets\n",
    "    targets = targets.copy().sort_values(by=['Date', 'Ticker'], ascending=[True, True])\n",
    "    targets['Date'] = pd.to_datetime(targets['Date'].astype(str), format='%Y%m%d')\n",
    "    # train/val/test split\n",
    "    train_targets_2015 = targets[targets['Date'] <= '2015-10-21']\n",
    "    val_targets_2015 = targets[(targets['Date'] >= '2015-10-22') & (targets['Date'] < '2015-10-28')]\n",
    "    test_targets_2015 = targets[(targets['Date'] >= '2015-10-29') & (targets['Date'] < '2016-01-01')]\n",
    "    # 2016\n",
    "    train_targets_2016 = targets[(targets['Date'] >= '2016-01-01') & (targets['Date'] < '2016-08-03')]\n",
    "    val_targets_2016 = targets[(targets['Date'] >= '2016-08-03') & (targets['Date'] < '2016-08-12')]\n",
    "    test_targets_2016 = targets[(targets['Date'] >= '2016-08-12') & (targets['Date'] < '2017-01-01')]\n",
    "    # 2017\n",
    "    train_targets_2017 = targets[(targets['Date'] >= '2017-01-01') & (targets['Date'] < '2017-11-07')]\n",
    "    val_targets_2017 = targets[(targets['Date'] >= '2017-11-07') & (targets['Date'] < '2018-02-12')]\n",
    "    test_targets_2017 = targets[(targets['Date'] >= '2018-02-15') & (targets['Date'] < '2019-01-01')]\n",
    "\n",
    "    #############################  save  ######################################################################\n",
    "    # FEATURES- Reshape the NumPy array to have dimensions ( # meetings, # sentences (with padding), num_features)\n",
    "    np.save(f'data/{save_directory}/train_features_2015.npy', train_features_2015.reshape(int((len(train_features_2015))/max_sent_len), max_sent_len, num_features))\n",
    "    np.save(f'data/{save_directory}/val_features_2015.npy', val_features_2015.reshape(int((len(val_features_2015))/max_sent_len), max_sent_len, num_features))\n",
    "    np.save(f'data/{save_directory}/test_features_2015.npy', test_features_2015.reshape(int((len(test_features_2015))/max_sent_len), max_sent_len, num_features))\n",
    "    # 2016\n",
    "    np.save(f'data/{save_directory}/train_features_2016.npy', train_features_2016.reshape(int((len(train_features_2016))/max_sent_len), max_sent_len, num_features))\n",
    "    np.save(f'data/{save_directory}/val_features_2016.npy', val_features_2016.reshape(int((len(val_features_2016))/max_sent_len), max_sent_len, num_features))\n",
    "    np.save(f'data/{save_directory}/test_features_2016.npy', test_features_2016.reshape(int((len(test_features_2016))/max_sent_len), max_sent_len, num_features))\n",
    "    # 2017\n",
    "    np.save(f'data/{save_directory}/train_features_2017.npy', train_features_2017.reshape(int((len(train_features_2017))/max_sent_len), max_sent_len, num_features))\n",
    "    np.save(f'data/{save_directory}/val_features_2017.npy', val_features_2017.reshape(int((len(val_features_2017))/max_sent_len), max_sent_len, num_features))\n",
    "    np.save(f'data/{save_directory}/test_features_2017.npy', test_features_2017.reshape(int((len(test_features_2017))/max_sent_len), max_sent_len, num_features))\n",
    "\n",
    "    ##################################################################################################\n",
    "    # 2015 targets\n",
    "    np.save(f'data/{save_directory}/train_targets_2015_3.npy', train_targets_2015['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2015_3.npy', val_targets_2015['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2015_3.npy', test_targets_2015['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2015_3.npy', train_targets_2015['3_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2015_3.npy', val_targets_2015['3_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2015_3.npy', test_targets_2015['3_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2015_7.npy', train_targets_2015['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2015_7.npy', val_targets_2015['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2015_7.npy', test_targets_2015['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2015_7.npy', train_targets_2015['7_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2015_7.npy', val_targets_2015['7_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2015_7.npy', test_targets_2015['7_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2015_15.npy', train_targets_2015['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2015_15.npy', val_targets_2015['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2015_15.npy', test_targets_2015['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2015_15.npy', train_targets_2015['15_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2015_15.npy', val_targets_2015['15_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2015_15.npy', test_targets_2015['15_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2015_30.npy', train_targets_2015['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2015_30.npy', val_targets_2015['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2015_30.npy', test_targets_2015['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2015_30.npy', train_targets_2015['30_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2015_30.npy', val_targets_2015['30_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2015_30.npy', test_targets_2015['30_day_single'].to_numpy())\n",
    "\n",
    "    ##################################################################################################\n",
    "    # 2016 targets\n",
    "    np.save(f'data/{save_directory}/train_targets_2016_3.npy', train_targets_2016['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2016_3.npy', val_targets_2016['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2016_3.npy', test_targets_2016['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2016_3.npy', train_targets_2016['3_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2016_3.npy', val_targets_2016['3_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2016_3.npy', test_targets_2016['3_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2016_7.npy', train_targets_2016['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2016_7.npy', val_targets_2016['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2016_7.npy', test_targets_2016['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2016_7.npy', train_targets_2016['7_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2016_7.npy', val_targets_2016['7_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2016_7.npy', test_targets_2016['7_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2016_15.npy', train_targets_2016['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2016_15.npy', val_targets_2016['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2016_15.npy', test_targets_2016['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2016_15.npy', train_targets_2016['15_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2016_15.npy', val_targets_2016['15_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2016_15.npy', test_targets_2016['15_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2016_30.npy', train_targets_2016['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2016_30.npy', val_targets_2016['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2016_30.npy', test_targets_2016['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2016_30.npy', train_targets_2016['30_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2016_30.npy', val_targets_2016['30_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2016_30.npy', test_targets_2016['30_day_single'].to_numpy())\n",
    "\n",
    "    ##################################################################################################\n",
    "    # 2017\n",
    "    np.save(f'data/{save_directory}/train_targets_2017_3.npy', train_targets_2017['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2017_3.npy', val_targets_2017['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2017_3.npy', test_targets_2017['3_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2017_3.npy', train_targets_2017['3_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2017_3.npy', val_targets_2017['3_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2017_3.npy', test_targets_2017['3_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2017_7.npy', train_targets_2017['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2017_7.npy', val_targets_2017['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2017_7.npy', test_targets_2017['7_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2017_7.npy', train_targets_2017['7_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2017_7.npy', val_targets_2017['7_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2017_7.npy', test_targets_2017['7_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2017_15.npy', train_targets_2017['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2017_15.npy', val_targets_2017['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2017_15.npy', test_targets_2017['15_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2017_15.npy', train_targets_2017['15_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2017_15.npy', val_targets_2017['15_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2017_15.npy', test_targets_2017['15_day_single'].to_numpy())\n",
    "\n",
    "    np.save(f'data/{save_directory}/train_targets_2017_30.npy', train_targets_2017['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_targets_2017_30.npy', val_targets_2017['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_targets_2017_30.npy', test_targets_2017['30_day'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/train_secondary_targets_2017_30.npy', train_targets_2017['30_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/val_secondary_targets_2017_30.npy', val_targets_2017['30_day_single'].to_numpy())\n",
    "    np.save(f'data/{save_directory}/test_secondary_targets_2017_30.npy', test_targets_2017['30_day_single'].to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5be3211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [00:38<00:00, 90.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1612500 entries, 934500 to 234999\n",
      "Columns: 1062 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(1059), int32(1), object(1)\n",
      "memory usage: 12.8+ GB\n"
     ]
    }
   ],
   "source": [
    "MAEC_praat_features['Date'] = MAEC_praat_features['Date'].astype('Int64')\n",
    "combine_audio_text_targets(MAEC_praat_features, MAEC_RoBERTa_features, MAEC_targets, 1051, 'MAEC_RoBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "118fc586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [00:33<00:00, 104.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1612500 entries, 934500 to 234999\n",
      "Columns: 1062 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(1059), int32(1), object(1)\n",
      "memory usage: 12.8+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(MAEC_praat_features, MAEC_RoBERTa_features2, MAEC_targets, 1051, 'MAEC_RoBERTa2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "999f7e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [00:25<00:00, 134.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1612500 entries, 934500 to 234999\n",
      "Columns: 806 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(803), int32(1), object(1)\n",
      "memory usage: 9.7+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(MAEC_praat_features, MAEC_investopedia_features, MAEC_targets, 795, 'MAEC_investopedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e13bb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [00:33<00:00, 104.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1612500 entries, 934500 to 234999\n",
      "Columns: 1062 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(1059), int32(1), object(1)\n",
      "memory usage: 12.8+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(MAEC_praat_features, MAEC_bge_features, MAEC_targets, 1051, 'MAEC_bge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8dad44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [00:25<00:00, 136.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1612500 entries, 934500 to 234999\n",
      "Columns: 806 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(803), int32(1), object(1)\n",
      "memory usage: 9.7+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(MAEC_praat_features, MAEC_bge_base_features, MAEC_targets, 795, 'MAEC_bge_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17714587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3443/3443 [00:13<00:00, 264.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NULL values -    0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1612500 entries, 934500 to 234999\n",
      "Columns: 338 entries, Sentence_num to 30_day_single\n",
      "dtypes: Int64(1), float64(335), int32(1), object(1)\n",
      "memory usage: 4.1+ GB\n"
     ]
    }
   ],
   "source": [
    "combine_audio_text_targets(MAEC_praat_features, MAEC_glove_features, MAEC_targets, 327, 'MAEC_glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fcb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd912028-cb72-4bdf-b82e-09c7c604a164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
