{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "\n",
    "#Generate sentence representation by Bert-As-Service\n",
    "text_all_embs = []\n",
    "sentence_len = []\n",
    "for i in range(len(text_all)):\n",
    "    text = text_all[i].split(\"\\n\")\n",
    "    sentence_len.append(len(text))\n",
    "    text_embs = bc.encode(text)\n",
    "    #text_embs = np.concatenate((text_embs,np.array([np.array(past_volatility_all[i])]*len(text_embs))),axis=1)\n",
    "    text_all_embs.append(text_embs)\n",
    "    \n",
    "# Padding\n",
    "dim = 1024 # Depends on the dimensions of your selected token-level pretrained model\n",
    "b = np.zeros([len(text_all_embs),len(max(text_all_embs,key = lambda x: len(x))),dim]) \n",
    "for i,j in enumerate(text_all_embs): \n",
    "    b[i][0:len(j),:] = j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append(package-path)\n",
    "import transformer\n",
    "import torch\n",
    "\n",
    "model = transformers.RobertaModel.from_pretrained('roberta-large')\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "def emb_str():\n",
    "    input_ids = torch.tensor([tokenizer.encode(str('inputs here'), add_special_tokens=False)]) \n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n",
    "        emb = last_hidden_states.cpu().numpy()\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feature = AUDIO_DATA\n",
    "audio_feature_3 = AUDIO_DATA_THREE_DAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature = np.load('SENTENCE_EMBEDDING_DATA')\n",
    "text_feature_3 = np.load('SENTENCE_EMBEDDING_DATA_THREE_DAYS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_audio_embs = [np.concatenate((text_feature[i],np.array(audio_feature[i])),axis=1) for i in range(audio_feature.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_audio_embs_3 = [np.concatenate((text_feature_3[i],np.array(audio_feature_3[i])),axis=1) for i in range(audio_feature_3.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimal alpha for connecting two tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 days -- validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "\n",
    "def mask_(matrices, maskval=0.0, mask_diagonal=True):\n",
    "    \"\"\"\n",
    "    Masks out all values in the given batch of matrices where i <= j holds,\n",
    "    i < j if mask_diagonal is false\n",
    "\n",
    "    In place operation\n",
    "\n",
    "    :param tns:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    b, h, w = matrices.size()\n",
    "\n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
    "    matrices[:, indices[0], indices[1]] = maskval\n",
    "\n",
    "def d(tensor=None):\n",
    "    \"\"\"\n",
    "    Returns a device string either for the best available device,\n",
    "    or for the device corresponding to the argument\n",
    "    :param tensor:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if tensor is None:\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "def here(subpath=None):\n",
    "    \"\"\"\n",
    "    :return: the path in which the package resides (the directory containing the 'former' dir)\n",
    "    \"\"\"\n",
    "    if subpath is None:\n",
    "        return os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
    "\n",
    "    return os.path.abspath(os.path.join(os.path.dirname(__file__), '../..', subpath))\n",
    "\n",
    "def contains_nan(tensor):\n",
    "    return bool((tensor != tensor).sum() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "#math package\n",
    "import random, math\n",
    "from numpy.random import seed\n",
    "#from tensorflow import set_random_seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Pytorch Package\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Customized Transformers Util\n",
    "# from transformer import util\n",
    "# from .util import mask_\n",
    "# from .util import d\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        self.tokeys = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.tovalues = nn.Linear(emb, emb * heads, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb\n",
    "\n",
    "        keys    = self.tokeys(x)   .view(b, t, h, e)\n",
    "        queries = self.toqueries(x).view(b, t, h, e)\n",
    "        values  = self.tovalues(x) .view(b, t, h, e)\n",
    "\n",
    "        # compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the lower half of the dot matrix,including the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2) # dot now has row-wise self-attention probabilities\n",
    "\n",
    "        #assert not util.contains_nan(dot[:, 1:, :]) # only the forst row may contain nan\n",
    "        assert not contains_nan(dot[:, 1:, :]) # only the forst row may contain nan\n",
    "\n",
    "        if self.mask == 'first':\n",
    "            dot = dot.clone()\n",
    "            dot[:, :1, :] = 0.0\n",
    "            # - The first row of the first attention matrix is entirely masked out, so the softmax operation results\n",
    "            #   in a division by zero. We set this row to zero by hand to get rid of the NaNs\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n",
    "\n",
    "        return self.unifyheads(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb, heads=heads, mask=mask)\n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult * emb, emb)\n",
    "        )\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attended = self.attention(x)\n",
    "\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        fedforward = self.ff(x)\n",
    "\n",
    "        x = self.norm2(fedforward + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        return x\n",
    "##RTransformer\n",
    "\n",
    "class RTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for sequences Regression    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0):\n",
    "        \"\"\"\n",
    "        emb: Embedding dimension\n",
    "        heads: nr. of attention heads\n",
    "        depth: Number of transformer blocks\n",
    "        seq_length: Expected maximum sequence length\n",
    "        num_tokens: Number of tokens (usually words) in the vocabulary\n",
    "        num_classes: Number of classes.\n",
    "        max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "\n",
    "        #self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_classes)\n",
    "        self.toprobs_b = nn.Linear(emb, num_classes)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        sentences_emb = x\n",
    "        b, t, e = x.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        #positions = self.pos_embedding(torch.arange(t))[None, :, :].expand(b, t, e)\n",
    "        #positions = torch.tensor(positions, dtype=torch.float32)\n",
    "        x = sentences_emb.cuda() + positions\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
    "        \n",
    "        \n",
    "        x_a = self.toprobs(x)\n",
    "        x_b = self.toprobs_b(x)\n",
    "        x_a = torch.squeeze(x_a)\n",
    "        x_b = torch.squeeze(x_b)\n",
    "        #print('x shape: ',x.shape)\n",
    "        return x_a, x_b\n",
    "\n",
    "class CTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for classifying sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0, wide=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout, wide=wide))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_classes)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        x = tokens + positions\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
    "\n",
    "        x = self.toprobs(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "#Customized Transformers Util\n",
    "from transformer.util import d, here, mask_\n",
    "from transformer.transformers_gpu import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformer import util\n",
    "\n",
    "from torchtext import data, datasets, vocab\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, math\n",
    "from numpy.random import seed\n",
    "# from tensorflow import set_random_seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random, tqdm, sys, math, gzip\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, texts, labels, labels_b):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.text = texts\n",
    "        self.labels_b = labels_b\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.text[index,:,:]\n",
    "        y = self.labels[index]\n",
    "        y_b = self.labels_b[index]\n",
    "        return X, y, y_b\n",
    "    \n",
    "def go(arg):\n",
    "    \"\"\"\n",
    "    Creates and trains a basic transformer for the volatility regression task.\n",
    "    \"\"\"\n",
    "    LOG2E = math.log2(math.e)\n",
    "    NUM_CLS = 1\n",
    "    \n",
    "    print(\" Loading Data ...\")\n",
    "    TEXT_emb = np.load(arg.input_dir)\n",
    "    LABEL_emb = np.load(arg.label_dir)\n",
    "    LABEL_emb_b = np.load(arg.label_dir_b)\n",
    "    print(\" Finish Loading Data... \")\n",
    "    \n",
    "    if arg.final:\n",
    "        \n",
    "        train, test = train_test_split(TEXT_emb, test_size=0.2)\n",
    "        train_label, test_label = train_test_split(LABEL_emb, test_size=0.2)\n",
    "        train_label_b, test_label_b = train_test_split(LABEL_emb_b, test_size=0.2)\n",
    "        \n",
    "        training_set = Dataset(train, train_label, train_label_b)\n",
    "        val_set = Dataset(test, test_label, test_label_b)\n",
    "        \n",
    "    else:\n",
    "        data, _ = train_test_split(TEXT_emb, test_size=0.2)\n",
    "        train,val = train_test_split(data, test_size=0.125)\n",
    "        \n",
    "        data_label, _ = train_test_split(LABEL_emb, test_size=0.2) \n",
    "        train_label, val_label = train_test_split(data_label, test_size=0.125)\n",
    "        \n",
    "        data_label_b, _ = train_test_split(LABEL_emb_b, test_size=0.2)\n",
    "        train_label_b, val_label_b = train_test_split(data_label_b, test_size=0.125)\n",
    "        \n",
    "        \n",
    "        training_set = Dataset(train, train_label, train_label_b)\n",
    "        val_set = Dataset(val, val_label, val_label_b)\n",
    "        \n",
    "    trainloader=torch.utils.data.DataLoader(training_set, batch_size=arg.batch_size, shuffle=False, num_workers=2) \n",
    "    testloader=torch.utils.data.DataLoader(val_set, batch_size=len(val_set), shuffle=False, num_workers=2)\n",
    "    print('training examples', len(training_set))\n",
    "        \n",
    "    if arg.final:\n",
    "          print('test examples', len(val_set))\n",
    "    else:\n",
    "          print('validation examples', len(val_set))\n",
    "          \n",
    "\n",
    "    # create the model\n",
    "    model = RTransformer(emb=arg.embedding_size, heads=arg.num_heads, depth=arg.depth, \\\n",
    "                         seq_length=arg.max_length, num_tokens=arg.vocab_size, num_classes=NUM_CLS, max_pool=arg.max_pool)\n",
    "    \n",
    "    if arg.gpu:\n",
    "        if torch.cuda.is_available():\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = arg.cuda_id\n",
    "            model.cuda()\n",
    "        \n",
    "    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())\n",
    "\n",
    "    # training loop\n",
    "    seen = 0\n",
    "    evaluation= {'epoch': [],'Train Accuracy': [], 'Test Accuracy':[], 'Test Accuracy B':[], 'Outputs':[]}\n",
    "    for e in tqdm.tqdm_notebook(range(arg.num_epochs)):\n",
    "        train_loss_tol = 0.0\n",
    "        print('\\n epoch ',e)\n",
    "        model.train(True)\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # learning rate warmup\n",
    "            # - we linearly increase the learning rate from 10e-10 to arg.lr over the first\n",
    "            #   few thousand batches\n",
    "            if arg.lr_warmup > 0 and seen < arg.lr_warmup:\n",
    "                lr = max((arg.lr / arg.lr_warmup) * seen, 1e-10)\n",
    "                opt.lr = lr\n",
    "\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            inputs, labels, labels_b = data\n",
    "            inputs = Variable(inputs.type(torch.FloatTensor))\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).cuda()\n",
    "            labels_b = torch.tensor(labels_b, dtype=torch.float32).cuda()\n",
    "            #if i ==0:\n",
    "                #print (inputs.shape)\n",
    "            if inputs.size(1) > arg.max_length:\n",
    "                inputs = inputs[:, :arg.max_length, :]\n",
    "                \n",
    "            out_a,out_b = model(inputs)\n",
    "            #print(out_a.shape,out_b.shape)\n",
    "            #print(out.shape,labels.shape)\n",
    "\n",
    "            loss_a = F.mse_loss(out_a, labels)\n",
    "            loss_b = F.mse_loss(out_b, labels_b)\n",
    "            loss = arg.alpha*loss_a + (1 - arg.alpha)*loss_b\n",
    "            train_loss_tol += loss\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradients\n",
    "            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "            if arg.gradient_clipping > 0.0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            seen += inputs.size(0)\n",
    "            #tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
    "        #print('train_loss: ',train_loss_tol)\n",
    "        train_loss_tol = train_loss_tol/(i+1)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.train(False)\n",
    "            tot, cor= 0.0, 0.0\n",
    "\n",
    "            loss_test = 0.0\n",
    "            loss_test_b = 0.0\n",
    "            for i, data in enumerate(testloader):\n",
    "                inputs, labels, labels_b = data\n",
    "                inputs, labels, labels_b = torch.tensor(inputs, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32).cuda(), torch.tensor(labels_b, dtype=torch.float32).cuda()         \n",
    "                if inputs.size(1) > arg.max_length:\n",
    "                    inputs = inputs[:, :arg.max_length, :]\n",
    "                out_a,out_b = model(inputs)\n",
    "            \n",
    "                loss_test += F.mse_loss(out_a, labels)\n",
    "                loss_test_b += F.mse_loss(out_b, labels_b)\n",
    "                #tot = float(inputs.size(0))\n",
    "                #cor += float(labels.sum().item())\n",
    "\n",
    "            acc = loss_test          \n",
    "#             if arg.final:\n",
    "#                 print('test accuracy', acc)\n",
    "#             else:\n",
    "#                 print('validation accuracy', acc)\n",
    "        #torch.save(model, '/data/exp/checkpoints_torch_volatility/checkpoint-epoch'+str(e)+'.pth')\n",
    "        evaluation['epoch'].append(e)\n",
    "        evaluation['Train Accuracy'].append(train_loss_tol.item())\n",
    "        evaluation['Test Accuracy'].append(acc.item())\n",
    "        evaluation['Test Accuracy B'].append(loss_test_b.item())\n",
    "        evaluation['Outputs'].append(out_a)\n",
    "        \n",
    "    evaluation = pd.DataFrame(evaluation)\n",
    "    evaluation.sort_values([\"Test Accuracy\"],ascending=True,inplace=True)\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = {'alpha': [],'best':[]}\n",
    "for i in np.arange(0.1,1.1,0.1):   \n",
    "    if __name__ == \"__main__\":\n",
    "        #print('OPTIONS ', options)\n",
    "        # Tuning Parameters: \n",
    "        import easydict\n",
    "        from argparse import ArgumentParser\n",
    "        parser = ArgumentParser()\n",
    "        args = parser.parse_known_args()[0]\n",
    "        args = easydict.EasyDict({\n",
    "                \"num_epochs\": 10,\n",
    "                \"batch_size\": 4,\n",
    "                \"lr\": 2e-5,\n",
    "                \"tb_dir\": \"./runs\",\n",
    "                \"final\": False,\n",
    "                \"max_pool\": False,\n",
    "                \"embedding_size\" : 1051, # 1024(textual feature)+27(audio features)\n",
    "                \"max_length\" : 520,\n",
    "                \"num_heads\" : 2,\n",
    "                \"depth\" : 2,\n",
    "                \"seed\" : 1,\n",
    "                \"lr_warmup\" : 1000,\n",
    "                \"gradient_clipping\" : 1.0,\n",
    "                \"input_dir\": \"SENTENCE_EMBEDDING_DATA\",\n",
    "                \"label_dir\": \"MAIN_LABEL\",\n",
    "                \"label_dir_b\": \"AUXILIARY_LAEBL\",\n",
    "                \"alpha\" : i,\n",
    "                \"gpu\": True,\n",
    "                \"save\": False\n",
    "        })\n",
    "\n",
    "        evaluation = run_gpu.go(args)\n",
    "        print('Results in alpha = ',i)\n",
    "        print(evaluation)\n",
    "        best_alpha['alpha'].append(i)\n",
    "        best_alpha['best'].append(evaluation['Test Accuracy'].iloc[0])\n",
    "        \n",
    "best_alpha = pd.DataFrame(best_alpha)\n",
    "best_alpha.sort_values([\"best\"],ascending=True,inplace=True)\n",
    "best_alpha.to_csv('./results/3_days_result_vocal.csv')\n",
    "\n",
    "best_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
