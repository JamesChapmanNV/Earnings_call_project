{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Build on word-level text to generate a fixed-length vector for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reference: https://github.com/hanxiao/bert-as-service\n",
    "\n",
    "###### Notes: You must run the BertClient in your server first, then encode the text here by the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_serving'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af23e9d19e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_serving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Generate sentence representation by Bert-As-Service\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext_all_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bert_serving'"
     ]
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "\n",
    "#Generate sentence representation by Bert-As-Service\n",
    "text_all_embs = []\n",
    "sentence_len = []\n",
    "for i in range(len(text_all)):\n",
    "    text = text_all[i].split(\"\\n\")\n",
    "    sentence_len.append(len(text))\n",
    "    text_embs = bc.encode(text)\n",
    "    text_embs = np.concatenate((text_embs,np.array([np.array(past_volatility_all[i])]*len(text_embs))),axis=1)\n",
    "    text_all_embs.append(text_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "import numpy as np \n",
    "b = np.zeros([len(text_all_embs),len(max(text_all_embs,key = lambda x: len(x))),1027]) \n",
    "for i,j in enumerate(text_all_embs): \n",
    "    b[i][0:len(j),:] = j "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Customized Sentence-level Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from former.former import util\n",
    "from former.former.util import mask_\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for single task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        self.tokeys = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.tovalues = nn.Linear(emb, emb * heads, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb\n",
    "\n",
    "        keys    = self.tokeys(x)   .view(b, t, h, e)\n",
    "        queries = self.toqueries(x).view(b, t, h, e)\n",
    "        values  = self.tovalues(x) .view(b, t, h, e)\n",
    "\n",
    "        # compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the lower half of the dot matrix,including the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2) # dot now has row-wise self-attention probabilities\n",
    "\n",
    "        assert not util.contains_nan(dot[:, 1:, :]) # only the forst row may contain nan\n",
    "\n",
    "        if self.mask == 'first':\n",
    "            dot = dot.clone()\n",
    "            dot[:, :1, :] = 0.0\n",
    "            # - The first row of the first attention matrix is entirely masked out, so the softmax operation results\n",
    "            #   in a division by zero. We set this row to zero by hand to get rid of the NaNs\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n",
    "\n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb, heads=heads, mask=mask)\n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult * emb, emb)\n",
    "        )\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attended = self.attention(x)\n",
    "\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        fedforward = self.ff(x)\n",
    "\n",
    "        x = self.norm2(fedforward + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RTransformer\n",
    "\n",
    "class RTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for sequences Regression    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param emb: Embedding dimension\n",
    "        :param heads: nr. of attention heads\n",
    "        :param depth: Number of transformer blocks\n",
    "        :param seq_length: Expected maximum sequence length\n",
    "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
    "        :param num_classes: Number of classes.\n",
    "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "\n",
    "        #self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_classes)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        sentences_emb = x\n",
    "        b, t, e = x.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t))[None, :, :].expand(b, t, e)\n",
    "        #positions = torch.tensor(positions, dtype=torch.float32)\n",
    "        x = sentences_emb + positions\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
    "        x = self.toprobs(x)\n",
    "        x = torch.squeeze(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.text = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.text[index,:,:]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your own the whole dataset\n",
    "TEXT_emb = np.load(\"...\")\n",
    "LABEL_emb = np.load(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Build the Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8dacaa1479b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from former.experiments._context import former\n",
    "from former.former import util\n",
    "from former.former.util import mask_\n",
    "\n",
    "from util import d, here\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, tqdm, sys, math, gzip\n",
    "\n",
    "\n",
    "NUM_CLS = 1\n",
    "\n",
    "def go(arg):\n",
    "    \"\"\"\n",
    "    Creates and trains a basic transformer for any regression task.\n",
    "    \"\"\"\n",
    "\n",
    "    if arg.final:\n",
    "        \n",
    "        train, val = train_test_split(TEXT_emb, test_size=0.2)\n",
    "        train_label, val_label = train_test_split(LABEL_emb, test_size=0.2)\n",
    "        training_set = Dataset(train, train_label)\n",
    "        val_set = Dataset(val, val_label)\n",
    "        \n",
    "    else:\n",
    "        train, val = train_test_split(TEXT_emb, test_size=0.2)\n",
    "        train_label, val_label = train_test_split(LABEL_emb, test_size=0.2)        \n",
    "        train, val = train_test_split(train, test_size=0.2)\n",
    "        train_label, val_label = train_test_split(train_label, test_size=0.2)\n",
    "        \n",
    "        training_set = Dataset(train, train_label)\n",
    "        val_set = Dataset(val, val_label)\n",
    "        \n",
    "    trainloader=torch.utils.data.DataLoader(training_set, batch_size=arg.batch_size, shuffle=False, num_workers=2)\n",
    "    testloader=torch.utils.data.DataLoader(val_set, batch_size=len(val_set), shuffle=False, num_workers=2)\n",
    "    print('training examples', len(training_set))\n",
    "    #print(f'- nr. of {\"test\" if arg.final else \"validation\"} examples {len(test_iter)}')\n",
    "    \n",
    "    if arg.final:\n",
    "          print('test examples', len(val_set))\n",
    "    else:\n",
    "          print('validation examples', len(val_set))\n",
    "          \n",
    "\n",
    "    # create the model\n",
    "    model = RTransformer(emb=arg.embedding_size, heads=arg.num_heads, depth=arg.depth, \\\n",
    "                                seq_length=arg.max_length, num_tokens=arg.vocab_size, num_classes=NUM_CLS, max_pool=arg.max_pool)\n",
    "#     if torch.cuda.is_available():\n",
    "#         model.cuda()\n",
    "        \n",
    "    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())\n",
    "\n",
    "    # training loop\n",
    "    seen = 0\n",
    "    evaluation= {'epoch': [],'Train Accuracy': [], 'Test Accuracy':[]}\n",
    "    for e in tqdm.tqdm_notebook(range(arg.num_epochs)):\n",
    "        train_loss_tol = 0.0\n",
    "        print('\\n epoch ',e)\n",
    "        model.train(True)\n",
    "\n",
    "        for i, data in tqdm.tqdm_notebook(enumerate(trainloader)):\n",
    "            # learning rate warmup\n",
    "            # - we linearly increase the learning rate from 10e-10 to arg.lr over the first\n",
    "            #   few thousand batches\n",
    "            if arg.lr_warmup > 0 and seen < arg.lr_warmup:\n",
    "                lr = max((arg.lr / arg.lr_warmup) * seen, 1e-10)\n",
    "                opt.lr = lr\n",
    "\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            inputs, labels = data\n",
    "            inputs = Variable(inputs.type(torch.FloatTensor))\n",
    "            labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            if inputs.size(1) > arg.max_length:\n",
    "                inputs = inputs[:, :arg.max_length, :]\n",
    "            out = model(inputs)\n",
    "            \n",
    "            #print(out.shape,labels.shape)\n",
    "\n",
    "            loss = F.mse_loss(out, labels)\n",
    "            train_loss_tol += loss\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradients\n",
    "            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "            if arg.gradient_clipping > 0.0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            seen += inputs.size(0)\n",
    "            #tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
    "        #print('train_loss: ',train_loss_tol)\n",
    "        train_loss_tol = train_loss_tol/(i+1)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.train(False)\n",
    "            tot, cor= 0.0, 0.0\n",
    "\n",
    "            loss_test = 0.0\n",
    "            for i, data in tqdm.tqdm_notebook(enumerate(testloader)):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = torch.tensor(inputs, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "                if inputs.size(1) > arg.max_length:\n",
    "                    inputs = inputs[:, :arg.max_length, :]\n",
    "                out = model(inputs)\n",
    "            \n",
    "                loss_test += F.mse_loss(out, labels)\n",
    "                #tot = float(inputs.size(0))\n",
    "                #cor += float(labels.sum().item())\n",
    "\n",
    "            acc = loss_test          \n",
    "            if arg.final:\n",
    "                print('test accuracy', acc)\n",
    "            else:\n",
    "                print('validation accuracy', acc)\n",
    "                \n",
    "        torch.save(model, '/data/exp/checkpoints_torch_volatility/checkpoint-epoch'+str(e)+'.pth')\n",
    "        evaluation['epoch'].append(e)\n",
    "        evaluation['Train Accuracy'].append(train_loss_tol)\n",
    "        evaluation['Test Accuracy'].append(acc)\n",
    "        \n",
    "    evaluation = pd.DataFrame(evaluation)\n",
    "    evaluation.sort_values([\"Test Accuracy\"],ascending=True,inplace=True)\n",
    "    \n",
    "    return evaluation\n",
    "    #tbw.add_scalar('classification/test-loss', float(loss.item()), e)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #print('OPTIONS ', options)\n",
    "    # Tuning Parameters: \n",
    "    import easydict\n",
    "    from argparse import ArgumentParser\n",
    "    parser = ArgumentParser()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    args = easydict.EasyDict({\n",
    "            \"num_epochs\": 10,\n",
    "            \"batch_size\": 4,\n",
    "            \"lr\": 0.0005,\n",
    "            \"tb_dir\": \"./runs\",\n",
    "            \"final\": True,\n",
    "            \"max_pool\": False,\n",
    "            \"embedding_size\" : 1024,\n",
    "            \"vocab_size\" : 50000,\n",
    "            \"max_length\" : 520,\n",
    "            \"num_heads\" : 8,\n",
    "            \"depth\" : 12,\n",
    "            \"seed\" : 1,\n",
    "            \"lr_warmup\" : 500,\n",
    "            \"gradient_clipping\" : 1.0     \n",
    "    })\n",
    "\n",
    "    evaluation = go(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: For adapting for your own research, we provide the single-task version coder here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
