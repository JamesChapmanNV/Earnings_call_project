{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earnings Call Project: Data Cleaning\n",
    "<br>\n",
    "CIS 831 Deep Learning â€“ Term Project<br>\n",
    "Kansas State University\n",
    "<br><br>\n",
    "James Chapman<br>\n",
    "John Woods<br>\n",
    "Nathan Diehl<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the transformer architecture from the [HTML paper](https://www.researchgate.net/publication/340385140_HTML_Hierarchical_Transformer-based_Multi-task_Learning_for_Volatility_Prediction). \n",
    "\n",
    "From the previous notebooks, we have features for both audio and text, of each sentence, of each meeting.<br>\n",
    "- audio \n",
    "    - PRAAT (27 features)\n",
    "- text\n",
    "    - GLOVE (300 features)\n",
    "    - ROBERTA (1024 features)\n",
    "    - ROBERTA with averaging (1024 features)\n",
    "    - FinLang investopedia (768 features) from huggingface sentencetransformers\n",
    "    - BGE financial (1024 features) from huggingface sentencetransformers\n",
    "\n",
    "This notebook performs 3 nested loops. Each audio/text pair, and for each N_DAYS, we train models with 17 different alpha values. The alpha value the lowest MSE, based on the validation set, is been used to train a model with Both training and validation sets. This model is finally tested on the test set.\n",
    "\n",
    "These notebooks also give us a playground to work with the data, and test the models. Now we can insert sentiment detection, segmentation of the text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "torch.cuda.manual_seed_all(777)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static hyper-parameters from paper\n",
    "max_norm = 1.0\n",
    "num_epochs = 10\n",
    "warmup_steps = 1000\n",
    "batch_size=4\n",
    "dropout=0.5\n",
    "heads=2\n",
    "depth=2\n",
    "\n",
    "lr=0.000002 # 1/10 learning rate of paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "#fresh start\n",
    "\n",
    "directory_path = r\"C:\\Users\\James\\OneDrive\\Documents\\GitHub\\Earnings_call_project\\runs\"\n",
    "if os.path.exists(directory_path):\n",
    "    shutil.rmtree(directory_path)\n",
    "    \n",
    "path = \"C:/Users/James/AppData/Local/Temp/.tensorboard-info/\"\n",
    "for filename in os.listdir(path):\n",
    "    filepath = os.path.join(path, filename)\n",
    "    print(filepath)\n",
    "    os.remove(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the hierarchical transformer from the HTML paper.\n",
    "\n",
    "## This is taken directly from the GitHub account for HTML paper.\n",
    "- a few adaptions are highlighted with ##############################################\n",
    "\n",
    "\n",
    "- [GitHub HTML - util.py](https://github.com/YangLinyi/HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction/blob/master/Model/Sentence-Level-Transformer/transformers/util/util.py)\n",
    "- [GitHub HTML - transformers_gpu.py](https://github.com/YangLinyi/HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction/blob/master/Model/Sentence-Level-Transformer/transformers/transformers_gpu.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/YangLinyi/HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction/blob/master/Model/Sentence-Level-Transformer/transformers/transformers_gpu.py\n",
    "def mask_(matrices, maskval=0.0, mask_diagonal=True):\n",
    "    \"\"\"\n",
    "    Masks out all values in the given batch of matrices where i <= j holds,\n",
    "    i < j if mask_diagonal is false\n",
    "\n",
    "    In place operation\n",
    "\n",
    "    :param tns:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    b, h, w = matrices.size()\n",
    "\n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
    "    matrices[:, indices, indices[1]] = maskval\n",
    "\n",
    "def contains_nan(tensor):\n",
    "    return bool((tensor != tensor).sum() > 0)\n",
    "\n",
    "# https://github.com/YangLinyi/HTML-Hierarchical-Transformer-based-Multi-task-Learning-for-Volatility-Prediction/blob/master/Model/Sentence-Level-Transformer/transformers/transformers_gpu.py\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        self.tokeys = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.tovalues = nn.Linear(emb, emb * heads, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb\n",
    "\n",
    "        keys    = self.tokeys(x)   .view(b, t, h, e)\n",
    "        queries = self.toqueries(x).view(b, t, h, e)\n",
    "        values  = self.tovalues(x) .view(b, t, h, e)\n",
    "\n",
    "        # compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the lower half of the dot matrix,including the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2) # dot now has row-wise self-attention probabilities\n",
    "\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # assert not util.contains_nan(dot[:, 1:, :]) # only the forst row may contain nan\n",
    "        assert not contains_nan(dot[:, 1:, :]) # only the forst row may contain nan \n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        \n",
    "        if self.mask == 'first':\n",
    "            dot = dot.clone()\n",
    "            dot[:, :1, :] = 0.0\n",
    "            # - The first row of the first attention matrix is entirely masked out, so the softmax operation results\n",
    "            #   in a division by zero. We set this row to zero by hand to get rid of the NaNs\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n",
    "\n",
    "        return self.unifyheads(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb, heads=heads, mask=mask)\n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult * emb, emb)\n",
    "        )\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attended = self.attention(x)\n",
    "\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        fedforward = self.ff(x)\n",
    "\n",
    "        x = self.norm2(fedforward + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class RTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for sequences Regression    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0):\n",
    "        \"\"\"\n",
    "        emb: Embedding dimension\n",
    "        heads: nr. of attention heads\n",
    "        depth: Number of transformer blocks\n",
    "        seq_length: Expected maximum sequence length\n",
    "        num_tokens: Number of tokens (usually words) in the vocabulary\n",
    "        num_classes: Number of classes.\n",
    "        max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "\n",
    "        #self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_classes)\n",
    "        self.toprobs_b = nn.Linear(emb, num_classes)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        sentences_emb = x\n",
    "        b, t, e = x.size()\n",
    "\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        # swap d() for device\n",
    "        # positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        positions = self.pos_embedding(torch.arange(t, device=device))[None, :, :].expand(b, t, e)\n",
    "        ##############################################\n",
    "        ##############################################\n",
    "        #positions = self.pos_embedding(torch.arange(t))[None, :, :].expand(b, t, e)\n",
    "        #positions = torch.tensor(positions, dtype=torch.float32)\n",
    "        x = sentences_emb.cuda() + positions\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
    "        \n",
    "        \n",
    "        x_a = self.toprobs(x)\n",
    "        x_b = self.toprobs_b(x)\n",
    "        x_a = torch.squeeze(x_a)\n",
    "        x_b = torch.squeeze(x_b)\n",
    "        #print('x shape: ',x.shape)\n",
    "        return x_a, x_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset builder and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embeddings, labels, labels_b):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        self.labels_b = labels_b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = self.embeddings[idx]\n",
    "        label = self.labels[idx]\n",
    "        label_b = self.labels_b[idx]\n",
    "        return emb, label, label_b \n",
    "\n",
    "def get_data(data_directory, n_days, year, no_validation_set, is_using_scaler, batch_size):\n",
    "    # features\n",
    "    train_features = np.load('data/{}/train_features{}.npy'.format(data_directory, year))\n",
    "    val_features = np.load('data/{}/val_features{}.npy'.format(data_directory, year))\n",
    "    test_features = np.load('data/{}/test_features{}.npy'.format(data_directory, year))\n",
    "    # targets (n_days volatility)\n",
    "    train_targets = np.load('data/{}/train_targets{}_{}.npy'.format(data_directory, year, n_days))\n",
    "    val_targets = np.load('data/{}/val_targets{}_{}.npy'.format(data_directory, year, n_days))\n",
    "    test_targets = np.load('data/{}/test_targets{}_{}.npy'.format(data_directory, year, n_days))\n",
    "    # secondary targets (log percent change of day n)\n",
    "    train_secondary_targets = np.load('data/{}/train_secondary_targets{}_{}.npy'.format(data_directory, year, n_days))\n",
    "    val_secondary_targets = np.load('data/{}/val_secondary_targets{}_{}.npy'.format(data_directory, year, n_days))\n",
    "    test_secondary_targets = np.load('data/{}/test_secondary_targets{}_{}.npy'.format(data_directory, year, n_days))\n",
    "\n",
    "    # after hyperparameters are tuned on the validation set\n",
    "    # add validation set to training set\n",
    "    if no_validation_set:\n",
    "        train_features = np.concatenate((train_features, val_features), axis=0)\n",
    "        train_targets = np.concatenate((train_targets, val_targets), axis=0)\n",
    "        train_secondary_targets = np.concatenate((train_secondary_targets, val_secondary_targets), axis=0)\n",
    "\n",
    "    if is_using_scaler:\n",
    "        # Scaling features\n",
    "        feature_scaler = StandardScaler()\n",
    "        train_features = feature_scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "        val_features = feature_scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "        test_features = feature_scaler.transform(test_features.reshape(-1, test_features.shape[-1])).reshape(test_features.shape)\n",
    "        # Scaling primary targets\n",
    "        target_scaler = StandardScaler()\n",
    "        train_targets = target_scaler.fit_transform(train_targets.reshape(-1, 1)).reshape(train_targets.shape)\n",
    "        val_targets = target_scaler.transform(val_targets.reshape(-1, 1)).reshape(val_targets.shape)\n",
    "        test_targets = target_scaler.transform(test_targets.reshape(-1, 1)).reshape(test_targets.shape)\n",
    "        # Scaling secondary targets\n",
    "        secondary_target_scaler = StandardScaler()\n",
    "        train_secondary_targets = secondary_target_scaler.fit_transform(train_secondary_targets.reshape(-1, 1)).reshape(train_secondary_targets.shape)\n",
    "        val_secondary_targets = secondary_target_scaler.transform(val_secondary_targets.reshape(-1, 1)).reshape(val_secondary_targets.shape)\n",
    "        test_secondary_targets = secondary_target_scaler.transform(test_secondary_targets.reshape(-1, 1)).reshape(test_secondary_targets.shape)\n",
    "    # Dataset & DataLoader\n",
    "    training_set = Dataset(train_features, train_targets, train_secondary_targets) \n",
    "    val_set = Dataset(val_features, val_targets, val_secondary_targets)\n",
    "    test_set = Dataset(test_features, test_targets, test_secondary_targets)\n",
    "    trainloader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=len(val_set), shuffle=False, num_workers=0)\n",
    "    testloader = torch.utils.data.DataLoader(test_set, batch_size=len(test_set), shuffle=False, num_workers=0)\n",
    "\n",
    "    print(train_features.shape, train_targets.shape, train_secondary_targets.shape) \n",
    "    print(val_features.shape, val_targets.shape, val_secondary_targets.shape) \n",
    "    print(test_features.shape, test_targets.shape, test_secondary_targets.shape) \n",
    "\n",
    "    # train_features.shape[2] is the number of features, need to instantiate the model\n",
    "    # need target scaler to inverse transform\n",
    "    return trainloader, valloader, testloader, train_features.shape[2], target_scaler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this training loop is actually reused for training, validation, and testing\n",
    "def  training_loop(trainloader, loader, emb_dimensions, heads, depth,dropout, warmup_steps, \n",
    "                   batch_size, num_epochs, alphas, max_norm, lr, run_name, target_scaler):\n",
    "    alphas_results = []\n",
    "    for alpha in alphas:\n",
    "        model = RTransformer(emb=emb_dimensions, heads=heads, depth=depth, seq_length=523, \n",
    "                            num_tokens=4000, num_classes=1, max_pool=False, dropout=dropout).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # Linear LR warmup for the first #warmup_steps training examples\n",
    "        scheduler = LambdaLR(opt, lr_lambda=lambda step: min(1.0, step / (warmup_steps/batch_size)))\n",
    "\n",
    "        seen = 0\n",
    "        val_loss_a_history =  [] # keep track of the loss for the current Alpha value\n",
    "        writer = SummaryWriter(log_dir= f'runs/{run_name}_{alpha}')\n",
    "        progress_bar = tqdm(range(num_epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
    "        for epoch in progress_bar:\n",
    "            model.train()\n",
    "            train_loss_total = 0\n",
    "            for i, (inputs, labels, labels_b) in enumerate(trainloader): #training data\n",
    "                seen += inputs.size(0)\n",
    "                inputs = inputs.to(device, dtype=torch.float32)\n",
    "                labels = labels.to(device, dtype=torch.float32)\n",
    "                labels_b = labels_b.to(device, dtype=torch.float32)\n",
    "                out_a, out_b = model(inputs)\n",
    "                out_a = out_a.view(-1)\n",
    "                out_b = out_b.view(-1)\n",
    "\n",
    "                # Compute the combined loss (multitask)\n",
    "                loss_a = F.mse_loss(out_a, labels)\n",
    "                loss_b = F.mse_loss(out_b, labels_b)\n",
    "                loss = alpha * loss_a + (1 - alpha) * loss_b # Alpha parameter\n",
    "            \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "                opt.step()\n",
    "                if seen < warmup_steps:\n",
    "                    scheduler.step()\n",
    "\n",
    "                # we only care about the primary target\n",
    "                # although we are training with the scaled loss, reporting the volatility must be unscaled\n",
    "                out_a_unscaled = target_scaler.inverse_transform(out_a.detach().cpu().numpy().reshape(-1, 1)).reshape(out_a.shape)\n",
    "                labels_unscaled = target_scaler.inverse_transform(labels.detach().cpu().numpy().reshape(-1, 1)).reshape(labels.shape)\n",
    "                loss_a_unscaled = F.mse_loss(torch.tensor(out_a_unscaled, device=device), torch.tensor(labels_unscaled, device=device))\n",
    "                train_loss_total += loss_a_unscaled.item() * inputs.size(0) # MSE of volatility training (multiplied by current batch size)\n",
    "\n",
    "            # Epoch average of unscaled training loss\n",
    "            train_loss_avg = train_loss_total / len(trainloader.dataset)\n",
    "            \n",
    "            # Epoch Evaluation\n",
    "            model.eval()\n",
    "            val_loss_a_total = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, (inputs, labels, labels_b) in enumerate(loader): #validation or testing\n",
    "                    inputs = inputs.to(device, dtype=torch.float32)\n",
    "                    labels = labels.to(device, dtype=torch.float32)\n",
    "                    labels_b = labels_b.to(device, dtype=torch.float32)\n",
    "\n",
    "                    out_a, out_b = model(inputs)\n",
    "                    out_a = out_a.view(-1)\n",
    "                    \n",
    "                    out_a_unscaled = target_scaler.inverse_transform(out_a.detach().cpu().numpy().reshape(-1, 1)).reshape(out_a.shape)\n",
    "                    labels_unscaled = target_scaler.inverse_transform(labels.detach().cpu().numpy().reshape(-1, 1)).reshape(labels.shape)\n",
    "                    loss_a_unscaled = F.mse_loss(torch.tensor(out_a_unscaled, device=device), torch.tensor(labels_unscaled, device=device))\n",
    "                    val_loss_a_total += loss_a_unscaled.item() * inputs.size(0) # MSE of volatility\n",
    "\n",
    "                val_loss_a_avg = val_loss_a_total / len(loader.dataset) # MSE of volatility of Val/test!!\n",
    "            # Epoch finished\n",
    "            writer.add_scalar('Train Loss/train', train_loss_avg, epoch)\n",
    "            writer.add_scalar('Loss A/val', val_loss_a_avg, epoch)\n",
    "            # Update progress bar description and postfix\n",
    "            progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            progress_bar.set_postfix({\n",
    "                \"Train Loss\": f\"{train_loss_avg:.3f}\",\n",
    "                \"Test Loss A\": f\"{val_loss_a_avg:.3f}\"\n",
    "            })\n",
    "            # for alpha optimization, collect each Epoch's average loss\n",
    "            val_loss_a_history.append(val_loss_a_avg)\n",
    "        # all Epochs finished for this alpha value\n",
    "        min_loss = round(min(val_loss_a_history), 3)\n",
    "        alphas_results.append((alpha, min_loss))\n",
    "        writer.close()\n",
    "    print(alphas_results)\n",
    "    best_alpha, lowest_val_loss = min(alphas_results, key=lambda x: x[1])\n",
    "    return best_alpha, lowest_val_loss # if testING, this is the MSE  of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_val_test(data_directory, n_days, year):\n",
    "    run_name = data_directory + year + '_' + n_days \n",
    "    # using training set and validation set, determine optimum hyper-parameters (just alpha)\n",
    "    no_validation_set = False\n",
    "    trainloader,valloader,testloader,emb_dimensions, target_scaler = get_data(data_directory, n_days, year, no_validation_set, is_using_scaler, batch_size)\n",
    "    best_alpha, _ = training_loop(trainloader, valloader, emb_dimensions, heads, depth,dropout, warmup_steps, \n",
    "                            batch_size, num_epochs, alphas, max_norm, lr, run_name, target_scaler)\n",
    "    # using optimum alpha, retrain with all training/validation sets and test on testset\n",
    "    test_run_name = run_name + '_test'\n",
    "    no_validation_set = True\n",
    "    trainloader,valloader,testloader,emb_dimensions, target_scaler = get_data(data_directory, n_days, year, no_validation_set, is_using_scaler, batch_size)\n",
    "    _, MSE_testset = training_loop(trainloader, testloader, emb_dimensions, heads, depth, dropout, warmup_steps, \n",
    "                        batch_size, num_epochs, [best_alpha], max_norm, lr, test_run_name, target_scaler)\n",
    "\n",
    "    print('----------------------------------------')\n",
    "    print(f'run_name-----------{run_name}')\n",
    "    print(f'best_alpha---------{best_alpha}')\n",
    "    print(f'MSE_testset--------{MSE_testset}')\n",
    "    print('----------------------------------------')\n",
    "    return best_alpha, MSE_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.47s/epoch, Train Loss=0.743, Test Loss A=0.603]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.735, Test Loss A=0.619]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.734, Test Loss A=0.657]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.40s/epoch, Train Loss=0.740, Test Loss A=0.578]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.751, Test Loss A=0.847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.48), (0.3, 0.535), (0.5, 0.533), (0.7, 0.527), (0.9, 0.525)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.721, Test Loss A=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.687)]\n",
      "----------------------------------------\n",
      "run_name-----------Roberta_3\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.687\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.43s/epoch, Train Loss=0.429, Test Loss A=0.279]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.437, Test Loss A=0.260]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.430, Test Loss A=0.273]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.425, Test Loss A=0.266]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.424, Test Loss A=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.261), (0.3, 0.247), (0.5, 0.244), (0.7, 0.249), (0.9, 0.26)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.90s/epoch, Train Loss=0.409, Test Loss A=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.346)]\n",
      "----------------------------------------\n",
      "run_name-----------Roberta_7\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.346\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.43s/epoch, Train Loss=0.257, Test Loss A=0.179]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.254, Test Loss A=0.165]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.253, Test Loss A=0.167]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.249, Test Loss A=0.211]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.250, Test Loss A=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.168), (0.3, 0.164), (0.5, 0.162), (0.7, 0.167), (0.9, 0.165)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.238, Test Loss A=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.238)]\n",
      "----------------------------------------\n",
      "run_name-----------Roberta_15\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.238\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.188, Test Loss A=0.113]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.188, Test Loss A=0.115]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.189, Test Loss A=0.115]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.40s/epoch, Train Loss=0.183, Test Loss A=0.116]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.185, Test Loss A=0.127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.108), (0.3, 0.111), (0.5, 0.11), (0.7, 0.109), (0.9, 0.109)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.178, Test Loss A=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.189)]\n",
      "----------------------------------------\n",
      "run_name-----------Roberta_30\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.189\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.740, Test Loss A=0.505]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.729, Test Loss A=0.560]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.714, Test Loss A=0.567]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.714, Test Loss A=0.753]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.737, Test Loss A=0.604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.481), (0.3, 0.502), (0.5, 0.519), (0.7, 0.485), (0.9, 0.515)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.708, Test Loss A=0.750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.678)]\n",
      "----------------------------------------\n",
      "run_name-----------Roberta2_3\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.678\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.426, Test Loss A=0.250]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.416, Test Loss A=0.240]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.417, Test Loss A=0.255]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.398, Test Loss A=0.307]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.412, Test Loss A=0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.243), (0.3, 0.236), (0.5, 0.237), (0.7, 0.24), (0.9, 0.247)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.397, Test Loss A=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.316)]\n",
      "----------------------------------------\n",
      "run_name-----------Roberta2_7\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.316\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.43s/epoch, Train Loss=0.253, Test Loss A=0.161]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.248, Test Loss A=0.164]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.238, Test Loss A=0.155]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.239, Test Loss A=0.180]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.239, Test Loss A=0.148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.16), (0.3, 0.162), (0.5, 0.155), (0.7, 0.155), (0.9, 0.148)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.87s/epoch, Train Loss=0.219, Test Loss A=0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.244)]\n",
      "----------------------------------------\n",
      "run_name-----------Roberta2_15\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.244\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.187, Test Loss A=0.106]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.185, Test Loss A=0.106]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.181, Test Loss A=0.112]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.178, Test Loss A=0.156]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.176, Test Loss A=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.106), (0.3, 0.106), (0.5, 0.101), (0.7, 0.106), (0.9, 0.107)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.167, Test Loss A=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.185)]\n",
      "----------------------------------------\n",
      "run_name-----------Roberta2_30\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.185\n",
      "----------------------------------------\n",
      "(391, 523, 795) (391,) (391,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.747, Test Loss A=0.513]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.715, Test Loss A=0.618]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.729, Test Loss A=0.583]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.718, Test Loss A=0.552]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.728, Test Loss A=0.595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.497), (0.3, 0.531), (0.5, 0.542), (0.7, 0.517), (0.9, 0.538)]\n",
      "(445, 523, 795) (445,) (445,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.96s/epoch, Train Loss=0.719, Test Loss A=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.726)]\n",
      "----------------------------------------\n",
      "run_name-----------investopedia_3\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.726\n",
      "----------------------------------------\n",
      "(391, 523, 795) (391,) (391,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.67s/epoch, Train Loss=0.428, Test Loss A=0.252]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.417, Test Loss A=0.235]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.413, Test Loss A=0.266]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.415, Test Loss A=0.237]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.419, Test Loss A=0.246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.251), (0.3, 0.235), (0.5, 0.238), (0.7, 0.237), (0.9, 0.245)]\n",
      "(445, 523, 795) (445,) (445,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.96s/epoch, Train Loss=0.398, Test Loss A=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.337)]\n",
      "----------------------------------------\n",
      "run_name-----------investopedia_7\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.337\n",
      "----------------------------------------\n",
      "(391, 523, 795) (391,) (391,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.252, Test Loss A=0.164]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.249, Test Loss A=0.155]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.248, Test Loss A=0.156]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.248, Test Loss A=0.155]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.243, Test Loss A=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.164), (0.3, 0.155), (0.5, 0.156), (0.7, 0.155), (0.9, 0.16)]\n",
      "(445, 523, 795) (445,) (445,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.95s/epoch, Train Loss=0.237, Test Loss A=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.243)]\n",
      "----------------------------------------\n",
      "run_name-----------investopedia_15\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.243\n",
      "----------------------------------------\n",
      "(391, 523, 795) (391,) (391,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.188, Test Loss A=0.112]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.187, Test Loss A=0.106]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.186, Test Loss A=0.100]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.182, Test Loss A=0.099]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.183, Test Loss A=0.115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.109), (0.3, 0.106), (0.5, 0.1), (0.7, 0.099), (0.9, 0.105)]\n",
      "(445, 523, 795) (445,) (445,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.95s/epoch, Train Loss=0.173, Test Loss A=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.193)]\n",
      "----------------------------------------\n",
      "run_name-----------investopedia_30\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.193\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.39s/epoch, Train Loss=0.745, Test Loss A=0.564]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.36s/epoch, Train Loss=0.721, Test Loss A=0.556]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.37s/epoch, Train Loss=0.722, Test Loss A=0.694]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.36s/epoch, Train Loss=0.713, Test Loss A=0.670]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.38s/epoch, Train Loss=0.715, Test Loss A=0.615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.525), (0.3, 0.532), (0.5, 0.543), (0.7, 0.548), (0.9, 0.555)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.82s/epoch, Train Loss=0.707, Test Loss A=0.677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.677)]\n",
      "----------------------------------------\n",
      "run_name-----------bge_3\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.677\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.39s/epoch, Train Loss=0.425, Test Loss A=0.261]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.37s/epoch, Train Loss=0.429, Test Loss A=0.256]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.37s/epoch, Train Loss=0.415, Test Loss A=0.242]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.37s/epoch, Train Loss=0.414, Test Loss A=0.241]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.37s/epoch, Train Loss=0.411, Test Loss A=0.239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.259), (0.3, 0.245), (0.5, 0.241), (0.7, 0.241), (0.9, 0.234)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.81s/epoch, Train Loss=0.386, Test Loss A=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.351)]\n",
      "----------------------------------------\n",
      "run_name-----------bge_7\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.351\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.42s/epoch, Train Loss=0.254, Test Loss A=0.159]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.245, Test Loss A=0.156]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.249, Test Loss A=0.172]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.239, Test Loss A=0.170]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.240, Test Loss A=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.159), (0.3, 0.156), (0.5, 0.164), (0.7, 0.156), (0.9, 0.153)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.228, Test Loss A=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.239)]\n",
      "----------------------------------------\n",
      "run_name-----------bge_15\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.239\n",
      "----------------------------------------\n",
      "(391, 523, 1051) (391,) (391,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.43s/epoch, Train Loss=0.189, Test Loss A=0.105]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.190, Test Loss A=0.104]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.40s/epoch, Train Loss=0.181, Test Loss A=0.100]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.182, Test Loss A=0.098]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.41s/epoch, Train Loss=0.182, Test Loss A=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.105), (0.3, 0.104), (0.5, 0.1), (0.7, 0.098), (0.9, 0.1)]\n",
      "(445, 523, 1051) (445,) (445,)\n",
      "(54, 523, 1051) (54,) (54,)\n",
      "(115, 523, 1051) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.89s/epoch, Train Loss=0.172, Test Loss A=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.184)]\n",
      "----------------------------------------\n",
      "run_name-----------bge_30\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.184\n",
      "----------------------------------------\n",
      "(391, 523, 795) (391,) (391,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.67s/epoch, Train Loss=0.740, Test Loss A=0.505]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.730, Test Loss A=0.561]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.723, Test Loss A=0.522]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.717, Test Loss A=0.647]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.715, Test Loss A=0.600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.5), (0.3, 0.537), (0.5, 0.486), (0.7, 0.526), (0.9, 0.513)]\n",
      "(445, 523, 795) (445,) (445,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.96s/epoch, Train Loss=0.698, Test Loss A=0.729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.677)]\n",
      "----------------------------------------\n",
      "run_name-----------bge_base_3\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.677\n",
      "----------------------------------------\n",
      "(391, 523, 795) (391,) (391,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.421, Test Loss A=0.244]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.417, Test Loss A=0.273]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.411, Test Loss A=0.235]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.409, Test Loss A=0.224]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.67s/epoch, Train Loss=0.409, Test Loss A=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.244), (0.3, 0.248), (0.5, 0.229), (0.7, 0.224), (0.9, 0.231)]\n",
      "(445, 523, 795) (445,) (445,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.96s/epoch, Train Loss=0.384, Test Loss A=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.331)]\n",
      "----------------------------------------\n",
      "run_name-----------bge_base_7\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.331\n",
      "----------------------------------------\n",
      "(391, 523, 795) (391,) (391,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.254, Test Loss A=0.168]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.247, Test Loss A=0.159]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.245, Test Loss A=0.147]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.241, Test Loss A=0.150]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.243, Test Loss A=0.148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.168), (0.3, 0.159), (0.5, 0.147), (0.7, 0.15), (0.9, 0.148)]\n",
      "(445, 523, 795) (445,) (445,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.95s/epoch, Train Loss=0.231, Test Loss A=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.248)]\n",
      "----------------------------------------\n",
      "run_name-----------bge_base_15\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.248\n",
      "----------------------------------------\n",
      "(391, 523, 795) (391,) (391,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.189, Test Loss A=0.107]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.184, Test Loss A=0.104]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.68s/epoch, Train Loss=0.182, Test Loss A=0.104]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.66s/epoch, Train Loss=0.180, Test Loss A=0.095]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/epoch, Train Loss=0.184, Test Loss A=0.097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.107), (0.3, 0.099), (0.5, 0.103), (0.7, 0.095), (0.9, 0.096)]\n",
      "(445, 523, 795) (445,) (445,)\n",
      "(54, 523, 795) (54,) (54,)\n",
      "(115, 523, 795) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.98s/epoch, Train Loss=0.170, Test Loss A=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.187)]\n",
      "----------------------------------------\n",
      "run_name-----------bge_base_30\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.187\n",
      "----------------------------------------\n",
      "(391, 523, 327) (391,) (391,)\n",
      "(54, 523, 327) (54,) (54,)\n",
      "(115, 523, 327) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.59epoch/s, Train Loss=0.748, Test Loss A=0.477]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.56epoch/s, Train Loss=0.745, Test Loss A=0.520]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.51epoch/s, Train Loss=0.744, Test Loss A=0.504]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.53epoch/s, Train Loss=0.747, Test Loss A=0.538]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.64epoch/s, Train Loss=0.746, Test Loss A=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.47), (0.3, 0.519), (0.5, 0.504), (0.7, 0.537), (0.9, 0.499)]\n",
      "(445, 523, 327) (445,) (445,)\n",
      "(54, 523, 327) (54,) (54,)\n",
      "(115, 523, 327) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.38epoch/s, Train Loss=0.727, Test Loss A=0.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.748)]\n",
      "----------------------------------------\n",
      "run_name-----------glove_3\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.748\n",
      "----------------------------------------\n",
      "(391, 523, 327) (391,) (391,)\n",
      "(54, 523, 327) (54,) (54,)\n",
      "(115, 523, 327) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.65epoch/s, Train Loss=0.428, Test Loss A=0.253]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.64epoch/s, Train Loss=0.433, Test Loss A=0.259]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.58epoch/s, Train Loss=0.428, Test Loss A=0.246]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.63epoch/s, Train Loss=0.429, Test Loss A=0.244]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.62epoch/s, Train Loss=0.433, Test Loss A=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.249), (0.3, 0.249), (0.5, 0.246), (0.7, 0.243), (0.9, 0.249)]\n",
      "(445, 523, 327) (445,) (445,)\n",
      "(54, 523, 327) (54,) (54,)\n",
      "(115, 523, 327) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.33epoch/s, Train Loss=0.406, Test Loss A=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.383)]\n",
      "----------------------------------------\n",
      "run_name-----------glove_7\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.383\n",
      "----------------------------------------\n",
      "(391, 523, 327) (391,) (391,)\n",
      "(54, 523, 327) (54,) (54,)\n",
      "(115, 523, 327) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.63epoch/s, Train Loss=0.256, Test Loss A=0.168]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.66epoch/s, Train Loss=0.255, Test Loss A=0.179]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.57epoch/s, Train Loss=0.254, Test Loss A=0.168]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.54epoch/s, Train Loss=0.256, Test Loss A=0.169]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.49epoch/s, Train Loss=0.254, Test Loss A=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.168), (0.3, 0.179), (0.5, 0.166), (0.7, 0.169), (0.9, 0.167)]\n",
      "(445, 523, 327) (445,) (445,)\n",
      "(54, 523, 327) (54,) (54,)\n",
      "(115, 523, 327) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.30epoch/s, Train Loss=0.244, Test Loss A=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.262)]\n",
      "----------------------------------------\n",
      "run_name-----------glove_15\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.262\n",
      "----------------------------------------\n",
      "(391, 523, 327) (391,) (391,)\n",
      "(54, 523, 327) (54,) (54,)\n",
      "(115, 523, 327) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.59epoch/s, Train Loss=0.189, Test Loss A=0.116]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.60epoch/s, Train Loss=0.191, Test Loss A=0.112]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.56epoch/s, Train Loss=0.191, Test Loss A=0.115]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.55epoch/s, Train Loss=0.189, Test Loss A=0.114]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.65epoch/s, Train Loss=0.189, Test Loss A=0.110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.116), (0.3, 0.111), (0.5, 0.114), (0.7, 0.114), (0.9, 0.11)]\n",
      "(445, 523, 327) (445,) (445,)\n",
      "(54, 523, 327) (54,) (54,)\n",
      "(115, 523, 327) (115,) (115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.32epoch/s, Train Loss=0.180, Test Loss A=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.2)]\n",
      "----------------------------------------\n",
      "run_name-----------glove_30\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.2\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "is_using_scaler = True # whether or not to use standardscaler\n",
    "directories = ['Roberta', 'Roberta2', 'investopedia', 'bge', 'bge_base', 'glove']#\n",
    "all_n_days = ['3', '7', '15', '30']\n",
    "alphas = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "final_results = []\n",
    "for data_directory in directories: # different feature engineering features\n",
    "    for n_days in all_n_days: \n",
    "        year = ''\n",
    "        best_alpha, MSE_testset = train_val_test(data_directory, n_days, year)\n",
    "        final_results.append([data_directory, n_days, best_alpha, MSE_testset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding Variant</th>\n",
       "      <th>n_days</th>\n",
       "      <th>best_alpha</th>\n",
       "      <th>MSE_testset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roberta</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roberta</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roberta</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roberta</td>\n",
       "      <td>30</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roberta2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Roberta2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Roberta2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Roberta2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>investopedia</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>investopedia</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>investopedia</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>investopedia</td>\n",
       "      <td>30</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bge</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bge</td>\n",
       "      <td>7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bge</td>\n",
       "      <td>15</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bge</td>\n",
       "      <td>30</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bge_base</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bge_base</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bge_base</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bge_base</td>\n",
       "      <td>30</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>glove</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>glove</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>glove</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>glove</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Embedding Variant  n_days  best_alpha  MSE_testset\n",
       "0            Roberta       3         0.1        0.687\n",
       "1            Roberta       7         0.5        0.346\n",
       "2            Roberta      15         0.5        0.238\n",
       "3            Roberta      30         0.1        0.189\n",
       "4           Roberta2       3         0.1        0.678\n",
       "5           Roberta2       7         0.3        0.316\n",
       "6           Roberta2      15         0.9        0.244\n",
       "7           Roberta2      30         0.5        0.185\n",
       "8       investopedia       3         0.1        0.726\n",
       "9       investopedia       7         0.3        0.337\n",
       "10      investopedia      15         0.3        0.243\n",
       "11      investopedia      30         0.7        0.193\n",
       "12               bge       3         0.1        0.677\n",
       "13               bge       7         0.9        0.351\n",
       "14               bge      15         0.9        0.239\n",
       "15               bge      30         0.7        0.184\n",
       "16          bge_base       3         0.5        0.677\n",
       "17          bge_base       7         0.7        0.331\n",
       "18          bge_base      15         0.5        0.248\n",
       "19          bge_base      30         0.7        0.187\n",
       "20             glove       3         0.1        0.748\n",
       "21             glove       7         0.7        0.383\n",
       "22             glove      15         0.5        0.262\n",
       "23             glove      30         0.9        0.200"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = pd.DataFrame(final_results, columns=['Embedding Variant', 'n_days', 'best_alpha', 'MSE_testset'])\n",
    "final_results.to_csv('data/final_results.csv', index=False)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>n_days</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding Variant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Roberta</th>\n",
       "      <td>0.687</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberta2</th>\n",
       "      <td>0.678</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bge</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bge_base</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>investopedia</th>\n",
       "      <td>0.726</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "n_days                3      7      15     30\n",
       "Embedding Variant                            \n",
       "Roberta            0.687  0.346  0.238  0.189\n",
       "Roberta2           0.678  0.316  0.244  0.185\n",
       "bge                0.677  0.351  0.239  0.184\n",
       "bge_base           0.677  0.331  0.248  0.187\n",
       "glove              0.748  0.383  0.262  0.200\n",
       "investopedia       0.726  0.337  0.243  0.193"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = final_results.drop(columns=['best_alpha'])\n",
    "final_results = final_results.pivot(index='Embedding Variant', columns='n_days', values='MSE_testset')\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test MAEC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.87s/epoch, Train Loss=0.760, Test Loss A=0.916]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.81s/epoch, Train Loss=0.747, Test Loss A=0.947]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.83s/epoch, Train Loss=0.747, Test Loss A=1.080]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.93s/epoch, Train Loss=0.753, Test Loss A=0.959]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.85s/epoch, Train Loss=0.754, Test Loss A=1.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.893), (0.3, 0.943), (0.5, 0.924), (0.7, 0.924), (0.9, 0.908)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.38s/epoch, Train Loss=0.782, Test Loss A=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.718)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2015_3\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.718\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.40s/epoch, Train Loss=0.838, Test Loss A=1.022]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.21s/epoch, Train Loss=0.822, Test Loss A=1.089]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:51<00:00,  5.19s/epoch, Train Loss=0.817, Test Loss A=1.020]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.23s/epoch, Train Loss=0.820, Test Loss A=1.204]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.21s/epoch, Train Loss=0.809, Test Loss A=1.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 1.002), (0.3, 1.043), (0.5, 1.02), (0.7, 0.977), (0.9, 1.016)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:03<00:00,  6.32s/epoch, Train Loss=0.836, Test Loss A=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.686)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2016_3\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.686\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:47<00:00,  4.78s/epoch, Train Loss=0.747, Test Loss A=0.956]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.88s/epoch, Train Loss=0.759, Test Loss A=0.992]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:50<00:00,  5.04s/epoch, Train Loss=0.757, Test Loss A=0.996]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:47<00:00,  4.76s/epoch, Train Loss=0.745, Test Loss A=1.062]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:47<00:00,  4.76s/epoch, Train Loss=0.765, Test Loss A=1.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.914), (0.3, 0.943), (0.5, 0.903), (0.7, 0.944), (0.9, 0.909)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:56<00:00,  5.70s/epoch, Train Loss=0.767, Test Loss A=0.934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.868)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2017_3\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.868\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.80s/epoch, Train Loss=0.384, Test Loss A=0.549]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.78s/epoch, Train Loss=0.373, Test Loss A=0.389]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.78s/epoch, Train Loss=0.372, Test Loss A=0.549]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.78s/epoch, Train Loss=0.373, Test Loss A=0.798]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.79s/epoch, Train Loss=0.369, Test Loss A=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.46), (0.3, 0.389), (0.5, 0.443), (0.7, 0.415), (0.9, 0.399)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.29s/epoch, Train Loss=0.379, Test Loss A=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.377)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2015_7\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.377\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:51<00:00,  5.20s/epoch, Train Loss=0.441, Test Loss A=0.504]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.22s/epoch, Train Loss=0.435, Test Loss A=0.560]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.27s/epoch, Train Loss=0.434, Test Loss A=0.467]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.34s/epoch, Train Loss=0.438, Test Loss A=0.503]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:51<00:00,  5.19s/epoch, Train Loss=0.432, Test Loss A=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.472), (0.3, 0.472), (0.5, 0.467), (0.7, 0.474), (0.9, 0.452)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:02<00:00,  6.29s/epoch, Train Loss=0.440, Test Loss A=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.338)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2016_7\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.338\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:47<00:00,  4.78s/epoch, Train Loss=0.401, Test Loss A=0.570]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:47<00:00,  4.78s/epoch, Train Loss=0.396, Test Loss A=0.621]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:47<00:00,  4.79s/epoch, Train Loss=0.400, Test Loss A=0.603]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:47<00:00,  4.76s/epoch, Train Loss=0.389, Test Loss A=0.581]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.81s/epoch, Train Loss=0.396, Test Loss A=0.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.57), (0.3, 0.555), (0.5, 0.542), (0.7, 0.526), (0.9, 0.52)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:57<00:00,  5.74s/epoch, Train Loss=0.398, Test Loss A=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.399)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2017_7\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.399\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.80s/epoch, Train Loss=0.278, Test Loss A=0.381]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.78s/epoch, Train Loss=0.274, Test Loss A=0.320]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.78s/epoch, Train Loss=0.275, Test Loss A=0.393]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.83s/epoch, Train Loss=0.277, Test Loss A=0.432]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.83s/epoch, Train Loss=0.280, Test Loss A=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.272), (0.3, 0.255), (0.5, 0.28), (0.7, 0.279), (0.9, 0.265)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.31s/epoch, Train Loss=0.277, Test Loss A=0.340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.252)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2015_15\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.252\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.26s/epoch, Train Loss=0.307, Test Loss A=0.350]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.39s/epoch, Train Loss=0.306, Test Loss A=0.354]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.36s/epoch, Train Loss=0.307, Test Loss A=0.423]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.33s/epoch, Train Loss=0.302, Test Loss A=0.326]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.22s/epoch, Train Loss=0.307, Test Loss A=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.347), (0.3, 0.346), (0.5, 0.401), (0.7, 0.326), (0.9, 0.332)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:03<00:00,  6.35s/epoch, Train Loss=0.316, Test Loss A=0.220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.185)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2016_15\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.185\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.96s/epoch, Train Loss=0.278, Test Loss A=0.488]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.97s/epoch, Train Loss=0.275, Test Loss A=0.488]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.87s/epoch, Train Loss=0.272, Test Loss A=0.505]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.85s/epoch, Train Loss=0.276, Test Loss A=0.533]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:50<00:00,  5.08s/epoch, Train Loss=0.274, Test Loss A=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.311), (0.3, 0.394), (0.5, 0.404), (0.7, 0.337), (0.9, 0.336)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:59<00:00,  5.94s/epoch, Train Loss=0.288, Test Loss A=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.326)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2017_15\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.326\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.94s/epoch, Train Loss=0.195, Test Loss A=0.191]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.93s/epoch, Train Loss=0.194, Test Loss A=0.173]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.95s/epoch, Train Loss=0.194, Test Loss A=0.184]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.91s/epoch, Train Loss=0.194, Test Loss A=0.175]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.91s/epoch, Train Loss=0.196, Test Loss A=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.183), (0.3, 0.171), (0.5, 0.176), (0.7, 0.175), (0.9, 0.175)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  3.46s/epoch, Train Loss=0.192, Test Loss A=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.189)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2015_30\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.189\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.41s/epoch, Train Loss=0.239, Test Loss A=0.192]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.36s/epoch, Train Loss=0.238, Test Loss A=0.193]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.37s/epoch, Train Loss=0.238, Test Loss A=0.186]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.38s/epoch, Train Loss=0.233, Test Loss A=0.260]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.29s/epoch, Train Loss=0.235, Test Loss A=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.19), (0.3, 0.188), (0.5, 0.186), (0.7, 0.195), (0.9, 0.178)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:03<00:00,  6.37s/epoch, Train Loss=0.225, Test Loss A=0.160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.131)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2016_30\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.131\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.92s/epoch, Train Loss=0.215, Test Loss A=0.420]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.94s/epoch, Train Loss=0.216, Test Loss A=0.310]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.89s/epoch, Train Loss=0.214, Test Loss A=0.307]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.98s/epoch, Train Loss=0.215, Test Loss A=0.398]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  5.00s/epoch, Train Loss=0.215, Test Loss A=0.399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.258), (0.3, 0.227), (0.5, 0.22), (0.7, 0.27), (0.9, 0.291)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:02<00:00,  6.21s/epoch, Train Loss=0.217, Test Loss A=0.308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.267)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta_2017_30\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.267\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.04s/epoch, Train Loss=0.745, Test Loss A=0.997]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.05s/epoch, Train Loss=0.735, Test Loss A=0.898]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.05s/epoch, Train Loss=0.728, Test Loss A=1.124]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.06s/epoch, Train Loss=0.721, Test Loss A=0.940]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.06s/epoch, Train Loss=0.727, Test Loss A=0.974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.929), (0.3, 0.879), (0.5, 0.898), (0.7, 0.911), (0.9, 0.934)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:36<00:00,  3.66s/epoch, Train Loss=0.760, Test Loss A=0.676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.675)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2015_3\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.675\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:57<00:00,  5.74s/epoch, Train Loss=0.810, Test Loss A=1.130]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:57<00:00,  5.73s/epoch, Train Loss=0.790, Test Loss A=1.042]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:57<00:00,  5.78s/epoch, Train Loss=0.784, Test Loss A=1.129]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:57<00:00,  5.73s/epoch, Train Loss=0.775, Test Loss A=1.107]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:56<00:00,  5.64s/epoch, Train Loss=0.781, Test Loss A=1.325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 1.02), (0.3, 1.042), (0.5, 1.021), (0.7, 0.986), (0.9, 0.977)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:08<00:00,  6.85s/epoch, Train Loss=0.804, Test Loss A=0.886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.678)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2016_3\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.678\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.24s/epoch, Train Loss=0.739, Test Loss A=0.957]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:50<00:00,  5.05s/epoch, Train Loss=0.720, Test Loss A=0.877]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:50<00:00,  5.03s/epoch, Train Loss=0.709, Test Loss A=0.873]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.93s/epoch, Train Loss=0.707, Test Loss A=0.874]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.92s/epoch, Train Loss=0.698, Test Loss A=0.939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.87), (0.3, 0.873), (0.5, 0.802), (0.7, 0.803), (0.9, 0.79)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:59<00:00,  5.91s/epoch, Train Loss=0.712, Test Loss A=1.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.829)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2017_3\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.829\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.90s/epoch, Train Loss=0.371, Test Loss A=0.452]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.86s/epoch, Train Loss=0.356, Test Loss A=0.445]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.87s/epoch, Train Loss=0.362, Test Loss A=0.460]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.86s/epoch, Train Loss=0.355, Test Loss A=0.580]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.353, Test Loss A=0.564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.425), (0.3, 0.4), (0.5, 0.452), (0.7, 0.433), (0.9, 0.421)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  3.45s/epoch, Train Loss=0.372, Test Loss A=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.359)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2015_7\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.359\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.43s/epoch, Train Loss=0.431, Test Loss A=0.469]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.49s/epoch, Train Loss=0.422, Test Loss A=0.484]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.34s/epoch, Train Loss=0.404, Test Loss A=0.452]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.32s/epoch, Train Loss=0.414, Test Loss A=0.457]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.39s/epoch, Train Loss=0.410, Test Loss A=0.505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.446), (0.3, 0.445), (0.5, 0.428), (0.7, 0.428), (0.9, 0.435)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:06<00:00,  6.65s/epoch, Train Loss=0.425, Test Loss A=0.368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.318)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2016_7\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.318\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:50<00:00,  5.04s/epoch, Train Loss=0.398, Test Loss A=0.457]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.21s/epoch, Train Loss=0.368, Test Loss A=0.491]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.96s/epoch, Train Loss=0.362, Test Loss A=0.538]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  5.00s/epoch, Train Loss=0.355, Test Loss A=0.547]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.92s/epoch, Train Loss=0.354, Test Loss A=0.588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.457), (0.3, 0.446), (0.5, 0.399), (0.7, 0.458), (0.9, 0.416)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:59<00:00,  6.00s/epoch, Train Loss=0.359, Test Loss A=0.684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.399)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2017_7\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.399\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.93s/epoch, Train Loss=0.275, Test Loss A=0.294]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.90s/epoch, Train Loss=0.265, Test Loss A=0.383]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.92s/epoch, Train Loss=0.263, Test Loss A=0.372]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.259, Test Loss A=0.385]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.06s/epoch, Train Loss=0.256, Test Loss A=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.254), (0.3, 0.261), (0.5, 0.257), (0.7, 0.257), (0.9, 0.254)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:36<00:00,  3.68s/epoch, Train Loss=0.273, Test Loss A=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.234)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2015_15\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.234\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:55<00:00,  5.58s/epoch, Train Loss=0.304, Test Loss A=0.359]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:56<00:00,  5.64s/epoch, Train Loss=0.292, Test Loss A=0.328]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.42s/epoch, Train Loss=0.290, Test Loss A=0.318]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.46s/epoch, Train Loss=0.288, Test Loss A=0.336]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.50s/epoch, Train Loss=0.289, Test Loss A=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.338), (0.3, 0.317), (0.5, 0.31), (0.7, 0.318), (0.9, 0.309)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:05<00:00,  6.59s/epoch, Train Loss=0.296, Test Loss A=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.197)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2016_15\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.197\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.95s/epoch, Train Loss=0.279, Test Loss A=0.478]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.22s/epoch, Train Loss=0.255, Test Loss A=0.487]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:50<00:00,  5.01s/epoch, Train Loss=0.242, Test Loss A=0.460]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.93s/epoch, Train Loss=0.241, Test Loss A=0.468]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.23s/epoch, Train Loss=0.238, Test Loss A=0.480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.361), (0.3, 0.281), (0.5, 0.324), (0.7, 0.33), (0.9, 0.324)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:00<00:00,  6.02s/epoch, Train Loss=0.251, Test Loss A=0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.263)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2017_15\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.263\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.97s/epoch, Train Loss=0.192, Test Loss A=0.176]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.91s/epoch, Train Loss=0.188, Test Loss A=0.212]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.90s/epoch, Train Loss=0.186, Test Loss A=0.221]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.99s/epoch, Train Loss=0.187, Test Loss A=0.187]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.94s/epoch, Train Loss=0.188, Test Loss A=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.17), (0.3, 0.171), (0.5, 0.173), (0.7, 0.17), (0.9, 0.162)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  3.49s/epoch, Train Loss=0.191, Test Loss A=0.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.179)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2015_30\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.179\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:54<00:00,  5.44s/epoch, Train Loss=0.234, Test Loss A=0.180]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.34s/epoch, Train Loss=0.228, Test Loss A=0.230]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.33s/epoch, Train Loss=0.225, Test Loss A=0.334]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.26s/epoch, Train Loss=0.218, Test Loss A=0.250]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.25s/epoch, Train Loss=0.219, Test Loss A=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.18), (0.3, 0.186), (0.5, 0.171), (0.7, 0.191), (0.9, 0.173)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:03<00:00,  6.37s/epoch, Train Loss=0.212, Test Loss A=0.309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.131)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2016_30\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.131\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.218, Test Loss A=0.315]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.195, Test Loss A=0.361]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.85s/epoch, Train Loss=0.192, Test Loss A=0.424]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.190, Test Loss A=0.348]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.82s/epoch, Train Loss=0.183, Test Loss A=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.218), (0.3, 0.229), (0.5, 0.217), (0.7, 0.262), (0.9, 0.227)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:57<00:00,  5.72s/epoch, Train Loss=0.189, Test Loss A=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.207)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_Roberta2_2017_30\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.207\n",
      "----------------------------------------\n",
      "(485, 500, 795) (485,) (485,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.86s/epoch, Train Loss=0.737, Test Loss A=0.995]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.85s/epoch, Train Loss=0.736, Test Loss A=1.113]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.87s/epoch, Train Loss=0.727, Test Loss A=0.966]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.94s/epoch, Train Loss=0.732, Test Loss A=0.989]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.93s/epoch, Train Loss=0.734, Test Loss A=1.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.964), (0.3, 0.926), (0.5, 0.884), (0.7, 0.927), (0.9, 0.904)]\n",
      "(552, 500, 795) (552,) (552,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.27s/epoch, Train Loss=0.752, Test Loss A=0.672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.672)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2015_3\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.672\n",
      "----------------------------------------\n",
      "(900, 500, 795) (900,) (900,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.59s/epoch, Train Loss=0.820, Test Loss A=1.197]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.57s/epoch, Train Loss=0.800, Test Loss A=1.123]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.57s/epoch, Train Loss=0.802, Test Loss A=1.139]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.58s/epoch, Train Loss=0.796, Test Loss A=1.010]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.57s/epoch, Train Loss=0.795, Test Loss A=1.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 1.125), (0.3, 1.024), (0.5, 1.053), (0.7, 1.01), (0.9, 1.047)]\n",
      "(1042, 500, 795) (1042,) (1042,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  4.28s/epoch, Train Loss=0.826, Test Loss A=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.69)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2016_3\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.69\n",
      "----------------------------------------\n",
      "(827, 500, 795) (827,) (827,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.758, Test Loss A=0.886]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.25s/epoch, Train Loss=0.746, Test Loss A=0.819]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.735, Test Loss A=0.808]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.726, Test Loss A=0.823]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.732, Test Loss A=0.836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.861), (0.3, 0.819), (0.5, 0.808), (0.7, 0.803), (0.9, 0.836)]\n",
      "(955, 500, 795) (955,) (955,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.91s/epoch, Train Loss=0.738, Test Loss A=0.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.738)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2017_3\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.738\n",
      "----------------------------------------\n",
      "(485, 500, 795) (485,) (485,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.92s/epoch, Train Loss=0.370, Test Loss A=0.446]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.90s/epoch, Train Loss=0.366, Test Loss A=0.429]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.366, Test Loss A=0.423]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.90s/epoch, Train Loss=0.363, Test Loss A=0.623]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.364, Test Loss A=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.408), (0.3, 0.412), (0.5, 0.423), (0.7, 0.397), (0.9, 0.396)]\n",
      "(552, 500, 795) (552,) (552,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.26s/epoch, Train Loss=0.371, Test Loss A=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.356)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2015_7\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.356\n",
      "----------------------------------------\n",
      "(900, 500, 795) (900,) (900,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.57s/epoch, Train Loss=0.435, Test Loss A=0.537]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.422, Test Loss A=0.523]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.426, Test Loss A=0.463]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.56s/epoch, Train Loss=0.419, Test Loss A=0.432]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.423, Test Loss A=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.47), (0.3, 0.481), (0.5, 0.462), (0.7, 0.432), (0.9, 0.441)]\n",
      "(1042, 500, 795) (1042,) (1042,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  4.28s/epoch, Train Loss=0.426, Test Loss A=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.312)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2016_7\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.312\n",
      "----------------------------------------\n",
      "(827, 500, 795) (827,) (827,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.407, Test Loss A=0.647]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.394, Test Loss A=0.432]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.384, Test Loss A=0.399]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.383, Test Loss A=0.423]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.381, Test Loss A=0.557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.591), (0.3, 0.432), (0.5, 0.399), (0.7, 0.423), (0.9, 0.442)]\n",
      "(955, 500, 795) (955,) (955,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.92s/epoch, Train Loss=0.396, Test Loss A=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.4)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2017_7\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.4\n",
      "----------------------------------------\n",
      "(485, 500, 795) (485,) (485,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.94s/epoch, Train Loss=0.278, Test Loss A=0.285]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.93s/epoch, Train Loss=0.274, Test Loss A=0.335]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.269, Test Loss A=0.295]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.92s/epoch, Train Loss=0.265, Test Loss A=0.348]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.92s/epoch, Train Loss=0.269, Test Loss A=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.262), (0.3, 0.283), (0.5, 0.257), (0.7, 0.289), (0.9, 0.259)]\n",
      "(552, 500, 795) (552,) (552,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.28s/epoch, Train Loss=0.270, Test Loss A=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.237)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2015_15\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.237\n",
      "----------------------------------------\n",
      "(900, 500, 795) (900,) (900,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.60s/epoch, Train Loss=0.308, Test Loss A=0.392]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.57s/epoch, Train Loss=0.302, Test Loss A=0.348]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.54s/epoch, Train Loss=0.298, Test Loss A=0.321]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.298, Test Loss A=0.357]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.297, Test Loss A=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.367), (0.3, 0.339), (0.5, 0.321), (0.7, 0.327), (0.9, 0.342)]\n",
      "(1042, 500, 795) (1042,) (1042,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  4.28s/epoch, Train Loss=0.309, Test Loss A=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.185)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2016_15\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.185\n",
      "----------------------------------------\n",
      "(827, 500, 795) (827,) (827,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.286, Test Loss A=0.375]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/epoch, Train Loss=0.275, Test Loss A=0.276]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/epoch, Train Loss=0.273, Test Loss A=0.340]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.268, Test Loss A=0.408]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.264, Test Loss A=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.348), (0.3, 0.276), (0.5, 0.309), (0.7, 0.325), (0.9, 0.28)]\n",
      "(955, 500, 795) (955,) (955,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  3.89s/epoch, Train Loss=0.276, Test Loss A=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.29)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2017_15\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.29\n",
      "----------------------------------------\n",
      "(485, 500, 795) (485,) (485,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.93s/epoch, Train Loss=0.193, Test Loss A=0.188]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.190, Test Loss A=0.199]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.189, Test Loss A=0.188]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.193, Test Loss A=0.201]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.187, Test Loss A=0.228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.188), (0.3, 0.184), (0.5, 0.174), (0.7, 0.18), (0.9, 0.178)]\n",
      "(552, 500, 795) (552,) (552,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.26s/epoch, Train Loss=0.189, Test Loss A=0.190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.189)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2015_30\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.189\n",
      "----------------------------------------\n",
      "(900, 500, 795) (900,) (900,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.57s/epoch, Train Loss=0.235, Test Loss A=0.192]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.233, Test Loss A=0.189]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.54s/epoch, Train Loss=0.230, Test Loss A=0.188]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.231, Test Loss A=0.186]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.228, Test Loss A=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.192), (0.3, 0.189), (0.5, 0.183), (0.7, 0.181), (0.9, 0.181)]\n",
      "(1042, 500, 795) (1042,) (1042,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  4.27s/epoch, Train Loss=0.221, Test Loss A=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.13)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2016_30\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.13\n",
      "----------------------------------------\n",
      "(827, 500, 795) (827,) (827,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.226, Test Loss A=0.258]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/epoch, Train Loss=0.215, Test Loss A=0.263]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.212, Test Loss A=0.193]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.208, Test Loss A=0.237]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.214, Test Loss A=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.223), (0.3, 0.212), (0.5, 0.193), (0.7, 0.195), (0.9, 0.181)]\n",
      "(955, 500, 795) (955,) (955,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.91s/epoch, Train Loss=0.206, Test Loss A=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.233)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_investopedia_2017_30\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.233\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.89s/epoch, Train Loss=0.754, Test Loss A=0.940]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.86s/epoch, Train Loss=0.735, Test Loss A=1.094]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.86s/epoch, Train Loss=0.724, Test Loss A=0.994]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.84s/epoch, Train Loss=0.721, Test Loss A=1.065]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.85s/epoch, Train Loss=0.724, Test Loss A=0.992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.92), (0.3, 0.941), (0.5, 0.898), (0.7, 0.887), (0.9, 0.932)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.36s/epoch, Train Loss=0.756, Test Loss A=0.670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.67)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2015_3\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.67\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.31s/epoch, Train Loss=0.813, Test Loss A=1.078]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.29s/epoch, Train Loss=0.805, Test Loss A=1.217]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.29s/epoch, Train Loss=0.799, Test Loss A=1.113]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.30s/epoch, Train Loss=0.779, Test Loss A=1.053]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.29s/epoch, Train Loss=0.787, Test Loss A=1.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 1.044), (0.3, 0.987), (0.5, 0.969), (0.7, 0.973), (0.9, 0.979)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:04<00:00,  6.41s/epoch, Train Loss=0.816, Test Loss A=0.872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.671)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2016_3\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.671\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.88s/epoch, Train Loss=0.756, Test Loss A=0.961]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.87s/epoch, Train Loss=0.748, Test Loss A=0.856]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.737, Test Loss A=0.861]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.724, Test Loss A=0.861]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.85s/epoch, Train Loss=0.720, Test Loss A=0.966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.848), (0.3, 0.856), (0.5, 0.843), (0.7, 0.861), (0.9, 0.873)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:58<00:00,  5.81s/epoch, Train Loss=0.743, Test Loss A=0.807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.739)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2017_3\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.739\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.86s/epoch, Train Loss=0.372, Test Loss A=0.456]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.84s/epoch, Train Loss=0.362, Test Loss A=0.508]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.86s/epoch, Train Loss=0.367, Test Loss A=0.450]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.87s/epoch, Train Loss=0.364, Test Loss A=0.492]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.86s/epoch, Train Loss=0.366, Test Loss A=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.416), (0.3, 0.407), (0.5, 0.423), (0.7, 0.417), (0.9, 0.38)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.38s/epoch, Train Loss=0.368, Test Loss A=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.364)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2015_7\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.364\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.35s/epoch, Train Loss=0.438, Test Loss A=0.470]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.30s/epoch, Train Loss=0.425, Test Loss A=0.457]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.29s/epoch, Train Loss=0.419, Test Loss A=0.539]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.29s/epoch, Train Loss=0.409, Test Loss A=0.622]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.29s/epoch, Train Loss=0.411, Test Loss A=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.459), (0.3, 0.45), (0.5, 0.459), (0.7, 0.428), (0.9, 0.459)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:03<00:00,  6.40s/epoch, Train Loss=0.419, Test Loss A=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.32)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2016_7\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.32\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.88s/epoch, Train Loss=0.403, Test Loss A=0.592]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.87s/epoch, Train Loss=0.398, Test Loss A=0.511]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.387, Test Loss A=0.612]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.385, Test Loss A=0.529]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.85s/epoch, Train Loss=0.380, Test Loss A=0.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.543), (0.3, 0.458), (0.5, 0.562), (0.7, 0.467), (0.9, 0.502)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:58<00:00,  5.80s/epoch, Train Loss=0.402, Test Loss A=0.432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.413)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2017_7\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.413\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.85s/epoch, Train Loss=0.277, Test Loss A=0.270]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.84s/epoch, Train Loss=0.270, Test Loss A=0.304]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.84s/epoch, Train Loss=0.267, Test Loss A=0.356]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.84s/epoch, Train Loss=0.261, Test Loss A=0.376]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.84s/epoch, Train Loss=0.259, Test Loss A=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.267), (0.3, 0.262), (0.5, 0.258), (0.7, 0.271), (0.9, 0.311)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.36s/epoch, Train Loss=0.265, Test Loss A=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.24)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2015_15\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.24\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.31s/epoch, Train Loss=0.304, Test Loss A=0.357]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.32s/epoch, Train Loss=0.299, Test Loss A=0.362]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.32s/epoch, Train Loss=0.296, Test Loss A=0.333]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.31s/epoch, Train Loss=0.291, Test Loss A=0.327]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.31s/epoch, Train Loss=0.295, Test Loss A=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.351), (0.3, 0.335), (0.5, 0.298), (0.7, 0.327), (0.9, 0.329)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:04<00:00,  6.41s/epoch, Train Loss=0.303, Test Loss A=0.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.188)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2016_15\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.188\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.89s/epoch, Train Loss=0.287, Test Loss A=0.445]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.87s/epoch, Train Loss=0.278, Test Loss A=0.444]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.274, Test Loss A=0.403]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.87s/epoch, Train Loss=0.265, Test Loss A=0.345]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:48<00:00,  4.86s/epoch, Train Loss=0.263, Test Loss A=0.359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.348), (0.3, 0.339), (0.5, 0.331), (0.7, 0.265), (0.9, 0.28)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:58<00:00,  5.80s/epoch, Train Loss=0.263, Test Loss A=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.307)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2017_15\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.307\n",
      "----------------------------------------\n",
      "(485, 500, 1051) (485,) (485,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.86s/epoch, Train Loss=0.190, Test Loss A=0.203]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.191, Test Loss A=0.191]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.188, Test Loss A=0.223]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.89s/epoch, Train Loss=0.190, Test Loss A=0.172]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.88s/epoch, Train Loss=0.189, Test Loss A=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.178), (0.3, 0.173), (0.5, 0.172), (0.7, 0.172), (0.9, 0.177)]\n",
      "(552, 500, 1051) (552,) (552,)\n",
      "(67, 500, 1051) (67,) (67,)\n",
      "(146, 500, 1051) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  3.45s/epoch, Train Loss=0.190, Test Loss A=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.178)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2015_30\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.178\n",
      "----------------------------------------\n",
      "(900, 500, 1051) (900,) (900,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.39s/epoch, Train Loss=0.233, Test Loss A=0.187]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.38s/epoch, Train Loss=0.230, Test Loss A=0.182]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.37s/epoch, Train Loss=0.227, Test Loss A=0.193]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.37s/epoch, Train Loss=0.228, Test Loss A=0.172]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:53<00:00,  5.37s/epoch, Train Loss=0.226, Test Loss A=0.176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.186), (0.3, 0.182), (0.5, 0.181), (0.7, 0.172), (0.9, 0.173)]\n",
      "(1042, 500, 1051) (1042,) (1042,)\n",
      "(142, 500, 1051) (142,) (142,)\n",
      "(266, 500, 1051) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:05<00:00,  6.52s/epoch, Train Loss=0.213, Test Loss A=0.148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.129)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2016_30\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.129\n",
      "----------------------------------------\n",
      "(827, 500, 1051) (827,) (827,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.99s/epoch, Train Loss=0.225, Test Loss A=0.248]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.97s/epoch, Train Loss=0.217, Test Loss A=0.209]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.96s/epoch, Train Loss=0.207, Test Loss A=0.244]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.94s/epoch, Train Loss=0.208, Test Loss A=0.280]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:49<00:00,  4.94s/epoch, Train Loss=0.215, Test Loss A=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.239), (0.3, 0.209), (0.5, 0.189), (0.7, 0.208), (0.9, 0.181)]\n",
      "(955, 500, 1051) (955,) (955,)\n",
      "(128, 500, 1051) (128,) (128,)\n",
      "(238, 500, 1051) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:59<00:00,  5.93s/epoch, Train Loss=0.196, Test Loss A=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.231)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_2017_30\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.231\n",
      "----------------------------------------\n",
      "(485, 500, 795) (485,) (485,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.92s/epoch, Train Loss=0.746, Test Loss A=0.948]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.734, Test Loss A=0.920]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.728, Test Loss A=0.997]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.90s/epoch, Train Loss=0.733, Test Loss A=0.979]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.733, Test Loss A=0.949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.928), (0.3, 0.92), (0.5, 0.919), (0.7, 0.911), (0.9, 0.912)]\n",
      "(552, 500, 795) (552,) (552,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.27s/epoch, Train Loss=0.752, Test Loss A=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.697)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2015_3\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.697\n",
      "----------------------------------------\n",
      "(900, 500, 795) (900,) (900,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.56s/epoch, Train Loss=0.817, Test Loss A=1.110]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.802, Test Loss A=1.074]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.54s/epoch, Train Loss=0.794, Test Loss A=1.015]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.799, Test Loss A=0.999]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.793, Test Loss A=1.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 1.11), (0.3, 1.074), (0.5, 1.015), (0.7, 0.999), (0.9, 0.998)]\n",
      "(1042, 500, 795) (1042,) (1042,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  4.28s/epoch, Train Loss=0.824, Test Loss A=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.677)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2016_3\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.677\n",
      "----------------------------------------\n",
      "(827, 500, 795) (827,) (827,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.752, Test Loss A=1.004]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.739, Test Loss A=0.877]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.25s/epoch, Train Loss=0.739, Test Loss A=0.834]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.730, Test Loss A=0.895]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.25s/epoch, Train Loss=0.734, Test Loss A=0.883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.907), (0.3, 0.83), (0.5, 0.834), (0.7, 0.882), (0.9, 0.858)]\n",
      "(955, 500, 795) (955,) (955,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.90s/epoch, Train Loss=0.760, Test Loss A=0.779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.737)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2017_3\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.737\n",
      "----------------------------------------\n",
      "(485, 500, 795) (485,) (485,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.92s/epoch, Train Loss=0.375, Test Loss A=0.474]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.368, Test Loss A=0.388]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.92s/epoch, Train Loss=0.363, Test Loss A=0.458]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.93s/epoch, Train Loss=0.360, Test Loss A=0.406]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.93s/epoch, Train Loss=0.361, Test Loss A=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.424), (0.3, 0.388), (0.5, 0.433), (0.7, 0.406), (0.9, 0.414)]\n",
      "(552, 500, 795) (552,) (552,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.28s/epoch, Train Loss=0.373, Test Loss A=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.364)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2015_7\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.364\n",
      "----------------------------------------\n",
      "(900, 500, 795) (900,) (900,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.59s/epoch, Train Loss=0.434, Test Loss A=0.500]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.57s/epoch, Train Loss=0.427, Test Loss A=0.463]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.58s/epoch, Train Loss=0.427, Test Loss A=0.443]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.59s/epoch, Train Loss=0.422, Test Loss A=0.446]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:36<00:00,  3.63s/epoch, Train Loss=0.424, Test Loss A=0.457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.5), (0.3, 0.463), (0.5, 0.443), (0.7, 0.446), (0.9, 0.457)]\n",
      "(1042, 500, 795) (1042,) (1042,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  4.32s/epoch, Train Loss=0.434, Test Loss A=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.305)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2016_7\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.305\n",
      "----------------------------------------\n",
      "(827, 500, 795) (827,) (827,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.406, Test Loss A=0.620]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.25s/epoch, Train Loss=0.396, Test Loss A=0.473]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/epoch, Train Loss=0.392, Test Loss A=0.419]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.387, Test Loss A=0.436]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.382, Test Loss A=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.591), (0.3, 0.473), (0.5, 0.419), (0.7, 0.436), (0.9, 0.476)]\n",
      "(955, 500, 795) (955,) (955,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.90s/epoch, Train Loss=0.395, Test Loss A=0.430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.382)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2017_7\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.382\n",
      "----------------------------------------\n",
      "(485, 500, 795) (485,) (485,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.93s/epoch, Train Loss=0.278, Test Loss A=0.262]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.90s/epoch, Train Loss=0.275, Test Loss A=0.283]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.268, Test Loss A=0.283]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.90s/epoch, Train Loss=0.265, Test Loss A=0.309]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.265, Test Loss A=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.262), (0.3, 0.247), (0.5, 0.254), (0.7, 0.272), (0.9, 0.267)]\n",
      "(552, 500, 795) (552,) (552,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.26s/epoch, Train Loss=0.268, Test Loss A=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.235)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2015_15\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.235\n",
      "----------------------------------------\n",
      "(900, 500, 795) (900,) (900,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.56s/epoch, Train Loss=0.305, Test Loss A=0.355]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.301, Test Loss A=0.379]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.54s/epoch, Train Loss=0.298, Test Loss A=0.335]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.299, Test Loss A=0.389]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.300, Test Loss A=0.360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.355), (0.3, 0.346), (0.5, 0.326), (0.7, 0.348), (0.9, 0.352)]\n",
      "(1042, 500, 795) (1042,) (1042,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  4.26s/epoch, Train Loss=0.309, Test Loss A=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.193)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2016_15\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.193\n",
      "----------------------------------------\n",
      "(827, 500, 795) (827,) (827,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/epoch, Train Loss=0.288, Test Loss A=0.402]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.282, Test Loss A=0.357]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.25s/epoch, Train Loss=0.272, Test Loss A=0.524]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.275, Test Loss A=0.346]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/epoch, Train Loss=0.269, Test Loss A=0.400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.372), (0.3, 0.357), (0.5, 0.358), (0.7, 0.293), (0.9, 0.3)]\n",
      "(955, 500, 795) (955,) (955,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  3.89s/epoch, Train Loss=0.273, Test Loss A=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.295)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2017_15\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.295\n",
      "----------------------------------------\n",
      "(485, 500, 795) (485,) (485,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.93s/epoch, Train Loss=0.193, Test Loss A=0.188]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.93s/epoch, Train Loss=0.192, Test Loss A=0.181]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/epoch, Train Loss=0.191, Test Loss A=0.198]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.92s/epoch, Train Loss=0.187, Test Loss A=0.187]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.92s/epoch, Train Loss=0.189, Test Loss A=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.18), (0.3, 0.171), (0.5, 0.195), (0.7, 0.187), (0.9, 0.18)]\n",
      "(552, 500, 795) (552,) (552,)\n",
      "(67, 500, 795) (67,) (67,)\n",
      "(146, 500, 795) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.27s/epoch, Train Loss=0.188, Test Loss A=0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.186)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2015_30\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.186\n",
      "----------------------------------------\n",
      "(900, 500, 795) (900,) (900,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.59s/epoch, Train Loss=0.235, Test Loss A=0.194]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.58s/epoch, Train Loss=0.233, Test Loss A=0.186]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.232, Test Loss A=0.175]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.55s/epoch, Train Loss=0.230, Test Loss A=0.180]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.54s/epoch, Train Loss=0.226, Test Loss A=0.180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.188), (0.3, 0.185), (0.5, 0.175), (0.7, 0.18), (0.9, 0.179)]\n",
      "(1042, 500, 795) (1042,) (1042,)\n",
      "(142, 500, 795) (142,) (142,)\n",
      "(266, 500, 795) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  4.28s/epoch, Train Loss=0.223, Test Loss A=0.137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.13)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2016_30\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.13\n",
      "----------------------------------------\n",
      "(827, 500, 795) (827,) (827,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/epoch, Train Loss=0.230, Test Loss A=0.282]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/epoch, Train Loss=0.223, Test Loss A=0.208]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.216, Test Loss A=0.291]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.216, Test Loss A=0.256]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/epoch, Train Loss=0.214, Test Loss A=0.239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.219), (0.3, 0.208), (0.5, 0.217), (0.7, 0.219), (0.9, 0.232)]\n",
      "(955, 500, 795) (955,) (955,)\n",
      "(128, 500, 795) (128,) (128,)\n",
      "(238, 500, 795) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  3.90s/epoch, Train Loss=0.221, Test Loss A=0.223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.217)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_bge_base_2017_30\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.217\n",
      "----------------------------------------\n",
      "(485, 500, 327) (485,) (485,)\n",
      "(67, 500, 327) (67,) (67,)\n",
      "(146, 500, 327) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.37epoch/s, Train Loss=0.754, Test Loss A=0.991]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.33epoch/s, Train Loss=0.752, Test Loss A=0.936]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.39epoch/s, Train Loss=0.749, Test Loss A=0.966]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.40epoch/s, Train Loss=0.744, Test Loss A=0.946]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.36epoch/s, Train Loss=0.747, Test Loss A=0.937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.958), (0.3, 0.928), (0.5, 0.943), (0.7, 0.927), (0.9, 0.935)]\n",
      "(552, 500, 327) (552,) (552,)\n",
      "(67, 500, 327) (67,) (67,)\n",
      "(146, 500, 327) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.11epoch/s, Train Loss=0.774, Test Loss A=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.721)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2015_3\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.721\n",
      "----------------------------------------\n",
      "(900, 500, 327) (900,) (900,)\n",
      "(142, 500, 327) (142,) (142,)\n",
      "(266, 500, 327) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.41s/epoch, Train Loss=0.825, Test Loss A=1.034]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.42s/epoch, Train Loss=0.821, Test Loss A=1.094]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.42s/epoch, Train Loss=0.823, Test Loss A=1.076]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.45s/epoch, Train Loss=0.821, Test Loss A=1.199]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.35s/epoch, Train Loss=0.814, Test Loss A=1.110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 1.012), (0.3, 1.089), (0.5, 1.043), (0.7, 1.122), (0.9, 1.035)]\n",
      "(1042, 500, 327) (1042,) (1042,)\n",
      "(142, 500, 327) (142,) (142,)\n",
      "(266, 500, 327) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.77s/epoch, Train Loss=0.861, Test Loss A=0.721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.7)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2016_3\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.7\n",
      "----------------------------------------\n",
      "(827, 500, 327) (827,) (827,)\n",
      "(128, 500, 327) (128,) (128,)\n",
      "(238, 500, 327) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.37s/epoch, Train Loss=0.767, Test Loss A=1.067]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.27s/epoch, Train Loss=0.757, Test Loss A=1.001]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.27s/epoch, Train Loss=0.764, Test Loss A=0.933]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.30s/epoch, Train Loss=0.761, Test Loss A=0.972]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.30s/epoch, Train Loss=0.761, Test Loss A=1.090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 1.037), (0.3, 0.953), (0.5, 0.927), (0.7, 0.919), (0.9, 1.019)]\n",
      "(955, 500, 327) (955,) (955,)\n",
      "(128, 500, 327) (128,) (128,)\n",
      "(238, 500, 327) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.64s/epoch, Train Loss=0.784, Test Loss A=0.850]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.781)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2017_3\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.781\n",
      "----------------------------------------\n",
      "(485, 500, 327) (485,) (485,)\n",
      "(67, 500, 327) (67,) (67,)\n",
      "(146, 500, 327) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.32epoch/s, Train Loss=0.375, Test Loss A=0.443]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.34epoch/s, Train Loss=0.375, Test Loss A=0.436]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.31epoch/s, Train Loss=0.373, Test Loss A=0.466]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.41epoch/s, Train Loss=0.376, Test Loss A=0.473]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.40epoch/s, Train Loss=0.376, Test Loss A=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.443), (0.3, 0.429), (0.5, 0.425), (0.7, 0.433), (0.9, 0.442)]\n",
      "(552, 500, 327) (552,) (552,)\n",
      "(67, 500, 327) (67,) (67,)\n",
      "(146, 500, 327) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:08<00:00,  1.12epoch/s, Train Loss=0.380, Test Loss A=0.409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.398)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2015_7\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.398\n",
      "----------------------------------------\n",
      "(900, 500, 327) (900,) (900,)\n",
      "(142, 500, 327) (142,) (142,)\n",
      "(266, 500, 327) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.41s/epoch, Train Loss=0.439, Test Loss A=0.513]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.44s/epoch, Train Loss=0.437, Test Loss A=0.499]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.46s/epoch, Train Loss=0.436, Test Loss A=0.528]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.43s/epoch, Train Loss=0.437, Test Loss A=0.520]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.50s/epoch, Train Loss=0.438, Test Loss A=0.545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.501), (0.3, 0.491), (0.5, 0.528), (0.7, 0.513), (0.9, 0.501)]\n",
      "(1042, 500, 327) (1042,) (1042,)\n",
      "(142, 500, 327) (142,) (142,)\n",
      "(266, 500, 327) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.69s/epoch, Train Loss=0.449, Test Loss A=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3, 0.345)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2016_7\n",
      "best_alpha---------0.3\n",
      "MSE_testset--------0.345\n",
      "----------------------------------------\n",
      "(827, 500, 327) (827,) (827,)\n",
      "(128, 500, 327) (128,) (128,)\n",
      "(238, 500, 327) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.28s/epoch, Train Loss=0.418, Test Loss A=0.654]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.26s/epoch, Train Loss=0.414, Test Loss A=0.658]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.32s/epoch, Train Loss=0.414, Test Loss A=0.598]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.32s/epoch, Train Loss=0.413, Test Loss A=0.632]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.26s/epoch, Train Loss=0.416, Test Loss A=0.631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.616), (0.3, 0.631), (0.5, 0.563), (0.7, 0.593), (0.9, 0.565)]\n",
      "(955, 500, 327) (955,) (955,)\n",
      "(128, 500, 327) (128,) (128,)\n",
      "(238, 500, 327) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.59s/epoch, Train Loss=0.435, Test Loss A=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.428)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2017_7\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.428\n",
      "----------------------------------------\n",
      "(485, 500, 327) (485,) (485,)\n",
      "(67, 500, 327) (67,) (67,)\n",
      "(146, 500, 327) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.27epoch/s, Train Loss=0.278, Test Loss A=0.287]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.29epoch/s, Train Loss=0.275, Test Loss A=0.301]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.34epoch/s, Train Loss=0.278, Test Loss A=0.299]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.32epoch/s, Train Loss=0.276, Test Loss A=0.301]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.31epoch/s, Train Loss=0.276, Test Loss A=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.277), (0.3, 0.272), (0.5, 0.284), (0.7, 0.283), (0.9, 0.271)]\n",
      "(552, 500, 327) (552,) (552,)\n",
      "(67, 500, 327) (67,) (67,)\n",
      "(146, 500, 327) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:08<00:00,  1.13epoch/s, Train Loss=0.278, Test Loss A=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.263)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2015_15\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.263\n",
      "----------------------------------------\n",
      "(900, 500, 327) (900,) (900,)\n",
      "(142, 500, 327) (142,) (142,)\n",
      "(266, 500, 327) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.40s/epoch, Train Loss=0.307, Test Loss A=0.391]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.45s/epoch, Train Loss=0.309, Test Loss A=0.392]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.40s/epoch, Train Loss=0.306, Test Loss A=0.408]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.40s/epoch, Train Loss=0.311, Test Loss A=0.384]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.39s/epoch, Train Loss=0.308, Test Loss A=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.384), (0.3, 0.385), (0.5, 0.392), (0.7, 0.384), (0.9, 0.405)]\n",
      "(1042, 500, 327) (1042,) (1042,)\n",
      "(142, 500, 327) (142,) (142,)\n",
      "(266, 500, 327) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.75s/epoch, Train Loss=0.321, Test Loss A=0.200]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.193)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2016_15\n",
      "best_alpha---------0.1\n",
      "MSE_testset--------0.193\n",
      "----------------------------------------\n",
      "(827, 500, 327) (827,) (827,)\n",
      "(128, 500, 327) (128,) (128,)\n",
      "(238, 500, 327) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.37s/epoch, Train Loss=0.296, Test Loss A=0.370]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.32s/epoch, Train Loss=0.294, Test Loss A=0.406]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.25s/epoch, Train Loss=0.296, Test Loss A=0.388]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.29s/epoch, Train Loss=0.292, Test Loss A=0.375]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.36s/epoch, Train Loss=0.294, Test Loss A=0.421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.358), (0.3, 0.379), (0.5, 0.365), (0.7, 0.332), (0.9, 0.385)]\n",
      "(955, 500, 327) (955,) (955,)\n",
      "(128, 500, 327) (128,) (128,)\n",
      "(238, 500, 327) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.53s/epoch, Train Loss=0.302, Test Loss A=0.360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.333)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2017_15\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.333\n",
      "----------------------------------------\n",
      "(485, 500, 327) (485,) (485,)\n",
      "(67, 500, 327) (67,) (67,)\n",
      "(146, 500, 327) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.35epoch/s, Train Loss=0.195, Test Loss A=0.191]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.35epoch/s, Train Loss=0.195, Test Loss A=0.185]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.29epoch/s, Train Loss=0.193, Test Loss A=0.188]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.34epoch/s, Train Loss=0.194, Test Loss A=0.199]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.36epoch/s, Train Loss=0.194, Test Loss A=0.190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.189), (0.3, 0.179), (0.5, 0.177), (0.7, 0.194), (0.9, 0.181)]\n",
      "(552, 500, 327) (552,) (552,)\n",
      "(67, 500, 327) (67,) (67,)\n",
      "(146, 500, 327) (146,) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.11epoch/s, Train Loss=0.194, Test Loss A=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5, 0.204)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2015_30\n",
      "best_alpha---------0.5\n",
      "MSE_testset--------0.204\n",
      "----------------------------------------\n",
      "(900, 500, 327) (900,) (900,)\n",
      "(142, 500, 327) (142,) (142,)\n",
      "(266, 500, 327) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.48s/epoch, Train Loss=0.237, Test Loss A=0.201]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.46s/epoch, Train Loss=0.238, Test Loss A=0.199]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.40s/epoch, Train Loss=0.236, Test Loss A=0.196]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.37s/epoch, Train Loss=0.237, Test Loss A=0.203]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.39s/epoch, Train Loss=0.235, Test Loss A=0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.198), (0.3, 0.196), (0.5, 0.192), (0.7, 0.198), (0.9, 0.19)]\n",
      "(1042, 500, 327) (1042,) (1042,)\n",
      "(142, 500, 327) (142,) (142,)\n",
      "(266, 500, 327) (266,) (266,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.71s/epoch, Train Loss=0.231, Test Loss A=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9, 0.128)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2016_30\n",
      "best_alpha---------0.9\n",
      "MSE_testset--------0.128\n",
      "----------------------------------------\n",
      "(827, 500, 327) (827,) (827,)\n",
      "(128, 500, 327) (128,) (128,)\n",
      "(238, 500, 327) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.31s/epoch, Train Loss=0.231, Test Loss A=0.284]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.37s/epoch, Train Loss=0.233, Test Loss A=0.252]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00,  1.37s/epoch, Train Loss=0.232, Test Loss A=0.290]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.29s/epoch, Train Loss=0.232, Test Loss A=0.287]\n",
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:12<00:00,  1.28s/epoch, Train Loss=0.230, Test Loss A=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.284), (0.3, 0.24), (0.5, 0.266), (0.7, 0.238), (0.9, 0.257)]\n",
      "(955, 500, 327) (955,) (955,)\n",
      "(128, 500, 327) (128,) (128,)\n",
      "(238, 500, 327) (238,) (238,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.57s/epoch, Train Loss=0.231, Test Loss A=0.293]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7, 0.276)]\n",
      "----------------------------------------\n",
      "run_name-----------MAEC_glove_2017_30\n",
      "best_alpha---------0.7\n",
      "MSE_testset--------0.276\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directories = ['MAEC_Roberta', 'MAEC_Roberta2', 'MAEC_investopedia', 'MAEC_bge', 'MAEC_bge_base', 'MAEC_glove']#\n",
    "years = ['_2015', '_2016', '_2017']\n",
    "\n",
    "MAEC_final_results = []\n",
    "for data_directory in directories: # different feature engineering features\n",
    "    for n_days in all_n_days: \n",
    "        for year in years:\n",
    "            best_alpha, MSE_testset = train_val_test(data_directory, n_days, year)\n",
    "            MAEC_final_results.append([data_directory, n_days, year, best_alpha, MSE_testset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding Variant</th>\n",
       "      <th>n_days</th>\n",
       "      <th>year</th>\n",
       "      <th>best_alpha</th>\n",
       "      <th>MSE_testset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAEC_Roberta</td>\n",
       "      <td>3</td>\n",
       "      <td>_2015</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAEC_Roberta</td>\n",
       "      <td>3</td>\n",
       "      <td>_2016</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAEC_Roberta</td>\n",
       "      <td>3</td>\n",
       "      <td>_2017</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAEC_Roberta</td>\n",
       "      <td>7</td>\n",
       "      <td>_2015</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAEC_Roberta</td>\n",
       "      <td>7</td>\n",
       "      <td>_2016</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>MAEC_glove</td>\n",
       "      <td>15</td>\n",
       "      <td>_2016</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>MAEC_glove</td>\n",
       "      <td>15</td>\n",
       "      <td>_2017</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>MAEC_glove</td>\n",
       "      <td>30</td>\n",
       "      <td>_2015</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>MAEC_glove</td>\n",
       "      <td>30</td>\n",
       "      <td>_2016</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>MAEC_glove</td>\n",
       "      <td>30</td>\n",
       "      <td>_2017</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Embedding Variant n_days   year  best_alpha  MSE_testset\n",
       "0       MAEC_Roberta      3  _2015         0.1        0.718\n",
       "1       MAEC_Roberta      3  _2016         0.7        0.686\n",
       "2       MAEC_Roberta      3  _2017         0.5        0.868\n",
       "3       MAEC_Roberta      7  _2015         0.3        0.377\n",
       "4       MAEC_Roberta      7  _2016         0.9        0.338\n",
       "..               ...    ...    ...         ...          ...\n",
       "67        MAEC_glove     15  _2016         0.1        0.193\n",
       "68        MAEC_glove     15  _2017         0.7        0.333\n",
       "69        MAEC_glove     30  _2015         0.5        0.204\n",
       "70        MAEC_glove     30  _2016         0.9        0.128\n",
       "71        MAEC_glove     30  _2017         0.7        0.276\n",
       "\n",
       "[72 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEC_final_results = pd.DataFrame(MAEC_final_results, columns=['Embedding Variant', 'n_days', 'year', 'best_alpha', 'MSE_testset'])\n",
    "MAEC_final_results.to_csv('data/MAEC_final_results.csv', index=False)\n",
    "MAEC_final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>n_days</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding Variant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAEC_Roberta</th>\n",
       "      <td>0.718</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_Roberta2</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_bge</th>\n",
       "      <td>0.670</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_bge_base</th>\n",
       "      <td>0.697</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_glove</th>\n",
       "      <td>0.721</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_investopedia</th>\n",
       "      <td>0.672</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "n_days                 3      7     15     30\n",
       "Embedding Variant                            \n",
       "MAEC_Roberta       0.718  0.377  0.252  0.189\n",
       "MAEC_Roberta2      0.675  0.359  0.234  0.179\n",
       "MAEC_bge           0.670  0.364  0.240  0.178\n",
       "MAEC_bge_base      0.697  0.364  0.235  0.186\n",
       "MAEC_glove         0.721  0.398  0.263  0.204\n",
       "MAEC_investopedia  0.672  0.356  0.237  0.189"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEC_final_results_2015 = MAEC_final_results[MAEC_final_results['year']=='_2015']\n",
    "MAEC_final_results_2015 = MAEC_final_results_2015.drop(['year'], axis=1)\n",
    "MAEC_final_results_2015 = MAEC_final_results_2015.pivot(index='Embedding Variant', columns='n_days', values='MSE_testset')\n",
    "MAEC_final_results_2015 = MAEC_final_results_2015[['3','7','15','30']]\n",
    "MAEC_final_results_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>n_days</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding Variant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAEC_Roberta</th>\n",
       "      <td>0.686</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_Roberta2</th>\n",
       "      <td>0.678</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_bge</th>\n",
       "      <td>0.671</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_bge_base</th>\n",
       "      <td>0.677</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_glove</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_investopedia</th>\n",
       "      <td>0.690</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "n_days                3      7      15     30\n",
       "Embedding Variant                            \n",
       "MAEC_Roberta       0.686  0.338  0.185  0.131\n",
       "MAEC_Roberta2      0.678  0.318  0.197  0.131\n",
       "MAEC_bge           0.671  0.320  0.188  0.129\n",
       "MAEC_bge_base      0.677  0.305  0.193  0.130\n",
       "MAEC_glove         0.700  0.345  0.193  0.128\n",
       "MAEC_investopedia  0.690  0.312  0.185  0.130"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEC_final_results_2016 = MAEC_final_results[MAEC_final_results['year']=='_2016']\n",
    "MAEC_final_results_2016 = MAEC_final_results_2016.drop(['year'], axis=1)\n",
    "MAEC_final_results_2016 = MAEC_final_results_2016.pivot(index='Embedding Variant', columns='n_days', values='MSE_testset')\n",
    "MAEC_final_results_2016 = MAEC_final_results_2016[['3','7','15','30']]\n",
    "MAEC_final_results_2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>n_days</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embedding Variant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAEC_Roberta</th>\n",
       "      <td>0.868</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_Roberta2</th>\n",
       "      <td>0.829</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_bge</th>\n",
       "      <td>0.739</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_bge_base</th>\n",
       "      <td>0.737</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_glove</th>\n",
       "      <td>0.781</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAEC_investopedia</th>\n",
       "      <td>0.738</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "n_days                3      7      15     30\n",
       "Embedding Variant                            \n",
       "MAEC_Roberta       0.868  0.399  0.326  0.267\n",
       "MAEC_Roberta2      0.829  0.399  0.263  0.207\n",
       "MAEC_bge           0.739  0.413  0.307  0.231\n",
       "MAEC_bge_base      0.737  0.382  0.295  0.217\n",
       "MAEC_glove         0.781  0.428  0.333  0.276\n",
       "MAEC_investopedia  0.738  0.400  0.290  0.233"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEC_final_results_2017 = MAEC_final_results[MAEC_final_results['year']=='_2017']\n",
    "MAEC_final_results_2017 = MAEC_final_results_2017.drop(['year'], axis=1)\n",
    "MAEC_final_results_2017 = MAEC_final_results_2017.pivot(index='Embedding Variant', columns='n_days', values='MSE_testset')\n",
    "MAEC_final_results_2017 = MAEC_final_results_2017[['3','7','15','30']]\n",
    "MAEC_final_results_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-17ac5b5f63da0e9c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-17ac5b5f63da0e9c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6013;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (1896645534.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[100], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    :)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/mpeg;base64,//PkZAAgGgkCAW8MLpyj4hVuGMsdCfywmqQRpgwYsLJItEkKqgEAjutRmDkMwCExjLtsTXemIXALiMkhiHMHQXQy934bZ2ztnbX3Ld9+IYfxyIcdhYRYjLHfcuH3zM/OyWrYJAkGEUpBAA4BMCYHwbiOZmZmJYNAaA0Eg8r5wYCWIYliWJauWzMSAaA0JhgJANCwsP7lcSzMSxLEs/iSCRF7BIJiz9O37r373cpRhZ0zh2SxLJ5LEMSzMzJjkzNJxYf/y+PV915MixYSCZXOYq/clk/215geLFixYsMFFNvdtfbrrFiyjCzfmpm/N/pVesWa+wsWVfbXmbxg5OUnzhRdgwPKLzt/J47P7rNfm8tvgZ4eHj8oJIBL6JYAIEEWmTxEABGHF9MIIQ0Jz0J+m7uHMkTd3/AxZolcDF/u7vBB+hOLOfehxcTeJE/rwAEU0UAI/m7k7wn6buBi4nwWehPq4QAinE3Ezr/9CEbIQhCaEnD5xM/6EY5Cf6nO/////qdyEDgABwAVBEdDIQpj4CPGISJ2YS4QZgHgIGBOAiDQUjAPAmKgKhkHgORglOTpAS9RVDK6i5d0EkFZjIkSsgVBwIIgaWQJElURWxFpUQQCLqpzrJjKXggDJjhgrZUI//PkZFcnsgz+YHtPLp4zGhikOk74IBy8aAcu/VVeA74AdBOwHR8BmejPIObB8AzgtR+BQzbNUb5OtQXpE4I0UhiFYRJUFfCERrWtRW8O/itDWFPfb4GciHxUVImoau1iNCEEQRojYakr9EeRnQq4NRBhmb0BjfV/4Gf7wIMBnFPtYo3z4MMiyNkSRNRDNEcR9XxFFHGHsWew1dckNKCKHcKwVnsKoVG+KwNbddQWFWxaxdZEFta1rZFXWtpVayxdCszWwoBErPnzELurnr0QUUyusCsgigtkDM9r+IfWCV498lfyrtWxGsWfFYPapVlXvj3I8iSoe2CgoKCoAG1EEOiHEGDYjCWJwfxNgpxV+ePnOdOCv87vBAPDn4f/sfiMB8XGFHOT73OT504KBVxUdP86dOHP0bnmTySJNASNkaSf8B00OChohh3iHDAcIMBmFIhiAN8QCA0KN/g0Fc0DDv//DEQea2cGAu+IP8QnfJEf//LFVQAxQNG5EWDBMg00wZcCtMBoAzzArQEYwEcCKMAEAJzAiwCYrAaSsAEMAFABSsAmMAmAaDAJgCcwCYAnMAaAUjAGgFIwFIAbMBHABSwAjGATgApgAgBMVgAhgAgCOYAIACmATAE5gAoAKBix//PkZGsrwgj6tK/QACVLRfwBTWgAcLhAMULC4cBQsBhAgRCAacKBpgoGECwiEhhgbB4YcMNC6wApYLrg2DINg4IlgwwYcGwaGGAy5cGwcGHDDg2DQwwXWDDBdYLrhhoXWC64YeES+DYNC60GwZwMIF+EQvAwgQGBQYECITAwoX4MCQiEAwoSDAmDAuDYMhdcMNDDg2D4YcLrBhoNg3C6+DYMwwwXXBsHBhwbB4YcGwdBsGwuuGGC68GwfC68LrhdeF1wusF1gbBoXWDDBhuEQkGBPBgX4MC/CIQGBYRC/hEIEQsGBPwiE//4RC+EQnDDQbB0GweF1guuF1gbBkLrYXWhh/hdf4XXBrBqBrwawa8NIaA0QJiGgCZhqDV+DWALgNIjhICREiJAFoDWBMQ1hrDQBMwJhw1YEzw1BpDSGqGoNQaQBc4NXg0w0+BMQJhDWGvDWJCC0CO4jxHCOBagWoR4ArRIcGn+DUDQALmDVwasGiDTg0/g0+DVDRDRDVw0hqDXhow04avwavBqBpwa8GsGj/Br8Gjg0/wa+DWqLFgyFoulqutrkckkaDM5gp5DCQaMWAkzuYDmVeMKg9fYsZzKYbOOHw/HKzC4XGAahMaYY/LhmANM6QXjQQWBUXCo//PkZEIn0hc3L85sASd7rkEVg0ABEcBAHAMAGWEknzZ3GiIrMOAisCTfMUOCwZmmkJzjg6b5xlfsbjMbdJ0HXAwY0xeCKDyM5dWMur9Ezt043GGcukYQBIKL8TjVsSQjcbjUZoGduq6FE+VHGqKjV2ztxy4CKDTFd08Quxe7SyW5FJPfp6alk96k/6N1lcUToM4dR86X7t+lk1PJJNdpae9ficnity5TX6aTX1bExAwDjcbfNnDqUf0TpxqMfQ0X0H0UaoKKi+jofoKH6Kg/6Ogo6D//6ONxiMUVB/xih9MRnDqPnGP////////v////////////////////uoztncbZxGIxRxiioIxGKOioo3QRgSFgwJDnihSRSvVILguqrNnEi5AiPE6JSUzDRQaDzu0MwlKLEcSsQzK3QhmwHgNxGEERjUFRiS7HZ5SB6WHYjmOZRxIhtFokYoPEA8Gx6ocY1jC6oRTZVFiJZ6HRA8se76ujwi/KzA5Fee9Et2kqtH5jlB7QtM+OlpuLi1qmW6074u4S4qar9OJHDLm75/RfyVMqRaPkK/VcsFCJ1/dVgFUJEUu+/21ut1t1p0OlQswxlkDAWAgysCMt6DASMaREMLSQBwOmJoaA4CTCkNjC//PkZDAmUhFQ387oASWj6hwLh1AAQZwIAwWEOBB4BzzrAIFFhgOBqQiSeV2mDbpsEoEC0ykYtTXlEHyoKIhExkLgAsPSRZ0zpNpJJ0I1GlbAQBfG4q2SqpPIoYraQAExqFW1fsao/+Ns5X+v1XUbQVV06VyKxaIxKS/J78Xu3rl58V4RJu6mkQcOIXKV/aS7evXqa7Jbt2lpYlTU1144lTRFulLfpYrEorFbty7SuLe/79Ldvxmg+jjdB9D9F/xr4zGvjMaoaKgjUZfCNPjRXaX/uf//9+7TXaa5ru9b1+vx3vmeWoNg9yoPcuDHKcuDvcuDoNcqDnK+D4P//pbl+7c+JUt7/vf9y/S3v////+5fu3P+///90gggFPBQFIMgrBng0P4eg3CMIweh74fcPg9gUoFKHvh5joF0ciiDaKRGEaH3jgvEQOx0PsPYegUA+h58cHR2DMUCiDYI4jRTwbcRsRgKIfB+HofB5D0PIfeHgeh4HmHgfQ/h78Ph0dHBFCIF4uEX/D7w9D0P8PcUwbuI8R8RxHEfD6H4eB6HgFCHvh/Dzh/w+//////gUyQCbvqcZ3MM7mSQNUY1aKxkKguGEmjyYuycRh0jnGFGEmduwJhntnTmeUC4YSQbJipg//PkZDImAgsQCe9QACDT2eQB05gA3GLIGyZyg7xhRiPmFEBSZWAjxhsjvGUeEAFgBjAXBvMEwBYwKAtTApBNMCkE0D37wGFoGHDAYcuBhroDA0DdlgGBgDA4SoMVBEOJoJUJqJWJoGKgxQJrF2F4C7F2MSMUQUEFgvIG6AguMUQVF0LsQViCguhFRcwuYhRcwuYXOPwucRUXIP4/j8P2PxCELj+P5C8hCWjmSUJSSvibxzyXkoOeJwktkrHOJSSxLEsSxL5L5LEvkrJQcyOaSxKDmDmDmxzSW8lJKZK//4gvxi/F0LoQXxdC68hCEIQhCEIQf4/kJkIQkfvIQf/+QnkIQhCR/i5CEH4fyE5Cx/j//IQGb/hHYRIDCBEkD3vA9agx4RdBj8GahHXwjqDNgzQR18De8GPwY8Dc7COwjsI6Bm4R3+Ed8I6/CO/COsIuCLwN7oRfgx4Mfwi/8Ga4M0Edf/8Gb4M3/gzQHvQR2Ed///gzXBm/4MdCLgY/CLv+EXYRd4McEX4Md/Bm/+DN////Bj/wY9UUDVzRlsyn4SlMO1BrzBIyGIxb0FfMN5A0jC7BBkxKQPcMInHNzEkQG8x68UwME3BwzG7jTZqJjes+zrFNjZu7DQ9wjpyOAn4j//PkZEkn1gcGFH+tPiIEAegAnOMcok0jB8CjOdmjAsHzFo0jL8CzB8HzTcMggtzDIHzEYM1GzAsHisHlGisCggFTAoHkVwgFFOAoBfoqIrKcGBQFhUClOQoBYQC5WBSKiK4QCqjYQCoAQoqCsKsVBWxXBOBXBOBXFQVQTgVYqwToE7FcVhXFYVcE7FUE4FQE5ipBOgjBEYRgiYRABvBEBHhG+ETwjBHCMEYI0I4BugG6AboR8A3QiMI4R4qirBORXFSKgJ18VBXxU8VhWFYI0I3CMAbvwiQiQjBGhEYRP4RIBugG6EYVYJ1FQE7/BOuKwrCrxXBORWFQVIqRW4rcVP4JzgnIq+K8VOK/hFwMfBj/wYTCJYRcEX/BmgPWvgb3hF4MeEXYMdwZqEdgzQM1/gzf/4R14M3wjvgzfgzWEdwZoGagzcIkgwgMJgZCAZShEkIkBhQYTgwn//wZv/CNgyBGQZQjMIz8IwIzBk//////4MuDKDJCNwZeDLCN4Mn/4YcLrBhvC63C60LrQuv+F1wuukBM0nR6jEpjE4we0LGMBlMeDGsQewwvIRPMFrBljEpxE4xCI4tM8zGsDDdQZcw3QETMNIDzDBEwcIw8wA6ORJxLEiHDIHmMpInjxInO//PkZE0qqgb+BX+tTiC7jfQAhBtcAdGtSJmtRImB4dmXh0GBwHmiS1mSIHFgDzA8ZDA8OjDsOjDoZTDoDjA4OisDjA4DisDzA8DysDjCcBCsBDAUBSsBCwAhgIAhgKAhgIApactIWm9NktMWAXTYAwWlpS06bKBSbBaQsAsWmQKLTFp4RARAROEQEcIwRwiADfCOCdgnIriuK0VAEEVwTkVBXioAEYIiAb0IgI4RIRPAN+EYI8I4RIRoFmAB4C3gWIFsADwAHgLcC3wAOeEYA38A3wDcCMEYI+AbwRoRuESEcI4RMIkI0AD3At//+BY8CzAtfgW/AsfAtgWALAFj/Atf4FsC2BbgAcAsAWsCyBbAtgWYFvAA5AtfAtgWv+BhhiiJr4MAiYRPhEgw8GMIoMQiBFhE+EQGEDUIsIgGARIMQiwihEwYQ84eeHmDyw88PKFkGFkMIuDCEXhE4RPAx+DEGHBjCJCJA0CJ/BiDEIv/hZFDzwsj//DyQshh5YeT/wYfgw/4MfwaAagaYNGDUDUDUDVg14NANPg1QaYNGDVVWAJo7tgn/TaqVo7GJ8J8aqon5n8CfmU0J8Ynx/JififGjsRYVtg+YWYwJkojAFgLIwsgszCzGBLAwBWU0Yn5//PkZD8mvgUACXttbB7TtgQAaBokTZWU157K8WF4sL5WvmvbBXIGdsnmHHZWdeBRcrFiwLAYt8rF02U2SwLFpU2E2UCi06bCKoUCkVQqFKNorBQLRXUaRXKwpRpFcrCkVFG1G/CBQIF0Cy0xaX02UCi0haRNgtMgWWlTZgW8CxgWcCxAA7AsgAcioK4rCsCdioKoJ0KsVBUgnWCdAnArgnAJ2KuCcAnYqCrFQE64rQTkVQTgVhWFQVRXFQVxUirFQVBXFbFYE7FYVhUioCcCqKwJxBOBXisK2KoqRUFYVBV/wTsVxVgnXFb////gWgLPwLYBuBGCMEQAb/ANwI4RuEYIkA3gDcwDdwjwjisK0VRXxUFUVRWFcVxViqK/ivBOYqirFcVhXFYVMVxW4ritFYVhU/iuK8VhXBOBU4r+KwJyKv/FeK4rYrfF4XMXwtYui4FrFwLTwtULQLv4q8VuKv4qYqgnEVor4rfirFWKorAnOKn8VP/xXFT/iv4q+K8VfFYV///+K6oB+gIE4dKA/3bwyhEExSFI01BorBoz/EEsCCVlCZQK8Z/iCYgCAYNkaVkYYpEaWDFMUwbKwaMUyNKwbKxSLANhAeFYFFYPIqorKcBAKlgClOUVlOPVKqRq//PkZFggBgEQKnXnniEjthRSao/o7V1TeWgCkWgI8shNeArCbFqWhagKImha/nyfJOScn3z7JyfZO+GuTg+D55OD5J0fJ9E75OOvNPXl5D/+vNC+vde/af0O/7Svr6/+0IavtCGdo6Gf9fX2le7R+0ftHQ1faUPX2hpaXbS0oav9D0NaUM6H9pQxo7ShqGn3D8PgOEHw/h4fgNw/iH8QiABgfgNh4hw4QQ/Dg7h8PDhBEEG/AA/BTBQGwU4KgRKVMdRmGeOmM8Z4zCN/+OuOuI1BOorRXFfx0jOM0VhUBO/ivjOM4jQzxnGbHUZo6iMxGRmEYGcZxm4z8dcZojcZ+CcCpBORXivFSK8E7FXFbBjw6DH/gFYcBmHkPIew/D2H/D3w/w+D8PvgxwYgx/8Av4cDoBTgpBn/BT+DODAb+DfwbRQO+LBPM7ANbzEMxSNM/SMMjAbMUj8MjD9MGxTMxD8M/SMMjSNNNBSMUzEM/QbKyMMGwbMGiNMUyMMjRT8sA0ViFiYxZjFEMUVFVFZFZTlFdFUrBMABU6pVSNUKwPar7VvLACplSvmXLZy+bOfZ2+LOxdF3i6FqC1BaBci4LsXhdF8XgtIui9i4I0GkNYjIjYOoRsdYaIjYzxnEZFYV//PkZJ4hKgUGFHctaB8DpgRIbhsAhWFYV4rxUxW4qYqR0EZGYdRnHUdBnHUZh0EaEYEZHQRkRmM46xnF0XovC6Lovi5xfC0wtfF3i5F0XcXIvhaheF/i9F4LTi7F2L8X8XYWgXcE6xX/iripxV/4qfi+Fo/FyLmLwWjC1i5i5xeAxARHCI4ROER/wjwiYRoR/oqhRSnKnKjaKyKgrioKwrCpFaFqi5F+Fpi7hEBG8I4RMLQL8LXC1C5xcF7BOxUFXFcVvFXFUVor4q4qir/wTmKuK+K/xXioK0VwToVBWFQVorCr+KwrcV/FSKvhG//hE//CNhG//CJ///8VlUxBTUUzLjEwMFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVUhB5V+Zw7lZmcE5gIRZhONBgIApmcRZWIxjQE5suqhkWAhYAQwmGksGcWBpMnJisFNpRzBIsycFNHJiwCigMKA5ckFHoIE0VCwFIrIr+pwHAKp2rqmVIqZqrV1TKmVO1dqzVfVN6plTKk//9nDO2cM59JFIx8GdJHM6GYRgZhmEbEYHQRkdI6jPEZGYZ8RmI2I0Ogzx1jNjOIyIyDoEYHQRkRkRsZxGRGhGcdR1HQXBcFwLR//PkZLQgHgcIFXdtTh+rUgQqa9sAxfha4ui/F6Fq+LsE4gnUVYrRUwTjFTFYVcVfFf/iuK/xX/Fb8Xov/xfi+L/i8LnwtUXIvRfF/HXHXGYdR0GcdIzDpjOOuOg6CMRG4jQ/QwLGEaKwqRVFfAsxUhH4qCrFYVxVFcVYr+NsHMNvjaBzRUFXFUVhWwLP4FoC0BZAt/8IwRgjcIwRoRgjRUgnYrgnIJ2K2CcCoCcRV+CdxX4qfiriqK2KwJ34qgnfFbBOOEeEcI+Ef/8I/+KsVv4qfir4q4qxXiv4q4rxU8V1Eb4BcxnpqhGDSBMWARzAnCKMEcK0wRgaTBoBoMh0RAwrBVDBHCLMK0Gk4toNHBDRs8sAptLQYJnliLMnJiwTmCExYRisENHBSwTmCAoGYgMXlpy0xaZNktMWlTYQLQKLSFpSsBVKHEapWrlYAqVUgVCwqFKNFYUpwpyiuisFQoA3AiAjBEBEhEhEBHCMESAb4BuhGCOAb4BuhHCJAN6ESEQESAbgBuhEwiIRMIjCMERAN4A3QjhHhEBEQjBEBGwiQiAjBEQDcAN4A3IrRWBOgTsE5FbFcE4FUVQToAIcE5FYVIJwK4rwLEC0BaAt8CyBaAs4FuBbgWwLOBb8A3oR//PkZP8l5fb+EHttPidjneAAnKfEwj8ImEcIiETCIwjQjQDe/hG//+BbgWvAtgWMC3wLMC1CJ+EaEbCMAbsI4RwiAjBE/gzXhHQR1hF4G92EdAe9QMpQiQDKQIkhHXge9+BucEXgb3AzQR2EdeDNwjoI6hHfgx0IvhF4ReEXgbnAb3BF8GbhHfCOuB71hHfCO4M3Bm/8I/gf8DPgzoR/gzgP/A+/CPAfdgffCPBH4M8D/sGdgzgPvA/6DOCPwZ3//4R1/BmwPW//4M3+DNf8IvBj/A3OCLoRf//CLvwN74RcEXJMQU1FMy4xMDCqqqqqqqqqqqpaDTOHT48JzXQzzf0BDCYaTAUijK0BTAQaDIsrDO5EDEcJiwIxp056Y56NJ6dB6Qhxox6U56I5YTlacsOyt2VjjHjzHD0VTFi1GgqKRVLAotN5YL+WlLTFpkVkVjFClOQgqFRanKnCnCKnlgV6K6jSjSnIFkCwBYAtAAdAtgWQLYFoC0AB2Ab0A3gDeCJCICJCOEYA3giQjBGCOAboRoBuhEgG7CICOAb4BvhGCMEYA3IRMA3uEYI4RARARoROAboRAJwK4JwCdAnMVxUFUVwAhgnAqiqCdAnIJwCdRVFUVRUAtgWoFkCx+BYw//PkZOsk3gj+FHdNPiPbxfQog2FkLPAsfgWALMC1wLeBZgWgLcCzwLXgWcCyBbwLUC0Bb8Cz/gW8Cx4FjgWQLfwLfwjBG8InCIANzhHwjeEQOXhEwsjDzhFwiAwCJwYfwYhE4RYRAigwBjgaYRQNAYhECJhFhFgYBFBiBqGvDWGkNPDUGuDSDRg1g1A0g1A0g1g1A0gahFgwCLBgDEIgRANAYwNIRcIoRQNAY4MPBhCIETwYBEA1Bj/4MOESEUGHCJ4MAicGMGPgx/4RcGHgxCL/+DAGP/8GH//gw/CJ/hFqPIdFY2wh6TGABLMOQOQwZAfgMEsYZoC5gYgYlYMpgyhZGH+AuYGAC5glgYGAuAuYJQGBgLglgQBYwSwMTAxBlAwZAEAXAwFgGBiLSAUBdNkCgLAYiDBFYAcEDEAE0IsAqgi4MoBVguGAVcLhQFUEXA7vC4QLhAuFhGguEiLxFQwwYeGGwuuGGDDBhwwwXChcOIrC4YReIoIsFwwXDhcOIqFwgioioi2Fwoi8RQRQRURWIvEUC4SIsItC4WFwoXCiKwuFEWhcLiLxFeFwsRURURbEUEWEVEWEW/EUEUC4QRYRbEWEUEUhcLEXEXC4fEWEWiKiKiLCKiKww/4Yf4Yc//PkZP8mVgr+AK9IACkjneQBVJgALrQw34XXDD4Yf8LrQw3hhv4YYLrf8LreGHxFuIsIqIpEUwuEEUiKiLCLCLCKCKxFOIsIsDEXhFGERAREwYihFEDAIMAhEDwiVwiUwiihFEBgABhBCIIRDBlAjSEaBGkGUhGmDKhGgRoDOhEAGAEIgCIYMCBhABoSEUcGICKQNKODEBFARRBieDE4RQDEQZT+DKAygRpCNYRqEaQjSEahGkGVCNYMpCNIRr4GhEGIhFMGJ4GlARRwimDE4RTwjSDK8I1/8GU///////BiIRTCKf//gxEGJ+EUVRMYDSRJhMAoAs3Ky6zGBBjMS5pQwJhdDA1B1MFkEcwdABjBrA+MloZAzMBrjAHAiAQNZWCMYCIDZh5j2mN+G2YdAkYMA8YPASYPg4LB+LAccan+ZEDgYmjKPA0YGA0IgNBgDEQMIBl2KJGS4ZmDwjBAeFYPSQeBhpBakwGAxpK7ACBqAUcAdpoOA9Ai0hdrZV3qMtISpaciQzB+30YG5a7IZEYDrtf5pLSEqGzP8/6HNQ5q4gAJqj9qmT1ch9YOdB9k9mCKgjTlOS5D8NUg1+aJ/F3QwX4UYUk05s1H8mbi//tmbdpa7H9aUlWu9pK7X8ae//PkZPIxNhcGFM90ADNT9jBdglgA05szTmlrTg5yHIg5yXKcuD1rwfB3//vk+L4vk+D4s69AitRyINgxTy1nJWrBqnblOU5Hwf/+tZafrXctylPLR9a7luS5bOnyfB83xZwzv2cPkzl8XwfJnLO2rf/tW9qntXar7V/9Uv+1T//2qf/tX9qzVPVM1Zq3tU9qjVmqqk//VM1f2qKnau1dq6pGre1dq/tU9q7VFSIKyxFlJOoIAcaADjggYw4OMCBAQGBD4Hpf1B3lpUsaGpubmpqPKhsPBsPJsobmhsampuoarqqm4PgPQbNjc1I+aDwarfmxooHgejUPRoaj+bNVamSaX1QPj4quHo2IpuaD+bZHIpuHrUNFR/Htcew8mhuB3H0SjtMTMdxgY+O/zFQjKmxqPA8j2Hk1I5sRiMP5qPw+mpuPhoqPOqbm6uuvqea/myqyn6mv6mbKDsMx3mRn+O0xMcyMh2ma6+uur/qf////+uqv////+ouqAAghaJRKskpqUqUmlSlSct7lhmTjSHdgBEZsxEpovlcmqEVwYUbVZr1gxmTMlSZ04chiSCyGSINIYp5wxhupXGBGJIZc5EhqjnDmP+GSAANDF8JfMSAWUw8grjBjDkMKMCIwYgIz//PkZGYtrgsoLs94AB5DYjALhVAACSDWMLoGosiYLgGhi1AnmC6CcYBYIAiALKwVDAJAI/zAyAC8dADMAsBMOAHEgF2yF9S/K7YHg5OWncpkD+goAdMlRF/E3I26nq2fQpiurQug6ElkipE2kzFTpmUDOlbvfCNxp8/+goKChZRB0CrvpoGXZStmdZW2ijb5UNFQRn6P6GNUUZ/2duu6lDG4x/0VFRxj3QfL40vyioqONUdF9HQUNBGaL6D6D38+TSX5O/kmfyTe/klf9kb/fQUH/GaKgooxGqGhdGgov/7lz79N///3bnqkf6TP/Jn8kz+SZ/5K//yR/JK/j/yf//43Q/QxiNfR0f0FBR0dHQRr//6D6H6H/o//6ONEkgQ6AXAMKgFD4cw/DwuHBdETi8XRFCI44IqLx2IsX+LsX5YvHpWVKlI7F4i8sWLFy8RUcF+PyhUfl+PR+UKD4uVLD8uU+O8cjovFwvHBcL4ig7l+VHuP+Of+WHkuPJWVLx8Wyg8y8ryuXL/y/8sPyg/K/lo9K//F9VBNCsrCzcvDesyXYS9MIdHGDCpQ2wwoIRYM1kNATJEBakxnAd+M/DC5jCHxDYxvgKDMIcAizCpAf0wEcEzMXKHfjD0xeoxi8TeM//PkZEormgsIBe/UAB4T+fwBzWgAZyIBTCHhWQwZYOPMFUBlzAiwREwGgCsMBoCgjAiwAUwO4DPA1buQMjEcDNCsA0WaANWM8Dd5oA3cBAMjiYDRYmAwIRwMCAQDNJHBgEAwIJwMCCYGAUDAoECIFhECgwCwBhYF1wbBwXXDDgwLg2DguthhgbBgNg0Lr4XXBsGhdcVkVQqxWIrAqhWRVisirishhgbB4XXwusGGC64YZHBsGBdbxVCsCr+Gr+Groqw1cKoVcNXxWOKzCIBDVorGKwGrOGrhWYq4rEVkVgVcNXCsBq+KwKrDV4q/xVCqxWRVRVxWBVis8VUVWKr/xVCsiq/EXxF8Rb//xFRFRFOKwKxFUKvxWQ1cKzw1bis4rGKoVkViGr/+BY4FsCzAN6EQEbAseBa+BZ8CwBbhEQiYRgjBGgWgLPAsgWfhHCIhEQjcIgIjhGCJwjhEQiIRvhGhEhEQj/gW///+BYwLf+Bb///gWPAt//4Fv//8C1FTivBOhWFbFSK0Vor4qxWFWKgreKuKwrCrFcVxU8VP/FU4aWvTBNJ6MhYK4wX0qDFRCQMKkHwzfzWDBHFcCyZhwrHOmr0IOYm4O5oAmBwKg5oa0Bv+PxleUZnlX5sevh4S//PkZEAo4gUQAHusPh67gfwAaBqAbRtmE5hyHJnYcBgSMpj6DRk4FoYlpqWOBgeGZg2ExgQKYJDgw/AkwJAAwtA4uQQAAQACgnTFTcU+zlfr5qfUQdN0HTV0v5fMZX7GKNDGwfI0SMOTETJGxmiMDIzMTEwE0TDMwQkKTIOTEwQzIxRI0YdB2iQjEwGwmGKGYmQcmBmhokNGhyYow+MTNCRyZCahyiRDeRMMg5RGRmhShIhuHImSYojIxlDlCQjJGHKEjQzFFJkiMjOUaNEZISGhASQ5RIZiYo5MUcoYcI0aEhoYdIaKZRog+MzEzMUZkhB0BNFJgjlCMEcozBHKEjkyM5QjFDQ0SJEjQ5QjNFMo0SNGiQkKZRmBjKPBoBpg1QaYa4Ewhqw1w0waoNMGsGrgC8DR/DRhpAmIEyDSBMA0gTCGnAmYEwDQGrAmYaw14ag1QagaQawaINX/g0cGsGn+DRwaYNGDXwacAXYNQNANX+DSDXg0A1YNfBp8Gvg0f8Gv/gC9/+DX//g0A0/4NPAF/g1/4NJgTk3ayNEhmMyBA7TFkYqMxcQQybyNDEkAzMa0yYy7hRDgUTpHAxDDUCjMY0gwwigGjBDIdMaQQ0wxRlTG8E1MZUaQxRAojIVD//PkZEko8gcKBXtvZiFDvgAAa9oAMMRUGIytbNrazMnsyICNL7Tm4I1MKMCIzExo1sTMKWg53MKEjEhoQhapgSBgolDgVUr+joFJZMme1boaSYkoZIa5ZklJMmhSzRFLFI6ZTKbTBoEnJASdpXl8kKGNAaxaoYvdf6GNJIuvNC+voYh7QhyGHwcTU1nG7azhVjW1O2tWftTW6X0MaOhqGIc0tC9+0tPaEMQ5DehqGFp+hiGIY0r3XmhfX/+h68vIehyHdDV/8ki8voc0ryHof19DkNQ1f/Q3tP6Gr///6Hr6+v9D+vL6+0If//0MQz//r36+09Dmhp6HdeaOmE100m0z/00m02mzRTBofpvprplMGmmoREbQOfwicI+EeKoqwiIR+FowjBGwicIjhHCN4RH4qQTnFcVxVgnAriqK/FWK4JzFfFUVME5FaERCP8I+EfiuKniqK2CdCtFeKgqCrxVFQVBVFQVsVMVxX+CcCoK8I0In/hHCNwjf8VcVYrip4qivFaK2K4qRXFWKkV8In///4Rwjf/8IijLjh6E0doS8MWpB/DCHRBQw9MQVMNtCpDAigmEwM4MrMOPCpDGkwysw/IR6MCLARzDTAsUw2wAnMMrDbCwBFmBFAZxgRYZW//PkZEcqLgb+AH+NbB67GgAAatvAYDQEfmEOioZgmQKoYGcBWmR9waLEx1RFmzqobuVhzJFGz0WYnExotFmrBOZGI5idFGBRMYmIxgUjGBAKWAKYnEybCBRafy03gULlpC0xaQtMmwmz6BRWFy0pYBaKqnKKiKynKK5WClGvU5UaU5U59RpRpFZFdTlThTlRtFYC1AtgWALHAsAWgLHgWAToVQAgisKkE5BOxXACBFWKwJ0CdirFYIwBvBHCJgG7CMAbsI4Bu4RPCIAN8A3wjhEQiAiAiAiYRIRwjQDdCJwjBEwiAj+EbCIhGhH4R/4RwiYRARwiIRARHCOERgWQLXAt/wLECz+Ba4FjgWsC0BbhEBGCJhGwjwj8Ij/hHhEfBqg1fgIQacR0R4LVg1/Bpg1cAn4ag1hphrDXAmUNAaAJhw1QJkGmGoNWGmBMsNAExw0Bqhrw1hr/g0f4PYAfhYLg9g+FwfAB8Lg+FsCYBoDWGnhpDRgTCGqGmGsNIaw0w14aMNWGjw0w1BqDRAmMNeGgNX//gE3//BrqM5N0IrR/LAZxg3g3FYw5g3A3mBuJgVgbmESEQYN5UJjDB9mH0DcYG4RBiYgbGG6BuZtEnumxm5sV7pm5uVmxWbGjCpig//PkZEUgWf8OAHttSCB7PgAAatuAqWEYxUUMVRjHAEsDhYHSwAGAgJYAIPU4g6DFVHIg8RoRoZxmGYFJHWJSPYXCwtHoPYewvFuWlUsEoLB7j0yoslRbKyotlRUVj0HqL49C0epYPcsLZUWj1HtHsVlhVj2HqWyvK5WVFUeo9iyPYslhUWFcsLY9sqlhYJWPbLB78rlUrLJUVFZWWlkrLCse5UWx6FQ9C0qKpVyoslpaW49eWeIzGbx1HQZ//8Z/xmjMVSsqyyWSyPcs49yotKisq4FvhHCPgWfwiQjhEeBZwLUC1AtBEfwjhG/FwXwtIWsB5C1C4LsVQTmKwrgnQrgnGKw4Kw7hwBABIOhwVYCAqCt4qipFeK+Kv4qYr4rCvBOBVFYE7iqKkVRUBOfFfFSCcCr4qirBOxXxXwTvFfFcVBXxVBO8E78I/+Ef/wiP8I//AtUJAEPU/EPBa5MCB7M7zINOxVMLgJNFCjMejJMe0UM7lhMyAvMozJMCTIMCDvLAXGURkGZIqGBCKGig9mF49mPQqmFwEmvXGJEmuqla4xIgxC81wnzECDEiSwIVgGgiqqqisMHQdB8HOQ5cGIqoqfGcdRmhGiMBHCJGcA3AjjoI0CkgpYRhnGcdRGhm//PkZIsgugcIGXdNZiBTpggIaptsBSB0jNjOIwOg6DqOo6iMjMIyM3HQZ4jIzjrEeJEFqxHCPgtMSHiOBahHDpjOM0dRmEZx0HUZxGRnHQZuMw6xmEYEbHXGeM+IxHWOuMwjERsRqOgkIEyhrw0w0/4aOGkNP//BauI/4kPEd8SER/xmx0iMf8Z8ZxnxG8RmAqYr4J2KsVorAnfirFYV/BOBXxV4uC5C0BavFUVRXxXFcVQTqK8E5gneAXw4DHDgMgxALhwGQYDodDoc+BSD8PcCiHgFOHgfAUQ/gF/w4DMAvDgdBkOB0AuK/FYVoJyK/8VhUFb/iuK+KgqfFfxVxVirFf//FfxW//FT/+K///xXTEFNRTMuMTAwVVVVVVVVVVVVNl9MMrdMMaQFMiiLMJyLMrBGNdRGMJwmMBSKNdIdMJwmMaBHNEBHMJzvMRyLNGiiwjHWEx58WZOjGTo5gpMYICFgFKwQycnMmRywLFpwMWFpC0ibAcBiAAasqVUypGqpIPkzl8fSOfNnC13Kg6DFpwfBjlwY+LOWdM6fBnPvm+b5vkOg6CMiMg6YzDOOgjQjUdIqCtFf4qirFYVBWHUdIOgHSI0IyI2I2DpGcRqIwMw6iNDqOg6DoIyIxGcR//PkZL0gegEGAHdtTiE7nfgABFpYkRgRsdcRvHQdYzxGvxehaMX8XYWuFphaovC4LmLkVIqioK3xVxVFbip/ivipxW4Jx8Vv8VhV/FTxc4u8Xxe4ui/Fzi/F0XRF/CNC64XXBsH+DIEZww2DYPhhgjP4RoRgjBEwDcBOoJ0KkE6BOATsVuEeETAN4A3QDeCIgWYAHf4FiK0VATkVwTsVATkVBUipBOeEcIkIiAboREI4RgjcI4RPCNhHwDcwj/CI4RARgiP/////////ivipFaKwqirFX+KkVRXFb8VhXgnaTEFNRTMuMTAwqqqqMuf1w9h2UTH8FUMAQO8wigRjDOCKMREO4wrAzjF1KlM00dsxdQrTAECLMTMAQwigrDDPFVMRACcwzwRzBGDuMVUTMwJgzjDPBGLANJhFBFGAICMYAgNJgTgTGCMFaYRYE5gjAjmAKCMYEwApYAmLAApzimLP5iTlgQ5hCwIYopiT+ViFgQsCFgQxRTFFKxDEELApWKWBCwIViGKIWBfMQT/MQTzFFMUUrn9Fb/CFEVkVAoWZZSnKjaKvoqJsoFFpP8tMmymx/+gWmz6BSbEIwDkA5QZAjQO2DIB2gdoMn4HKDJCNCN+B24RoMsIwDtwOwGTB//PkZPMmegr4AHsxiiLLHegAnGdQkhGQZQjfCNgywZYRoRmDL8GTwjOEZwjeEbCMA7eDKEbCNgy4HYByeEZ/BlwZfCNwjfCM/+DJCMwjYMoRgMv/wjP4Rf/8I6wjvxFBFAErCKhFxFfg2DQbB4qgHhDVwRCDAhq4NXAYY4RoHZgyBGgyAcnA7AO0GUGWEZBkhGAyAcoMgMgMkDlgcoRgRoMsDtwjcGXBkCNBkBk/A7AjQjQZeEbBkCMgy+EXhF2EX8Iv8IuCL8GP4M3///4R1hEvCJPgwv+DCBEsIk/CJIM1TBVCaJhRBnRiaGH6GKeIpmpKRjSmakNGpRh4qkeLilgbNSUjGxosKRYjSxGFhTMaUzFhcDmIEMDFhYycFMFJiwClYIYKChUeMKCkVTCgoKBanACqxFQuEAVQBFhcKDYPC64Ng4LrhdcGwaGHBsHBhoXWwusF1wbBgYeIpC4XhcIItBlCKgyguFBlCLBcOIvC4aIuIpFVDVgrIqxVRWRVhqwVQqg1cKuGrRFwuHBlxFAuEEXiLhcPC4eIqIsIoFwoXCQ1eGrw1bFZFYisCqFXFZFZhqwVUVnFVw1cKwGrBVCrFYxWfFZFZDVoqxWBWRWcVYrGIp4ivEWEVC4cRbxF//PkZP4mKgsCFXtyDiYrIfBAlSSioisRX8RTEXiLYisRb+IqIt4imItEW4iuIvxFBF4XCCLiLhcLEWiLxFOIvEVhcOIpiLxFYA4GihFAYgRWDECP4XWg2DgYkDRQZeCN7CKAxAYkGIGrw1aGrBVgPENXBGwjQioigigioi8IpCKeDYNhdfg2DwuuF18Lrg2D8LrBdeF1oigXCCLiKBcIDLEUhcNEUEWEUwusDYMDDQbB4YeGHC6/hdfhFcIp/wYnwYgMTgxPBiYMUIr/8Ir/gxQYsIqDF/CPf/wZ//+Ef8I+NljCMyRDdDE+JEMLwJEwvBEjAOETMewA8wZQvTDoBlMOgWsxEhEisLwwvQDiwAcYHYMpgyh0mB2DIYXgnxgdBemIkDIYHYSBgHgdmAcB0YB4MhgHAd+WAZCwcfZ59neZx5nHlYnlYpzif5YEQLNZctIWkPBdNnwKsVieYghYEMUQsC+VilgQxBSwKYohzimKKYovlYpYE8xBS0ibHlpQNaWnLTf/psIFJshFIRUGKDECKwYgMUIqEVwYgMUDVQYkIpBiBFIMUIoEUBieEVwivwioGqwYkGJwNUhFAikIqBqvgzwZ+DP4R8I/4R7wZ8I9Bngzgj/Bn4H/gzgP+CPc//PkZP8mqgD2AHsyeiZEEewijKKgGeDO4R+DOA/8GfhHwjwR/8GdwP/BngzsGdwZ3Bnf/4RUIqEUhFMGIBosDRQYoMT4MUGKDEhFYBgyQjAOQGXCMwZeDKDKDJBnQZwMsGWEZ4HYEaIrEVC4cRULhQuFgyQZIMv/CM/CMBl4HaDJhG8IwDk/BkCMwZQZOEZA7QZYMkIzBlBkCMgywjQOUI3gyAy4RvwjQjAjcIzBlCNgyhGAyBGcIzCNBk+EaEaEZwO3//gygyfCNhGcDt/4Rv/wjPwjAjP//CNwZf/8GXBkgywjajhjHPMlEBcxZg/zMoXMYko2aMTWQxMLH8xiMTGFNMLBcsGQ1mZDC4WMYhcz+ME2AIMAKZSsYFpysYAQLmFgsVhZAowsF02SsLgULJsoFlpgOqEWAVYLhMGUIsB3QXDhGguHC4WFwgavAcAqwiMNXishq0VQioXChcKFwkRYLh4iwMoRYRYRcRfEVC4YRQLhxFBFwuEhhww0MOF14YeF1oXXBsGBdYLrQuuFwwXCAyhFYXCCKCKwuFhcKIpEVEUEVC4URSEbEVhcOFwoXCCKYMsRb4i4isRSIsIuFwgigioimIvC4QRQReIrEVxFQuEEUhcMIqIoIsF1/wuv//PkZPsl0fz8AHuSDibTuewAnCXohhgutDDwwwYeF1vC634XXC6wXXDDhdeGGDD//DD4YfDDhhuF1sMN4XX/4YeGG4YfDDhdYGJwingYQwjEPIHkBgMIpwiEGB4MRhFPBiQYnA0ohFMGJ/BicIgCIAYEDCAGBCIcIgCyOHm4eUPMHlDy4efCyELIQsgCyCHmCyIPPBgPhEIMADAAwAMAEQBEARDhFEGJgxP4MT4MRgwgYQYBFhF/BjwiQYAaQMAigYgx8GHwigYQMANMGGDCDGEThF/wYgxBjwYf///////wZARmcmBthkBmd0CUYTITBg/gYmBiAuWAFzALAeLABSjYFAXLAGAEAxMAoAssAFBUAorAXLSJspSAYGBAtNgOABDgAg4ANUqpwZYi4HVhcIAmoiwi4XCCKiKgKuFwgasDVwatAcIrAatDV8VgVkVQauisBq2A0EGCGrhWRVgOEVkVQqhWRWA1YKwKxDV4qw1fiscVgVYavFZxVxWA1cKoNXBq0VkNWCqFZFWKwGrw1eKzFUKwGrorAauisRWYasDVsNWCqhqwVmGrxVcVQasBgCrDVgrMVUVQ3hvQ4YoCN0b43xuigMbsbo3BvCgo3Rv434rIrEVXFV/hq7FZFYis//PkZPwkwgkEFa9IACnL7ewBUpAAiqisxWBWPFVis/8NXhq+KyKxDVmKxw1dFY8ViKz4q4rP/hq3FYFYFVirBkwZGDI4MADAYMkGTCIgYABiIGAhEAYMGRBkBGIRGEQBgAYDwYARAIiEQhEYRADEYREGABgIMn8GQDI4REDAYRHgwMIiBiIMAGDAwGBiIGI4Rn4Mjgyf+ERwiIREIhBgQYPwiIGAcGD+DAhEYRAGDBgBEf/Bk//+DJ4RngyP//wZGEZw8weT4eTDyh5oefw82HkCyEPOHmw8uHnh5Q80GDBgfwYH8GBVACAwFYYBAYQAMCAAPFZTBJw7Ax3whWNoVJmjGpQf8OAFwUANmNSBohkK4kSYkQIOmpLlORYAAg4AUMEUDUjCRgHoxPUJqMakLtxICwv/50DP1GnoTUVgnGJO/WcKy5plsGiFm12NmL8GRmPaY1IspiQhBmJAGMYp4kBhXCQGHmGMYVweYkAugQXeWSL8mDUBUmYCgJBCAWCgBzCuCDBgCINAnKwVDAnAQdBJKifJ1V/umkev9f7oK4EIBYcAOWAEiwAGyNDd0vo43R/GGXwP9xdisCANsjZi/QCAVXaIgCy/Vykk1y98UuXWTyVU7J3/f9U7J1SCEAJk//PkZPkxOhUePs/4ACnkBiwziVgAyplSMjZE1R/H//70kpKa/JPuX5MyT2QP5JGTv4/jJ/kr/qmTIarJ2RP7JZKyKSySS/9FRf9B/0VHG///////f+Tyb/k7+SZ/5O2Vs3/67WzNm9sjZ2y+2VdxZL//////////////5PJH8ZIXLVO/7JJLJ/k3//v4/km//AIBRfVdq7WyNkbL7ZGz/7Z//2yNkAmECEAQPci6aN7kKNyXe5EDS2urq/m5ooPyi+aKm+aB7H1Rb/UUNDZY2UNwHI5NzcPqii2bKraxqrmo9B6Hojmw/rLLLq65qvmi6+ZmNlEuLSggFNY1U1lF9RY3VVU1FDbH4fh5WWNR8UV9ZU3WV/W1TdRX1NRZfUzQeTRXUzdVRZdc0W///11TXzb83W1PU1NdbVzf1v/9b//NvW9fX1P/1v/1PXU1lT0/MfMy50U+MPVCUTDhgtEyP0bcMQjDHjSFwekwwQnMM0jT6jO2g6Ex18HOMPVHrTDoAc8wMoQJMI2DhzAFkCs0F4sdMpOPDzNPQYEwMsB+MUkASTEIgh0wRkFMMCYA/zASgjcwjYFMMFnEigOkinwNaCAwMphMwMjAfgMmQZAMTA/gMZpMgMf4ZAMGI5AMJQFg//PkZJIxJgcKAO/YABsbeewB0pAABhlADGaAxZhKBgZQMMoMABQyAYFwLAYFgLA2DwBgWgYFgLgwCwNg+AMC8LrA2DYAoFwBgXAYMQLAYFwLBhwutDDg2DQbBoXWg2DQbBoNg4GwbBsHBdcMOF1gw4YYGwaDYNDDBhguuGG4XWhdYNYDYNC6wNg2DYOC64Ng4MMGHBsGg2DYNg0LrBhguuDYO8GwbhdeIuIvC4URQRcLhguGiKiLBcLEUiKCLBcN/EWfEXEWiK4iwXDxFMRWIuFwwioi3hdbDDQw3+F14YbDD+F18MPhcNiKCKCL4iwioXC+Fw3EX/EW/EXFV8VfisfxWQ1cKwKsViKsVf4R6DOhFANFA0WEUgzgZ4H3Az/wPuhh4Ng/8I9wZ8I/+DP/gxYRWBqmEU8GfwZ8I9gf/hH/4R6DOhHwj2BomDFBiQiuEVwNVhFfBi/wYvCKQYvwYvwj////+DF4MXhFOBqn/8Ipwj3//wZ2DOWJwq9aHVShgYCYSRjinXmceI4ZTxQ5nbEGGIIF6IC/jz6g9Mt8z4zJyhgUPwZSwKRhFgJmkwE2Zf4DZiyF7GYMSaY1p05nzCWmGICGdtDZpsBmUuqYLJRhJmmgnaYjJZgNFmUwmY7Z//PkZGctIgsQAXuPaBvDwgS0bFvkhgNBGmzEYDGZiIJmGwUIQmPAQwmAg4amCg2yYOAg8BGTP4/jJn8VMm3JWTlgBqlaoGUSIskNJGWjQWhI/0M68hwY7T2gs18TZD0N/XmlDyyQxeaEPaSyJAhqHdoX14kgmhJEPE3Q9D0PJP+vLxJV/800wmDRTYniZE/TRpdNieGmaHNA0E0mE0aSGLzQvEkQ5oaevocvIYvIc0NC+h7SvL6+hjSv9D0OJAv9DF79eJI0Ly90MaCQEhJGh5J0O/6+vIYvoeWSHoZ+hiGNC80dDGhoLJoaehxarzQhjQ0Ie0rzSh/7Sh6+m03xPDQTP/TSZNBMmmNtNJg0OmTQTSY/Tf6aBJDgYJwK4qRUitCICIwjwieEYInCPCOESEeEThGCP+ET+ETCJ/+AB8CwBZgWwLX/gWvxUxUFbFWKsVYBuhG//hEeDLgyYRvwjIRsGUGSEaDLBsHfBsH8MN/ww8MPwZP//hH/CN/////ip/+KsVhVQfEYg5uroaGPAHOYIJ8JnGjRGMyNGYJYEBj2lJmKKx6Y+6H5jCEVmK0SqYjZEJinFOmOEOaZCojRgqC3mCCRUY+wZAyVkYsgwpg1glHZFmYMKpiAVGVSoYr6//PkZFordg8OBHuPahyr+ggAa1Vc5r0gmFxAYMBhlQqhUbmIT+EFYxCDDBogMLAMwEDRkQmDQEiuqso2qqqsqoo0qqpw5asDlIrqqeqorAPAeTP7JX/f5kjJX8f9/jiamtXK8nPPt0fatdq5DyQlo0fryHIav9pQ1DF79NmgmAcwpSbE9G1+m/0wmzRNE0emf+aRpClGmaKbTBpGkaSbNJNpk0010zzRTXTKb/6aTSa/TPNI0v010waJpJkT1NJs0fzR//NE0U1+mjSNJMpo0RPUwmk30ymE2aIpPTSa6ZTSZTBpGkaPTSaTJpGkm02mUwaX/TXTXTP//XkOQ39Du0/r7SvoahrQvIcvLyHL680tDR0NQztIrxVFUE6irgnXFTivxWFeCditivFXiqK4qioK4rwTjFeKnxXFXxUFfxVFbF6L4ui5i5F+LguRV4r/gneCc8VxV/FbFbBOYqf/ivFSK4rcVP/FXxX//iqKnirxV/4q4rfwZ+DIc+DIc/BjwZ/Bngz4c/+HVQCbZRzM3S2N4t8Ex4xUTI+GJMvcTUxtRrTFjBGMbQn80ZFATXJCaMuMP8wow0DJaA/MZEZsxrRdDDGERMb4PUxfgvjDbAjMCIPUySQkTCGB1PHrTYi0//PkZFYpZgsKKnttah3TBfQAk+GA6yaN1IzIpo5F1MtPwEjlkjRkYzUiLB8ZoDmWmg0jDQcWRDA0y8ADgMsABgACHABgAAIRArIXwLAmkgzh8vZz4JBisBVMIQBUnmAgKp1Sqmau1QAIIqiviuCcAEwqCuKgqxfi4FqFwLUFo8XBfF8XoJ0CcAnYripFYVxUBOwTmCc4qirxei4FrF7F0XIuBaReFwXYvRf4ui+LwuxcC1i+Lgv8XcXwtIuC6FoF0XBfi7GYRiMwzDoOgzjqI18dYjYzcZ4zjqM+Oo6i5F6LwvRcC1C+LgueLvhasXsXRfi4L4u4q4rxUioKorioK2Korip+KkVoqCt/BkcIx4eTBg4RGDBCIYeYPMHlh5/Bk4MAGD8Ih+DACIAwQZPhGPhqSP/DUhqSPI8jh7j34anhECKDEGPgwwiYeX8PNDyB5A8sPL8IoMeEUGP4GPhFCLh5+HkDzB5A8gecPKHnCyOHnh5cPJ//BjCLgx/gY8IvCKoJrfyJ35fhjuCpjsbZgoIxiOOxWbRlShhYNo0+pkx2EYwVBQwVBUrQwxHNsxGBUxGNox3HYyoBUypQw0NHYrBU4xQypUsFTKRjjxisCVujAADAASwBMABU7DBgYOMM//PkZF4hVg0SAXdPZB7qmgQAABoEHTFTETGN50O03T6V5xq4nKt7W7a2vnwvyzSoYqXilVD/z+eZ6pZ5UMfKh91f3X7tXtXa2trdNTvumo++rGvq906av2tXtXd+ed5NNO/lnklfyPZHk0sj6bzdrVjW6/av3Su///au1d0rP/3asdO1c7av+6/dNTp33XdNatdtX7t11c6a3Tpqaur3f/7prdtf/////dfq3+aWZ+98080nfTeR/NJNNI+f+bzyyz4RME4iv/CICPCJCPhEwjhEhHwDf+EaEfBOYq8VRXwToVhVFXisK4rC4LguQtWFqC0i4FpF+KuKwJ1FQVhVFYV4rivFXxXFWK0VhWFeK2KoqRdi4Loui8L8X8X4ui6LkLVhaRcC18XhfF8XYuC7xeF+Foi5C1i+L4ui/CIqGyyTLq2SvsDE4TjCNZTOQIzCIIytTywTHmySTmk4nGXYnGJ4nlguysujJlTysmDJkmTE9JysTjLsTvMIhj8xiCMwjCPywHgNCcHBEoygGBwRjIBjQCKcqNweiv7khQA3Kcj3J9y0waXTKZTYnqZTIZCHLy8hq9y0QxeQ1NdNGjzQFJTRpGl0x02aH5p9MJhMc0emkwmzRaevoc0ry+hyHr68//PkZKIfHfsOGXXnrCCLrgQQa1rcvf/tKGof/17tC915Duh3aWlfX17r//a1cb7r9r6ua3XVvVjW1/tbtq58OumMOiHAZ/DohAeA4PgODsB3iHxDiDiAQh+IfDw4PxBh4hh2DPwZwUgA8GQYIUV4q4q4J3itFQVoripFQVcVhUxXBOBUiv4rCuKkVYqirxXxUitFQVOCdxV4qYqCt4rirFYV+Koqip4rCoK8V8CzwLUCyBZ4FqBb4AH+K3xWFYVxU/+FpFwLUL0X4uhaoWoXuL3xd/FSK4qivxXirFTFX8VRW8VxWirxV4rf///8VjUubzOO6801Ly+DKJBSMW4acwGgUjHeGmMMUW4xNRbzE0MzMsEFMxpiUTKIAbMIwacxbiFzCMGnMFIFMx3g/TUvwxqMPEGjUowrjTGxs1PEMajDjFM2Q7MPDzDg4sHRnZ0VnZnbKYcdlgPMOD/LAcYcHgQXLSFpkCi06BRaX0Cy0ybJaX02UCkCkCysAau1dUrVFSKnauqb1TgwA1cKrCIBq4ViKsVkVjwutDDww/C64Ng4LrhdcGwaFw8LhIXCwuEEWC4YRYLh8RQLhAuHiKCKRFRFxFQuHC4cLhBFMRaIqIrAVcRXiKCL4YaF1gbB//C6//PkZPIlmfj+AHtyXCRr5fQAg2FM0GwZ4XW4XWDDhhoXWwutwbB3DD4XXDD4XXC63hh4XW4Ybww+F1v/4XX8Lr+DYNiLiLiKCKiK//iKRFw80PMHmh5eHn8IuEQDHgxhF4Mf4MAigwBhAxwYQYwBfg1A0A0wa4NANINANcGkGsGjg0ATINENENOGoNYag1QagaQaga8GiDSDUDVBqg1g1BEgxgxgx4RAiQYfw8wWQYeSHk4ecPMFkOFkAecPIHm8GP8GH//Bh4RcIoMf//+DH8Iv8InAx///DyB5g83/h5A81UxBTUVVVVUyd3nTecZ2M1ACwydgLDEGAtMQYQcxUAdTB0B0MVAVAyvxUDHjB0LAOhixfmLPGZ0X5wcWGvxYa+g51ComvhYa+Opiw6lg6lYsMWCwxYLSsWGxA0WA2WA2VhosBssBswUCggLIrhQFGHwUo0o0iupwEBZThTlFRTnxXBORVAJgEAAQYJwK4rAnAJ2CcirFQVhWBOxVFYE6BOBWFeKorgnEVgTnCIAN4IkInCICIAN2EYI0I0VIAA4qAnQrCqKwqxVBO4qwTsVBUFQInCMERAN8IgIgI4RMImAbwR4RARIRsIwREIiEcA34R8IwRPCI/gG78Cz8Cz/A//PkZPclrgcAAHuNPCVTWegAlGWotQAPcC3+BYwiAicIjANz8I0A3QiAiOEcI/COAbwRPivioKoJ0K4rCrisKoqQTkVhUFUVPiuKnCP4R4I+DPBnhHuEegxQigM4Gfgzgj4H/4M/wjwH/YR+DOhHwjwM7Bn4RXCKgxPBiBFYRQGJA1UGLwZ3gzgZ0I/hHgjwRgMgMoHLhGgckDlBkCMwZAZAZMI0DlCNCNCNCMBkgywZYHKDKDLwZQZAjQjQZYMoRsIz4RvCMCM/wZ//hHgjwR6Ee//hHvhH//8Gd/+EewZyNqRPU1zB/zNEJrMawFUwogLzAuAvME8E4sAnGMkCcYXQJ5jJhdGF0CeYRIRBgbgbGBuESYXYXZgnhdlYJxhdCTlYyXlgE440c48YrKnHKGVKg0iacgowDkBkCKAcrIFZFRkHIFE/UTQDlZFRNRJRhRMsEPTFKw6YgYMU+mMp9TynSnananSYinvU+mKp9Tr1OlPqeU7TGTGU/6nan1PKeTEUZUTQD+owgH9RL/BhFRP0AijHwaQBdg0A0waga4NQAvgC/BpBowasNIaQJgGoNYEyw1hrhpDXAmGBMsNcGoGmDXg0waMGkAXwaQawaQBcBqg1A1wawaAa+ATeAAN///PkZP8mkgb+AHtNbiWLmegAnKMYAAG8Am/AQ/wagaINEGqDUDV8Gjg0g0QaAaYNYNUGnhohohrAmQaQ0w0hrDXDQGqGnDT8NIaP+BlKDChEkIvwi+EdeEdAzXBm/+DFA0QD/uDOBn4XXDDhdcGwaDYMhhoYcLrBdcLrQbBgXWC64YYMPC68GLBiAaKEVhFQioRSEVwj/CP/hHoR8GfgzoR/wj+EfCPgz/BlA7MGTBkCNCNgycGQI0GUDkA5AjfgyBG/wZYRvCNwjAZQjAZAZf/////CM8I0IyB2/wZP/4MlTEFNRTMuMTAwCfNageETueooMYYiUgWYLjIZfhYWAtMvx0MoChK1fMoSgLApmKYNGKYNFigLCAdCgmWuhXfGWFpupaVspYDvKw8w4OMnJzJyYrBTJybywTFY8o2o0o0VhaKynKK/hAqiuiuFQtThRtTlFT0V/UaRU9RotMgWgUWlTY9ApApNlNhAsA34RABuAG8Ef4RwiQiYRwiQjAG+EQAbgRgDfCIgG4ESEcIgA3oRARARARARHCJhHCMERCPCOAbsIjCMESEcA3oRIRoBuBEgG+AbsIiERhE4BugG+AbwRGEThHwj+Ab4RsIkI8IiAboRgiOET4BvgG/4RwjQ//PkZPYlagr+EHdtTiWjBeQApOR8jwiAiYRwiIRGEYIkIwBvYREI/CJ4REI4ROEbhEYBufCNhGCJhE4RwieEYIgIgI8IkI3CJhH/gyDga1aDFoMdCLsIuhHUGa4M1/BhQMpQYUDIQGFAyECJAMhOBlIESAZSgZS/Bm8I64R/hH8GfBnQjwM7hHgP+CPcI/gz/wZ/CPBFQYgRUGIDFBiBFAigMQDVMIpgaIBokIqDEBiAaLBieDFgaKEUCK4MXgaoEV8GfCPAzsI+Ee4R7CPfgz4H/cI/hHwj/8I/4M7hH4R5TEFNRTMuMTAwVVVVVRrAf1zYbqeIZ3BeYAh2Ydh0YAgAY9gQYEBcViqY9lGViqYXj0YEheVgQYXioYXheYXBeY9D35j0KphcKhYAkwOjBcMEAwQDBBDLwuOp5MUxxlOkAyjKAVRPywggFU+GXBhoWHCw6YqngsMomDolGVEkAyiZWiokgHU69TpMf0xysZMRMRTyY/piJiJieVjqd+Fh0xywMp9MaGmBMg1w1gTMNMNGGqBMg0BoDWGgNIaoaoEww1Bq4ExDUGkCYYEyAmeGmGiGsNeGngTCGiGnAmQaw1eGgNIacNQaQ1hpw0BohrgC4DRBoBrg0A0g0cGiDQDQ//PkZPIkhgsAGHctbCZrKegAliTADT4NGAL4NQNMGj8GiDTBr4NP8GqDTBp4NHg1w0/DSGrDXhqDTDSBM/DSGkNIaw1hoDXBnYH3/4R+DPgzwjwR8Gf9NhAtNgsWBncI8DPCKwYuDFwivA1UIqEV4M6EewP+BigxQYsDVAigGqwYsDVMIqEVBiQikDVANUBigxOEe//BnQNEBi4MUDRAYsDVIRUIoBosIoBqgGqhFcDRIGq4MQGLCKAxANVCKQYkDVAigMTBiBHsI//wZ/gz8Gdwj3gz/gzwZ3//wZ4R74M5QA1gDXRdDoOYTKxdSwB5YJAw7DoxGEYwFCcxHAUxHGgyLCYxpCYwmEYxoEcxHGgwFEYwFCYwECcwmAQxoAQrCYwmAVRsI8KywhY3ywhRFcKFBUosF+WOz7OM/srOKzywcZ5xYEK5isUsCFYhWIYgpiCIF+gWWkQKLSemwWnLTpsJslpy0oGsLTAawtOWkTZ//UbU49FRFVRpRv1GlG1OFGlGghRRtTj1OFOFOVG1OFOVGvUbCIhGCNCOEQESEQAb4R8IgA3AjhEBE4RoRGESEeERwiIRGEQEYI/wLMC0BY/8C2BbAt+BYwjhGCMEfAN6EQEaAbkA3wjBEAG9CJCJ//PkZP8lfgj+eHctbiei9fAgjiL0AN0IgImEYI2ET8ADngW4FjAswLP8C38CzgWQLUA3PhG8I4BuYR+EeEfwDdCML8DlwjIMnhhww0LrhhvBkoFps+myWmQLLCkVQiyjajSKyjSjSnKnHorKceo36bCbPps+gWWmQKQK9Nn///TZLSlp/UaRURVCKorKNoqKc+iso0iuF1ww0LrBdaF18MPC6/gycI2DJgygdoMkDtwZQZfDDA2DAw0MNwuuF1oYeDYOwbB8MMF1//wjfwZeDJ8LrfDDcLrww8MOF1ww/DDKNIqWQyniuTEzGaN+sywynZZpzBiaULgWyLAsbKYnmCxpYsYuyGlixWygQWAzEYu/AbIK0ozEXAxeBRYDFybIGlQO8CWQLQKAtwJc1+9ThRpTlFRRpTlFcIupwo2o2FVIr+mwgWmwgUB3lpSxZAv02ECi0ybCbKBRaRApNhNhNlNktOWk9NlApApNlAvy0voFJspsIFemwgV6bPlp02U2C06bKbKBZaUtOmz6bKbP+mz/psoF+gUmx6bCjaK6K6nCK6jaKqK3qNIq+pypwVqRVUa9TlFZRtTlTj/LSFp//0C02U2f//LS/6bP+gX/psIFoFoFegV6bHps/6bJaX02//PkZP8odgD6AHt4DCXrafQAk+GEfQK9Ar0Cv/0C0Ck2S03+mx/+Wm//8tN6BSbPpspsemz/+gV////6bH//+mz/pspsf/+mx6BSBfoF/6Bfpsf6bHps/6bH/CM//wiAGA+HkCyDE0hisMU4mgMn/AxGDA4RH4eeHlh5A84eQVv/DuAzis4rA7g7hWCtHuGqI8ew9yPHsR49g1QanhqA1IagewMYRMGP4MYRQYYROBhCJwYf/BiEUPNCyMPPCyMLIQ84ecLIQ8sPIHkh5IWRB5A8sPOFkEPOHnBj/+EUIv/wicGPh5oWRB5g8/w8+Hl4eSo10sfjwgsENRQ/cwoAQDFeCgLA3pjeB/mK+FAYII3pjeDeGK+H8YUIIJXQGg/xWgmg/5/9AaB/mq4h/mKamNf5oKCWP80GhNBQPLBaVuvmWFpWWFgtK3Q3Ut83UtK3UywtN0dCwWeWC0rLPO+LTLS0xpS8sDZWNmNDRWNlgbMbGjG1IGLMDWrQitCKwIrQYsBiwDWrQNasA1iyDFkDWrQitA1q0DWLIR6gaxYEVkDgwQZBA4MADgQQZACMEGQOEYEGQIMgAcCADFoRWAaxYBrVgH1WcIrIRWgxaBrFgMWQithFYBrVoGsWhFaDFgMW//PkZO4vggDoAHt0PiQzhegAjiboAxZCKwDWLANYtBi0IrYRWQisBizhFaBrVuEVkDWLYMWwNYtCK0IrIMW4RWgxbwYt4RWYRWgxZwYt+DFgRWQYswNYthFbBiwGLQNatgxbBi3Bi2EVgRh/BkHBkHhGDgyBCMAGQcIweDIEIwQZBCMAIwIMgYMWQYsCK0IrIGsWQYswYsBiwGLeBrFgRW8GLf/gcmB2QjYHLCMhGQjYRqK6nJWrwqoIuo2pyo2iqFFBFUVPCKeo0isioVrU4hEn4RLCJQiT4MIESAwgMLBhAiSDCQiQDIXCJAYXwZvCO/8Il4RKESwYUGEwYWESgwoRLgwn///BmvgZC4MLCJAYXCJAiXwMhIRLCJP////COuEd+ESBEv8IlBhf8IkhEsIlBhINDNkKwOIo1ExDwbjBvAyMG4FowCwqz3HzFiiwKCRpnzxWKCosKxzFsiwKMULKzxiowQXCMxWLRWCjMKiis+iu1QOgGQAFYD1TlakVkVAosIqa1oqlpy0oEsWkLTFi6bKbJ/UWFhVanKKgRUrUo2isFV+iupwiqFVqNqNor+ir6bP+WlQKLSIFJsFp0Ck2E2UV1OVGlOUVlOVOFOFOUVEVPU5/1G/U5U4Ciwoo//PkZKwsEgj6FHtYHCSjNghUAtpgIuiupx6KyKiKinCjXorKNqNKcqcKcKNorf6K/qNKNepwpwir/qchVaK6jfqNqcepyo0isispx6nKnKnKKynCnCnCnPqNKNIq//qN+o2o16jfqN+o16nH+iqpx6jfqcIqf/+o16jSjajfqc+px6nCjX/6jSjf+o0isir6nKjaK6jSnCjajQUX6nPqNf6janHqNqc+px6jbVFS/7Vmr/7VWqtWLAWqeqVU//7VPasqZq3tWao1YI5UkDFYCfi6FphG8A3wjBEBHgG7FTxUAN4A3QDfCJCOEThGgG+EaEYA3AifgnWKwqCoK8V+KoripFX+ETCJCP4RwjhHxmHURsRodR1EaGcRiIwOvEaEaHWCcCuK4qCuCdCoCc4qCvFYVcVBU4r/iqKorwToE5FUVYrCrxV+L2LuLni5haxeF/F3i9F7F0Xxc/hHMOg7w0hAmzBfFUNeXxCnnNCBl5+Z9NmfgBl4gVkJp68aefqlEIgWBExABMQAQ5cDl0wEhEAiWAEwAQEAAYAAiAQUQFAcVBjHgYuUzotRNgFYtQFQBXLItAAGAoAAEswRxZibAP4CiWhZgAAsyyAU+AqFmWpaCaAKID8A/CagKJZCbAjh//PkZIQrHgz6AHtvDiVrcgBKa9skNRNC1E1AVwFEtQFD8tBNxNCyAUOJuWpaiaCacteJqCPLIsyzE2BHloWQmgI/8sxNCzE1LIBRLITXloJry1LQtCy5ZflkJuWhaloJqWYCuJpyyLMsxNy0E05a8tC0E2E05ZCbFmJsWRZlpxNC1E04m4CqWhactCy5ZFp+Ao8suA/iaiaFoJsJsJoWYD8JoWZZfgKwmhZgjuJuWhZlly15Ziaflry1LIsv/y1E05aln+WhaFl+WhZlqWvLItC15Zcsi1LMsvy1LIsizLItSy/LTlqWpZFoI2Ef4FoIgImKorioKmLwvwtYR8ImFqF4XhcF+ERCOETCJjYByjbByDaG0NoHMWhaFqJsJqWgmwCtwjAG7hGCJCN8I+EQEcImERCMERhG/wj+ETCJhE4R+Ab2ERwjeERCJ4RwiIR+ETCNCJCICIAN4I8IwBvBEAG4EQEfCIAN3COEYIjCJwjYFr///hGhEf/hG4RH/4R6MyEw68lDOC5FlgCRWKBMWK5gYPFgbioeBQ+FgaYTA5ckrA5YCZckwmB0kEjS5ZchJFnL4FgDPkmJBiYi11qOQ+aRjOmcem0kctGDYMLmOWp0tH+fRO+Ts+OAsnwfPPs+//PkZGAncgsAAHHrriiDWfwgbhrCg7T5PsnROz6JyTknJOidBKidE4J2fZ8k6DtPknISc+T6J0fR9n0fR9n0Tk+vz7AWOfZ8H2TgnR8hrHyfROQkxOj6CVH2fPJwff5OufZ98nIax9n0fJOD7Ps+efR8n0fROOTo+D4Po+efZ9n0Tg+j7PgnR9E65OPydH0Tvk4J2fR8H3z4Pk+CcHwTonfPs+idk6PonBOidnyHZycc+D659H2AmAkKxWKwEQ6Kw6Kw7isOAIhzFYc8OgICoBH8OfhwVBwVCoVgJB0OhwVBzATw5hzDnhzw6HRWKsOB1HgG6ET4RgDdFWCdRUBOoFoCxK1KchRajSnAqRVFeKoROEQEcI+CKFoC0C4LwD2FqwToVBUFQV4qRVFeCdCoKwqCqCcAnQRgifhHhH4q4rgnQqxXgnArioCdgnAJ2KgJyKmK0E7ACAKoqgneKkVRVir8I8I+ESEcA3fCJCJgG9wDcwiOER/CJCICPCPwicI+ERwj/8I8IiEcI/+EfhEeEf8Imjd8PjuQmzE8jTE8ADC4TjCAPjE4EA4AzBAERAAJjaCJgCCAeCqcQ/KIptJJFiAsAtVauXKFBiscUGSSasqUsAmii1dUqY61YPWkXOWq//PkZE0jagj8AHctPitLogggao+w5HptpIs4fJnYKOZyqYsAFYCpWrNVVKqVq4qiqCdAnQqitFYAI4rAnUAIEE4BOwTjBO4J0KkE4BORUFYVYJ3FeCcxUBOcVsVYrQTsVATsVBVBOgCcVATkVBUwTkVgTrFcVwTkE5BOhVFcE48VxVFcE4isCc4qCoKoqCpFbFbFQE7FTxVFcE5gnYriuKkVhXgnQqCoKgrirFTiuK0VxUFcE6itFUE7BOxWFaKuKsE68VME6FfiuKkVhUFYE4iqKmCccVIqRX8V4reKgJ34rxX9F6L2FoxfF2LkXhcF2KorxWxUBORWhagHeLgWoXhGR0GcdBmEbHWOsZh1jMM4ui/F+LwvhaMXsLQLovC7F0XBnjrGcdBGxGRGB1B24MQCodh0OgzALgyAWDodgzDuAVgxDodDgBUGIMAF+AVgFocAKgyHODAMB0Av+HPAK4MwYDkGA58O8GQZh2AVBiDGHIM/gz4M/4cBnBvBkFAZBWADgoDAbgpBXg3BoAEFAZUEVoHHdqGdIfmH5VGCAnmLwfGJwfmCAImCIQmCIAGgAaKAggEAAf+1bytAwARAA1YOBK0EViwUELBC4UKKwfDgg4IOiLACjSKiK6jaKynK//PkZE8g5gr+dHctLid0Dgjoa9r0KwVKCF1GkVQqV5lFqNgBAFaCcCuCcwTgBACdgnME4BOBXFYVBUiqL4vC+FpF+FqC0i4Lgui+LwqRWBOhVFeK4J3BOYJ0KkVIqCoKgqRW4J0K3iuKgq8VoqiqKnivFYVBXFUVYqipitxUioKviqKsVOKorgncVxWgnMVRUFf+KuK4qfioKorYrYqcV8VRVitioCdivFaK0VcE5FfFaK/haRexcF4XxcFyFqF0XxcF2Lgvi9i9F2L2LwMOo/8VouhaBeFSKgJyCcCuKuLsXzQGOaHTHE3E0LQsgFUTUtUwaXTRpJk0k2aCZTf/NA00wJt+WvLUTYTXlmJoaX6bTBpjFTJomhzTTIuxeF8XhchaYvxeF/4WsLWLsX8XxfF4LR8Xxdi/4ui+L8LSLgWvC1YWsXxc/wTjip/FQVhW4rf+Kviv/iqKkVP//8VOKv/8V8X8XPi7//i6MsSTP6ySMOhYMEBUMmQmUYMOhZMHBZMdhZMHAAMAQdMAAAMHA6MHRZMHAdMEARBoIIBywDhYAArAErB0sB2VgCYAAD5g4DphAYQFgJhCVhPoCsJWEscMISsJWArCgGUYQCg8IMgokoz5Y6VgKwGABgAWAmAJ//PkZHUhSgT0AHcQfC8b+fzAa9rUYD6iajCjKiSiX/6if/6iSjKiaiaAVRNRn1EgeNAL6AYrj6jCjCjCjKjCiSARAOgEUT9RNALwYBEBgETgwgYwYYRQYhEgwhE4RPBiDDAxBgBpwYYMYMeBrgwgaQiQihEBiEUIgRANIMeESET/8GHwiQigYgxwYQigYwYeETwYwi8GPwYwiYMODD/CKETCJhE8IgRQigxFtwiMIjAA+BZAswiQiQiYRoFrPonHPgnPG2DkG0NnjY4OcbfGwDkE0LMTUsy1LMsuWvE2LUtC1LQTUsxNi15ZFmJsWnE3LQTYVBWiuCcgnAJ2KuK4qiqAbn4R4Rgj4RgjAG6KwrRXFSKorCsKgqiuKsVQTjFSK0VuK3ioK/xU8VBWirxXBOBVFXiqKnisKwrCvisK0VoJxwTsVIr4J2Kwr/ipFYE5FYE78ImET4RP/wjcI3hH8VcVv/+Kwq05JIM6ND0yDFQxUD0w8FUwRBAwnBEwQCZRJRgHBGDAmBwmlgJzBEEQuY73DGNmbIWRL8qdhYyYpWdMf1GEAiAX0Ayn0xisyngudMdT6AVAMokgEUSUSUYQDoBVGEAqjKjKiX//qdep0VnU9/qdqdJigTCGkCZ4aA1h//PkZHkfYfz4AHcNTjHTbfgAa9qcrDXAmAEy4EzhpDQGsNENAag0w1ATMNIaw04ag0BpDUGsNfDXAmOGgCY8CZgTICZATENMNMNeGgNYaPwagBdBp4NH8GgGrBq8GoGkGuDV+DQDTgC7/BpwaQagBdg14aoaYag1w1hrhrhq4aeGjDQGjhohohoDQGkGsGoGr/BpBq/waf/hEhagHYFqGyNsHOTk+efZ8H0HafASQ+Scc+D5PkJVydn2To+yck5Ps+ick4PknROwkx9Brn0HafR8nyAsHwTgnROj4JyTo+Cd8ncXhcFwXgHYCKFqFwLTFwXoR4RoR4BugG+EYIkA3QjQjBHCPCOETgG/wDcgG74RoRgiIRARABvgG/CPwj/CICICJhHhHwiMI0I0IkIkA3vhEwjBHCJhH8IwRwjhGCJCJCPhEAG/4R4RHwjwjYRIRIRwiIRGEQERhGhH/CIVFlAxMxzjSLB/AwzZacwSgSwKAuYGICxaRAowSgSgMBeBAFzXwLSmssYsxWIYsybCbBWsBVy0wFWTZAixaQrKChRWUFCvRWChQUKRXUbRWMspRr/TYQLTYQLTZQKQKLTFpEC0CkCy0iBSbOFwgi+IsIsIuIsFw4XCiLAJoRWFw4iw//PkZIEdnfz2ZHsyPjUz9fwga9sGigigigi/C6wXXwusF1+GG8GwcFw8LhoXDiLCKYivC4TiL+F1gw3/8LrQw2F18LrBh/+GHC634YYMNxF4i8RfxFRFOFwoi4inhcPxFRFcRYRQMP4XWDD/4Ng74XWDDQw4YfC68MN/hdb//wusEV4risKmKmK4qCsCcioKsVxUxWBDi4FpFwLSK+K2WRZFqWQmxZiaAjxNAR4mhaCblkJoKwqwTsE7gncVovhaQQwWji6L8VhVxXwTkVorRUC1C+L4WgLQL8EMLovxdxfC0xUFWCcQTsE6BOhVxVFWCcCqCdirFQVQTgVIripFeK4qYqCtFYE7FTBOuK0VwTvFeK0VsVoqRXioKoJwKorCsKoqiriuKgrip8VwTv+KgrCtFWCdYJ2K+CcRUFYVgTgV+KmKwr8VxU4rCv4Jx8V8VOKvisKiMTcPo0QQqjBbBbCCwVFhBcxQsz7IsCixGRXMUeRUCh8Kig5AVgCsCYECYEgqZqyKijaKoVForNWVIqZUqpPFQVorRUFUB4haQtcLRC0QiYRMI8A3AiAjhEhEBEBEYREA3oqAnQrCrFQE7iuCdYrwjhG+EYIiEYIkI8IgIwBvQjwjQjhHCIhGgG94//PkZIoYvgL4AHtNDjVj9fwga9rcRPhEwiQDcCNCPhEfCN8IiEf4RHhEAG8EbhEf/hHhEcI34RMIgIjhEhEBEBEf///CI+ERCP//wif/CICOQV4rgnOCdioK4qCtisKwqCqKgDzhaScHyfZOj6JwEq4a5OA1j7Pk+w7D4/JwfR9E6Ps+OfASg+Sdk7J1z7Pk+ydcnJOCck4PonB8k6CUHzw7D4PonB9BKOfHJxxNS1E15alkWhZCbcTUsyzE0LL8VBWBOcVoJ2KwrCvFQVhWBOoq8VsVQTqKorRXwToE6FbisCdAnAJyK4qRUFYV8VxVBORWFTFTBO+KwJyCdCqKorRVipwToVeKwrxVxWxXFeKgq4qeKgrYrcVvFfir4rRX/ip//ioKlSIGPk2H54xcEQ4AGIQiYQEIRXwiphgqUwAKwlgHlgIgAqVNt8Wcly022cpHf6pVStXVK1X3KgxaynblrRg+LguRcF4LUL4risCdAnIJyK8VMLWLkLV8Xovxci8LwWrF/i4LsX4uha4uQtXF8ZhGRGxGI6DoMwzjOOgz4jQzjOI0Mwufi/wtUXovYuRfFwXYWni6L2LwWoX8XvF0Xhci9F0VRUiqK8V8VoqgnWKn/////xUitisKn//F//PkZLkXtfz8VHMNHjub+fAAxJuMf/ir4rFpU2PQKLSIFemz6nCK5YWgV6BSnKnCjXoFf6pmrqlDgFYHzSTSNURSNfJnPpHs5Z2XLSQasIANVKwqkVK1X0VvUb9ThFYsLU4RX9qocBUzVGrqlEIPaq1YRcRfEVC4QRcRULh4XDxFhFhFBF/iLxFhFoXCiLRFIi8ReIqIqIriLhcIFwsRYRcRbgywuEhcKIqIuIuIvEXiKfC4XgWOBaAsf/At8CwBbAsAWAAPwLUC3gWQLAAHALUCwBYwLXgWIFjwLcC2AB74FoC1gWQLXgWfwLYFgC3gWMC0BawLQFjwiAiQjfCICMERCMEYInCJTEFNPsFk+yWDDpYMdhww4WTLABMAD5wwAPvPPgSsJ8CWAmEJWEwA8whLATCBAKWIAyAPGomokgHUZQDKJqJKfU+FzqfKzf+ATAIQEHgC8ALgNYAvQagaAagaQauDXAFwGqDRBrwaMGsGvg0A0g0wa+DQDXg1QBdBr4kILWI6C1gtAjoLSJERwLWI8SAkQWiDXwaf/8Gr8BD/gE3//g0eDV8GrwBd/waINPAQ///4CD/gE8Gv+DV4NQNINQeoALUTUtBNvy1E2/LItQR5ZCbDZByg5ODlByDb//PkZNUXBfLyAHMNHjoj+fzQfBtAE1E0E14mxalkJuJvy0AUuAoFmJuWomp8k6Po+CdHwfQSuEUBpBjCKKyKzF0LSL4vBawtQugO0XYWgLWCGC1C4A7ouC9C0Rc4rCtFUVxVFQE4FSCcCsKwqhaBcBFFwB6FwXwtEXRfF8XYvBaAtPFYV4qwTgVhUisKgq8VcV/BO8E4itFcE6FeKoJz4rCr4qCqKwriqKoqivFUVBXiqKoqQTjFUV8E6FXFTiuCdioKkVPFYVcE7FX4qfFeK8VYrip4J2Kwr+KvFfFXioK6NFrERD438BoyKYinOKdEQqRTnoQiKWIpiwiKWERTRFimOKdEUrREBkRQZEWDGiBHUgG0XUoRaIE2iAbRWigxosGNEwi0UGNEA2itEgxooMaKDGiBFPwRT+EU/QNP6fgNP6fgin/A2iNFwi0XBjRQi0XBjRQY0UGNFeEU/BFP+DE/4MT8DE/wNP6f//wi0ThFouEWieDGifgw8oRPIDDygw8gRPKDDyAw8mBnlPJwieX/4VEThOIvCMRP/7f/gxP/CKfm/wi0T+EWi4RaJ/CLRcGNFgxovhFon8ItEBjRYMaKEWif///+DGihiKkhAL6jHoBFGCwVK43+oyokgHQD//PkZP8dLea8AH/WBEKb5fGg09tMIB/UYUSKxBiRBiRJWIOOUMoVK4xlSplCqjKjCAdRj/TFDB4YOTFTHTEU7THU+GDkxkxAwcp349w1X4asjyOI7h3h3cVv4d3DuFaKwDKKwVodwdwGUVod4d4GcO8O8O7h3AZQ7uK0VnAyh3B3AZg7uKwO8GjBpAF7Bqg1g0A0A0A0A0g0A0waAagaQaMAXwagagaQaQaQaAagBeBog0g0cAXAaININeDVg0QaAaga8GvAF/Bo4NANOALgAuQaAaoNANINcGkGsGjBpBr/BpBqBqBqg0Qa+DXBp+DQDSDRg1g1A1A0QBeBowaf/BoBoBp4NXBrwaJMQU1FMy4xMDARjNpnwmmZbBYQFjDxHMZgtTkrWa1BVQUUE+CqwipX9FVRr1OVG1GgivCMEYA3QicIgIkC0BYAA6BawLARMIgA3IRARwjhH8I4RARuERhEhGhG/wjBEQj4Rgj8IgIj4RsIkI/wiYREIgIwRIRgjcI+EbCOEcIwRIRABuAG/+AbnhGipFcVIJ2CdAnEVorcVIrYqxV4RHCNwjQj8I3/wiQjeEf4RIRIBvhE/hEBEcIiESEYIgIj/+Ef/CNwjQiIRvCOdEVBVBOIripipxXB//PkZM0YHgrwEHMNGDYD9fwoa9qwOQTvLQsy1LRNGgMY0zTJ2AtnyAtHyTsNc+QkwC2Tk+j5E2E0LPgP5aibCaC+LsX8LSA7cVRXACOKwqAIYqCuCdxWFaKuKwJyKgqCuK4J1BO4rCuCdiuKgJ3FTFXxXivFQV4rivgncVRXFcVRVFUVYrivFSKgrcVfFUVxVitxVFcVhViqK+K0VhXiqCd+K0VuKuK/4qgnOCcCsK8VwTkE4FUVOK0X4vC5FyFpC1i+FowtMXIuBaIWoXYvhaYWiLoWoXQtfxUir8V/FTipNrUmXz5ng703qyKGMO8DvDHlQ7wx5UO8K+9K+8K+88+870+97wGO9hF3mDHeWBi28IraA1tra/hF3gGi1FmDEWwiiz4RRaDEW/4MNOBmnNNCJpoMNPhE0+ETThE00IotwYiz+EUWf/+EVtQYiyDEWwiizwii3/4MRbCKLAiiwIotBiLQiizCKLYRRZ4RRZwii34RRZgxFsGIt/8IotCKLYMRb4RW0EVtf4RW3/hFbX///Bi2/8GLb8IrbgxbcGLahFbf/4GttbVIAz8GdwZ8GKEVBihFYMWBqgRQrbK2its2mzOOM44sdlfZWcYgpWJ5WIYs5iihCwQoisFSlG/C//PkZP8Y/ei6AH+2EkJMAeRAlmS8FkCvLCyBSbCBRaUtKo0o2EKqcoqoqlh5RpFUtN/+gWmwWmTY8tIBr02U2E2C0qbKBRaRNktL/laxabCKhFAioGqAaqBqgRUDRAigMWDFBiAaLgxIMQIoDFwNUA1SDFhFeDFwZ0I/CPgzwjwR4GdBn8I9gzoM/BiQivA1SDFA1QIp8IoDECKQisGLBihFODE8IpwYgMXCK/CKAaJBigxQNE/gzuDOBncI8DO/hHsI9CPfBngzgusGHDD4NgyDYNC60LrhdYGwcF1guuGGC6/hhoXXBrOdcnmZLpkgUJC0se95b9dhtSkeliNjm2V7Xt/1ox54AUASYePwUk8z/zwApgTR4YpGE4nf+eeQa9//Qg3ISiXjw7YmUCaz+PiJVjP5Aj1k7Mx+fzPikqKcMvzOPXR7P7/3RR/FOgDtiGyXBnPlgHDKP4ghcI8A8olDtea/9Dt0eL9+johBjteHbo2jWvf5PYz7oHB4x74gH6hTIdt7opXo2/odsceqhsnigTWHpPGJqHHNRBU16HaZhsnts2TUPV+/O7Z8mYesTVKIIcLw+zxfnsPTzaJ2UkQhQ/kqe0eAjqoSr0bs0crqOXJNJtJGUtGgadEgmYaG//PkZPkc2gruAG1PXj/sHgAAe9d8FyLiW7ml0ymjS6aTaYLlff6YTRpJlMP0UmVKXFLHoI6W5AGmlTRoX1+aCWTaaSJ2GmlkimuaSRSplLJcaUXNEgl4Yyj6/5pfKSPNaypkUaZcSNHfHS5fU1AVSRNA00gW4uKbSr9Uvy+pRU36VMpPHLAVV4apNFSiLHOaWBHo6W0mkzc0EmaKqie5o0p6JFMbSyQTPpHNFKJZMHmhiDTCVv6UTaYTJo3SSDSuFVTRpe6aS1OaN7pdDJ0cpS4phBppdP7pJM01/7ltumktTSZS1HiqWk0lkB5PvEYXj6PTxDHEIDyB7e8gj9f/vEY5SkxBTUUzLjEwMKqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqoYDIhMQASaQdeKXjWIKCoKqxo1WNV121JtdSacUnoZoUtj5e///7cRFnoYIk2Vp///7cVYbH1NkiLIyVZoq4VLtRjSI6OgkCpGOtfxrf5KsExVhFe9NyIqgJgqdPBo6JREPgZB4wTSVZWehhnis2SljRDNDCUpkpEjf7QnDQVQBpImVQpGiFATLbH1uSvUJZGZIk3IsRTJVm2cksnB6FZdh6y5kVFzKK6SehUNIVHxmhIi46Fk//PkZLUXwgriIGRpHjCj8eQASZKRYpGmEUlXIk3SIl2HkpZuLIhGB4KkAmBTF1/LXOAwpNFq84dh1z6RJmRCQA0CJQnPIirwqQCYhg17krBqNahLJTg2hOGhSozKU0NbJXULJEfCpAGgVRkvWehSehVSXdtayslNkiXMopoZoWBUMmBMQnGBUXEoiPgiQBoibZl5KuRLTciKsEyroxlfjW5K4wRJuRT2SbkScGkKbJE2hgiXDIyYFSbNSVSg0qwiVhL3T0KTbpKpsrNkqzbPkrBpWCJNyqT0KTSrlgL+NoZaTEFNRTMuMTAwqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ":)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
